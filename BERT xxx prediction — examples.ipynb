{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'transfomers_bert_completions' from '/home/stephan/notebooks/child-directed-listening/transfomers_bert_completions.py'>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "import rpy2.robjects.lib.ggplot2 as ggplot2\n",
    "import childespy\n",
    "import numpy as np\n",
    "import os\n",
    "import imp\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import scipy.stats\n",
    "import copy\n",
    "from string import punctuation\n",
    "import transfomers_bert_completions\n",
    "imp.reload(transfomers_bert_completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Language Prediction Softmax with BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Older models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertForMaskedLM\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_in_childes = pd.read_csv('phon/cmu_in_childes.csv')\n",
    "cmu_2syl_inchildes = cmu_in_childes.loc[cmu_in_childes.num_vowels <=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_bertMaskedLM = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
    "adult_bertMaskedLM.eval()\n",
    "adult_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "adult_softmax_mask, adult_vocab = transfomers_bert_completions.get_softmax_mask(adult_tokenizer, cmu_2syl_inchildes.word)\n",
    "\n",
    "#\n",
    "ft1_bertMaskedLM = BertForMaskedLM.from_pretrained('model_output')\n",
    "ft1_bertMaskedLM.eval()\n",
    "ft1_tokenizer = BertTokenizer.from_pretrained('model_output')\n",
    "ft1_softmax_mask, ft1_vocab = transfomers_bert_completions.get_softmax_mask(ft1_tokenizer, cmu_2syl_inchildes.word)\n",
    "\n",
    "\n",
    "ft2_bertMaskedLM = BertForMaskedLM.from_pretrained('model_output2')\n",
    "ft2_bertMaskedLM.eval()\n",
    "ft2_tokenizer = BertTokenizer.from_pretrained('model_output2')\n",
    "ft2_softmax_mask, ft2_vocab = transfomers_bert_completions.get_softmax_mask(ft2_tokenizer, cmu_2syl_inchildes.word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'mommy', '[MASK]', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>word</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.234673e-01</td>\n",
       "      <td>said</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>1.197654e-01</td>\n",
       "      <td>says</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>6.757364e-02</td>\n",
       "      <td>screamed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4552</th>\n",
       "      <td>4.424965e-02</td>\n",
       "      <td>screams</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>4.048109e-02</td>\n",
       "      <td>knows</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4308</th>\n",
       "      <td>9.112945e-09</td>\n",
       "      <td>hire</td>\n",
       "      <td>7899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4387</th>\n",
       "      <td>8.820145e-09</td>\n",
       "      <td>pupil</td>\n",
       "      <td>7900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5744</th>\n",
       "      <td>8.202045e-09</td>\n",
       "      <td>fencing</td>\n",
       "      <td>7901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7465</th>\n",
       "      <td>4.160909e-09</td>\n",
       "      <td>blooded</td>\n",
       "      <td>7902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7359</th>\n",
       "      <td>2.671569e-09</td>\n",
       "      <td>gallon</td>\n",
       "      <td>7903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7904 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              prob      word  rank\n",
       "83    1.234673e-01      said     0\n",
       "592   1.197654e-01      says     1\n",
       "2943  6.757364e-02  screamed     2\n",
       "4552  4.424965e-02   screams     3\n",
       "1510  4.048109e-02     knows     4\n",
       "...            ...       ...   ...\n",
       "4308  9.112945e-09      hire  7899\n",
       "4387  8.820145e-09     pupil  7900\n",
       "5744  8.202045e-09   fencing  7901\n",
       "7465  4.160909e-09   blooded  7902\n",
       "7359  2.671569e-09    gallon  7903\n",
       "\n",
       "[7904 rows x 3 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(transfomers_bert_completions)\n",
    "test= transfomers_bert_completions.compare_completions(\"mommy [MASK] .\", \n",
    "        adult_bertMaskedLM, adult_tokenizer, adult_softmax_mask)\n",
    "test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'where', '[MASK]', '?', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>word</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.473997e-01</td>\n",
       "      <td>to</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>2.112121e-01</td>\n",
       "      <td>else</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>3.109264e-02</td>\n",
       "      <td>now</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.295018e-02</td>\n",
       "      <td>then</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.068886e-02</td>\n",
       "      <td>at</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6355</th>\n",
       "      <td>4.944750e-10</td>\n",
       "      <td>jays</td>\n",
       "      <td>7899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7866</th>\n",
       "      <td>4.470875e-10</td>\n",
       "      <td>condor</td>\n",
       "      <td>7900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7220</th>\n",
       "      <td>2.171247e-10</td>\n",
       "      <td>spitting</td>\n",
       "      <td>7901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7037</th>\n",
       "      <td>1.800662e-10</td>\n",
       "      <td>alfa</td>\n",
       "      <td>7902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7576</th>\n",
       "      <td>9.601243e-11</td>\n",
       "      <td>gust</td>\n",
       "      <td>7903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7904 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              prob      word  rank\n",
       "29    6.473997e-01        to     0\n",
       "643   2.112121e-01      else     1\n",
       "107   3.109264e-02       now     2\n",
       "86    1.295018e-02      then     3\n",
       "41    1.068886e-02        at     4\n",
       "...            ...       ...   ...\n",
       "6355  4.944750e-10      jays  7899\n",
       "7866  4.470875e-10    condor  7900\n",
       "7220  2.171247e-10  spitting  7901\n",
       "7037  1.800662e-10      alfa  7902\n",
       "7576  9.601243e-11      gust  7903\n",
       "\n",
       "[7904 rows x 3 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.compare_completions(\"where [MASK] ?\", adult_bertMaskedLM,\n",
    "    adult_tokenizer, adult_softmax_mask)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hi', '[MASK]', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>word</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.663202e-02</td>\n",
       "      <td>j</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>2.526271e-02</td>\n",
       "      <td>dr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.271786e-02</td>\n",
       "      <td>g</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>2.210053e-02</td>\n",
       "      <td>mr</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.103107e-02</td>\n",
       "      <td>n</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5008</th>\n",
       "      <td>6.478103e-08</td>\n",
       "      <td>spit</td>\n",
       "      <td>7899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6482</th>\n",
       "      <td>6.150631e-08</td>\n",
       "      <td>drown</td>\n",
       "      <td>7900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4904</th>\n",
       "      <td>3.894868e-08</td>\n",
       "      <td>drowned</td>\n",
       "      <td>7901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7420</th>\n",
       "      <td>3.889998e-08</td>\n",
       "      <td>writ</td>\n",
       "      <td>7902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>3.848243e-08</td>\n",
       "      <td>drowning</td>\n",
       "      <td>7903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7904 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              prob      word  rank\n",
       "9     3.663202e-02         j     0\n",
       "652   2.526271e-02        dr     1\n",
       "6     2.271786e-02         g     2\n",
       "562   2.210053e-02        mr     3\n",
       "13    2.103107e-02         n     4\n",
       "...            ...       ...   ...\n",
       "5008  6.478103e-08      spit  7899\n",
       "6482  6.150631e-08     drown  7900\n",
       "4904  3.894868e-08   drowned  7901\n",
       "7420  3.889998e-08      writ  7902\n",
       "5425  3.848243e-08  drowning  7903\n",
       "\n",
       "[7904 rows x 3 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.compare_completions(\"hi [MASK] .\", \n",
    "    adult_bertMaskedLM, adult_tokenizer, adult_softmax_mask)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', '[MASK]', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>word</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>4.661836e-01</td>\n",
       "      <td>if</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>3.032369e-01</td>\n",
       "      <td>happened</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.079754e-02</td>\n",
       "      <td>the</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2.106155e-02</td>\n",
       "      <td>now</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>1.707017e-02</td>\n",
       "      <td>happens</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5969</th>\n",
       "      <td>2.850364e-09</td>\n",
       "      <td>watkins</td>\n",
       "      <td>7899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6780</th>\n",
       "      <td>2.795745e-09</td>\n",
       "      <td>teller</td>\n",
       "      <td>7900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7037</th>\n",
       "      <td>2.744674e-09</td>\n",
       "      <td>alfa</td>\n",
       "      <td>7901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6821</th>\n",
       "      <td>1.950509e-09</td>\n",
       "      <td>milky</td>\n",
       "      <td>7902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7652</th>\n",
       "      <td>6.248128e-10</td>\n",
       "      <td>coli</td>\n",
       "      <td>7903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7904 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              prob      word  rank\n",
       "91    4.661836e-01        if     0\n",
       "775   3.032369e-01  happened     1\n",
       "25    3.079754e-02       the     2\n",
       "107   2.106155e-02       now     3\n",
       "2592  1.707017e-02   happens     4\n",
       "...            ...       ...   ...\n",
       "5969  2.850364e-09   watkins  7899\n",
       "6780  2.795745e-09    teller  7900\n",
       "7037  2.744674e-09      alfa  7901\n",
       "6821  1.950509e-09     milky  7902\n",
       "7652  6.248128e-10      coli  7903\n",
       "\n",
       "[7904 rows x 3 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.compare_completions(\"what [MASK] .\", adult_bertMaskedLM,\n",
    "    adult_tokenizer, adult_softmax_mask)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'go', '[MASK]', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>word</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>2.241399e-01</td>\n",
       "      <td>ahead</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.854041e-01</td>\n",
       "      <td>on</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>6.621100e-02</td>\n",
       "      <td>home</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>3.992473e-02</td>\n",
       "      <td>away</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>3.229499e-02</td>\n",
       "      <td>back</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5358</th>\n",
       "      <td>3.414107e-08</td>\n",
       "      <td>crushing</td>\n",
       "      <td>7899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7263</th>\n",
       "      <td>2.671358e-08</td>\n",
       "      <td>cords</td>\n",
       "      <td>7900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6986</th>\n",
       "      <td>2.521703e-08</td>\n",
       "      <td>terence</td>\n",
       "      <td>7901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7342</th>\n",
       "      <td>2.447127e-08</td>\n",
       "      <td>tending</td>\n",
       "      <td>7902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6962</th>\n",
       "      <td>1.685288e-08</td>\n",
       "      <td>pleas</td>\n",
       "      <td>7903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7904 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              prob      word  rank\n",
       "1237  2.241399e-01     ahead     0\n",
       "35    1.854041e-01        on     1\n",
       "190   6.621100e-02      home     2\n",
       "187   3.992473e-02      away     3\n",
       "93    3.229499e-02      back     4\n",
       "...            ...       ...   ...\n",
       "5358  3.414107e-08  crushing  7899\n",
       "7263  2.671358e-08     cords  7900\n",
       "6986  2.521703e-08   terence  7901\n",
       "7342  2.447127e-08   tending  7902\n",
       "6962  1.685288e-08     pleas  7903\n",
       "\n",
       "[7904 rows x 3 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions = transfomers_bert_completions.compare_completions(\"go [MASK] .\", \n",
    "    adult_bertMaskedLM, adult_tokenizer, adult_softmax_mask)[1]\n",
    "completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'it', \"'\", 's', 'time', 'to', 'go', 'to', 'the', '[MASK]', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.4456957e-07, 1.4368452e-07, 1.6101605e-07, ..., 3.6886934e-07,\n",
       "        2.9436049e-07, 9.5527675e-08], dtype=float32),\n",
       "               prob      word  rank\n",
       " 5723  1.267168e-01  bathroom     0\n",
       " 2902  7.437576e-02  hospital     1\n",
       " 3509  5.318297e-02     beach     2\n",
       " 5691  3.887066e-02    movies     3\n",
       " 3460  2.636691e-02    doctor     4\n",
       " ...            ...       ...   ...\n",
       " 1104  7.018393e-09         ı  7899\n",
       " 1577  6.265811e-09         ℓ  7900\n",
       " 5605  5.924017e-09     ##hip  7901\n",
       " 1537  5.464219e-09         ⁱ  7902\n",
       " 1105  4.721904e-09         ł  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.bert_completions(\"It's time to go to the [MASK] .\", adult_bertMaskedLM,\n",
    "    adult_tokenizer, np.array(range(len(adult_softmax_mask)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT without Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_phono = pd.read_pickle('csv/pvd_utt_glosses_phono_cleaned_inflated.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[MASK]', 'nonfiction', 'book', 'had', 'a', 'call', 'number', 'on', 'its', 'spine', '[SEP]']\n",
      "[3.6191159e-01 1.4366881e-05 5.9900581e-06 ... 1.3998201e-08 1.2068814e-08\n",
      " 7.0058026e-09]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>word</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.361912</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.297483</td>\n",
       "      <td>each</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.110310</td>\n",
       "      <td>any</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.101745</td>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.078346</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.022545</td>\n",
       "      <td>this</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.005070</td>\n",
       "      <td>his</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.002908</td>\n",
       "      <td>my</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.002811</td>\n",
       "      <td>that</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.002086</td>\n",
       "      <td>all</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prob  word  rank\n",
       "0    0.361912     a     0\n",
       "175  0.297483  each     1\n",
       "159  0.110310   any     2\n",
       "25   0.101745   the     3\n",
       "80   0.078346    no     4\n",
       "51   0.022545  this     5\n",
       "39   0.005070   his     6\n",
       "54   0.002908    my     7\n",
       "37   0.002811  that     8\n",
       "63   0.002086   all     9"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(transfomers_bert_completions)\n",
    "_,predictions =  transfomers_bert_completions.bert_completions(\n",
    "    \"[MASK] nonfiction book had a call number on its spine\", adult_bertMaskedLM, adult_tokenizer, \n",
    "    adult_softmax_mask)\n",
    "predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'an', 'excellent', 'plan', '[MASK]', '[SEP]']\n",
      "[4.6391021e-03 2.6643262e-04 4.5387544e-05 ... 3.8616272e-05 4.1947722e-05\n",
      " 1.9335086e-05]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>word</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.198977</td>\n",
       "      <td>and</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.054641</td>\n",
       "      <td>because</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.036092</td>\n",
       "      <td>that</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.031438</td>\n",
       "      <td>for</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.026816</td>\n",
       "      <td>the</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.025527</td>\n",
       "      <td>but</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.018605</td>\n",
       "      <td>as</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.010626</td>\n",
       "      <td>this</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.009459</td>\n",
       "      <td>now</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.009208</td>\n",
       "      <td>my</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prob     word  rank\n",
       "27   0.198977      and     0\n",
       "148  0.054641  because     1\n",
       "37   0.036092     that     2\n",
       "34   0.031438      for     3\n",
       "25   0.026816      the     4\n",
       "49   0.025527      but     5\n",
       "33   0.018605       as     6\n",
       "51   0.010626     this     7\n",
       "107  0.009459      now     8\n",
       "54   0.009208       my     9"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# otsb: off the shelf BERT\n",
    "_,predictions =  transfomers_bert_completions.bert_completions(\n",
    "    \"what an excellent plan [MASK]\", adult_bertMaskedLM, adult_tokenizer, adult_softmax_mask)\n",
    "predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'an', 'excellent', '[MASK]', '.', '[SEP]']\n",
      "[1.2686984e-05 5.9214440e-06 3.2423511e-06 ... 4.4299151e-05 3.3103188e-05\n",
      " 2.0205528e-07]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>word</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>0.098999</td>\n",
       "      <td>question</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0.089799</td>\n",
       "      <td>plan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.058697</td>\n",
       "      <td>man</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>0.040538</td>\n",
       "      <td>choice</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0.028590</td>\n",
       "      <td>friend</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.024965</td>\n",
       "      <td>thought</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.022737</td>\n",
       "      <td>day</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.020267</td>\n",
       "      <td>woman</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>0.014959</td>\n",
       "      <td>job</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0.013705</td>\n",
       "      <td>story</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          prob      word  rank\n",
       "847   0.098999  question     0\n",
       "703   0.089799      plan     1\n",
       "165   0.058697       man     2\n",
       "1111  0.040538    choice     3\n",
       "595   0.028590    friend     4\n",
       "232   0.024965   thought     5\n",
       "162   0.022737       day     6\n",
       "382   0.020267     woman     7\n",
       "809   0.014959       job     8\n",
       "392   0.013705     story     9"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# otsb: off the shelf BERT\n",
    "_,predictions =  transfomers_bert_completions.bert_completions(\n",
    "    \"what an excellent [MASK] .\", adult_bertMaskedLM, adult_tokenizer, adult_softmax_mask)\n",
    "predictions.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.7512279e-05 4.6954942e-06 1.7185309e-06 ... 1.9388933e-06 7.3790943e-06\n",
      " 2.9924431e-07]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/notebooks/child-directed-listening/lib/python3.7/site-packages/pandas/core/indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "# no context, otsb: off the shelf BERT\n",
    "priors, completions, stats = transfomers_bert_completions.get_stats_for_failure(\n",
    "    all_tokens_phono, 16764425, adult_bertMaskedLM, adult_tokenizer, adult_softmax_mask,\n",
    "    None, use_speaker_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.7512279e-05 4.6954942e-06 1.7185309e-06 ... 1.9388933e-06 7.3790943e-06\n",
      " 2.9924431e-07]\n",
      "              prob      word  rank\n",
      "179   7.037868e-01     place     0\n",
      "429   6.678923e-02     thing     1\n",
      "165   1.722324e-02       man     2\n",
      "823   1.432071e-02       guy     3\n",
      "5387  1.370183e-02  nonsense     4\n",
      "...            ...       ...   ...\n",
      "6446  1.696233e-09  forwards  7899\n",
      "7072  1.578553e-09   graders  7900\n",
      "4302  1.565044e-09      laps  7901\n",
      "6001  1.560177e-09     wills  7902\n",
      "4745  8.996724e-10    thirds  7903\n",
      "\n",
      "[7904 rows x 3 columns]\n",
      "       rank  prob   entropy  num_tokens_in_context  bert_token_id\n",
      "26826   NaN   NaN  2.876822                      4          26826\n"
     ]
    }
   ],
   "source": [
    "print(priors) \n",
    "print(completions)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5922917e-02 1.4876368e-02 1.5768111e-02 ... 1.0458241e-05 4.0509703e-06\n",
      " 3.4971272e-06]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1.5922917e-02, 1.4876368e-02, 1.5768111e-02, ..., 1.0458241e-05,\n",
       "         4.0509703e-06, 3.4971272e-06]], dtype=float32),\n",
       " [              prob      word  rank\n",
       "  15    1.550260e-01         p     0\n",
       "  1335  7.418367e-02        ed     1\n",
       "  2720  2.585679e-02        op     2\n",
       "  0     1.592292e-02         a     3\n",
       "  2     1.576811e-02         c     4\n",
       "  ...            ...       ...   ...\n",
       "  3858  2.001719e-07   smelled  7899\n",
       "  4844  1.871311e-07    tasted  7900\n",
       "  4427  1.673072e-07    blamed  7901\n",
       "  7190  1.354257e-07   blaming  7902\n",
       "  5984  1.214368e-07  highness  7903\n",
       "  \n",
       "  [7904 rows x 3 columns]],\n",
       "     rank      prob   entropy  num_tokens_in_context  bert_token_id  \\\n",
       " 42  7113  0.000002  8.392303                      1             42   \n",
       " \n",
       "     mask_position  token  utterance_id  \n",
       " 42              0  mommy      16759315  )"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no context, otsb: off the shelf BERT\n",
    "transfomers_bert_completions.get_stats_for_success(all_tokens_phono, 16759315, adult_bertMaskedLM, \n",
    "        adult_tokenizer, adult_softmax_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[MASK]', '.', '[SEP]']\n",
      "[1.1490965e-02 2.0865996e-03 1.2678547e-03 ... 3.6077824e-06 1.9368913e-04\n",
      " 7.7008317e-06]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.1490965e-02, 2.0865996e-03, 1.2678547e-03, ..., 3.6077824e-06,\n",
       "        1.9368913e-04, 7.7008317e-06], dtype=float32),\n",
       "               prob     word  rank\n",
       " 25    1.754317e-02      the     0\n",
       " 80    1.313224e-02       no     1\n",
       " 0     1.149097e-02        a     2\n",
       " 38    1.031996e-02       it     3\n",
       " 35    9.670383e-03       on     4\n",
       " ...            ...      ...   ...\n",
       " 7663  5.419910e-07    boxed  7899\n",
       " 7465  4.390227e-07  blooded  7900\n",
       " 5448  4.197854e-07   billed  7901\n",
       " 5146  3.850632e-07    posed  7902\n",
       " 4542  3.315166e-07    petty  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tuned BERT: don't use speaker labels\n",
    "transfomers_bert_completions.bert_completions(\n",
    "    \"[MASK] .\", ft1_bertMaskedLM, ft1_tokenizer, adult_softmax_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2034437e-02 9.0256333e-04 1.4439654e-04 ... 5.4023002e-07 2.6916116e-05\n",
      " 6.8373361e-06]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/notebooks/child-directed-listening/lib/python3.7/site-packages/pandas/core/indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.2034437e-02, 9.0256333e-04, 1.4439654e-04, ..., 5.4023002e-07,\n",
       "        2.6916116e-05, 6.8373361e-06], dtype=float32),\n",
       "               prob      word  rank\n",
       " 56    9.787571e-02       one     0\n",
       " 429   8.345397e-02     thing     1\n",
       " 32    5.292996e-02        is     2\n",
       " 185   4.436808e-02      here     3\n",
       " 34    3.257630e-02       for     4\n",
       " ...            ...       ...   ...\n",
       " 5126  6.021823e-09  airports  7899\n",
       " 7241  6.009501e-09   braking  7900\n",
       " 7866  3.241802e-09    condor  7901\n",
       " 5583  2.310708e-09   motions  7902\n",
       " 5679  1.696319e-09    leaked  7903\n",
       " \n",
       " [7904 rows x 3 columns],\n",
       "        rank  prob  entropy  num_tokens_in_context  bert_token_id\n",
       " 26826   NaN   NaN  7.68566                      4          26826)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no context ft1: Fine-tuned BERT: don't use speaker labels\n",
    "transfomers_bert_completions.get_stats_for_failure(\n",
    "    all_tokens_phono, 16764425, ft1_bertMaskedLM, ft1_tokenizer, ft1_softmax_mask,\n",
    "    None, use_speaker_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1490965e-02 2.0865996e-03 1.2678547e-03 ... 3.6077824e-06 1.9368913e-04\n",
      " 7.7008317e-06]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1.1490965e-02, 2.0865996e-03, 1.2678547e-03, ..., 3.6077824e-06,\n",
       "         1.9368913e-04, 7.7008317e-06]], dtype=float32),\n",
       " [              prob     word  rank\n",
       "  25    1.754317e-02      the     0\n",
       "  80    1.313224e-02       no     1\n",
       "  0     1.149097e-02        a     2\n",
       "  38    1.031996e-02       it     3\n",
       "  35    9.670383e-03       on     4\n",
       "  ...            ...      ...   ...\n",
       "  7663  5.419910e-07    boxed  7899\n",
       "  7465  4.390227e-07  blooded  7900\n",
       "  5448  4.197854e-07   billed  7901\n",
       "  5146  3.850632e-07    posed  7902\n",
       "  4542  3.315166e-07    petty  7903\n",
       "  \n",
       "  [7904 rows x 3 columns]],\n",
       "     rank      prob    entropy  num_tokens_in_context  bert_token_id  \\\n",
       " 42   723  0.000264  10.683244                      1             42   \n",
       " \n",
       "     mask_position  token  utterance_id  \n",
       " 42              0  mommy      16759315  )"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no context, ft1: Fine-tuned BERT: don't use speaker labels\n",
    "transfomers_bert_completions.get_stats_for_success(all_tokens_phono, 16759315, ft1_bertMaskedLM, \n",
    "        ft1_tokenizer, ft1_softmax_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'transfomers_bert_completions' from '/home/stephan/notebooks/child-directed-listening/transfomers_bert_completions.py'>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transfomers_bert_completions\n",
    "imp.reload(transfomers_bert_completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[chi]', '[MASK]', '.', '[SEP]']\n",
      "[7.1630292e-03 1.6241993e-03 1.0531463e-03 ... 1.5241826e-06 1.6998670e-04\n",
      " 2.9817779e-06]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([7.1630292e-03, 1.6241993e-03, 1.0531463e-03, ..., 1.5241826e-06,\n",
       "        1.6998670e-04, 2.9817779e-06], dtype=float32),\n",
       "               prob        word  rank\n",
       " 80    2.959480e-02          no     0\n",
       " 630   1.779688e-02          oh     1\n",
       " 73    1.736975e-02       there     2\n",
       " 56    1.563328e-02         one     3\n",
       " 988   1.324041e-02        yeah     4\n",
       " ...            ...         ...   ...\n",
       " 5984  5.000543e-08    highness  7899\n",
       " 1195  4.420516e-08      latter  7900\n",
       " 7100  3.975406e-08     boasted  7901\n",
       " 7334  3.718563e-08  yourselves  7902\n",
       " 7541  1.118408e-08     parades  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ft2: Fine-tuned BERT: with speaker labels\n",
    "transfomers_bert_completions.bert_completions(\n",
    "    \"[chi] [MASK] .\", ft2_bertMaskedLM, ft2_tokenizer, ft2_softmax_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[cgv]', '[MASK]', '.', '[SEP]']\n",
      "[4.4205962e-03 2.5260656e-03 1.5678611e-03 ... 1.1706089e-06 8.7236323e-05\n",
      " 1.4746232e-05]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([4.4205962e-03, 2.5260656e-03, 1.5678611e-03, ..., 1.1706089e-06,\n",
       "        8.7236323e-05, 1.4746232e-05], dtype=float32),\n",
       "               prob        word  rank\n",
       " 80    1.917182e-02          no     0\n",
       " 89    1.139848e-02        more     1\n",
       " 38    9.651138e-03          it     2\n",
       " 988   8.615519e-03        yeah     3\n",
       " 203   8.411843e-03        good     4\n",
       " ...            ...         ...   ...\n",
       " 6964  5.794139e-07     annette  7899\n",
       " 7334  5.395387e-07  yourselves  7900\n",
       " 6494  5.354856e-07     frankly  7901\n",
       " 7188  5.113905e-07    counters  7902\n",
       " 5146  4.953320e-07       posed  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ft2: Fine-tuned BERT: with speaker labels\n",
    "transfomers_bert_completions.bert_completions(\n",
    "    \"[cgv] [MASK] .\", ft2_bertMaskedLM, ft2_tokenizer, ft2_softmax_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.21968761e-03 2.89252523e-04 1.01761514e-04 ... 6.29579688e-07\n",
      " 2.96900471e-05 3.03201432e-06]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/notebooks/child-directed-listening/lib/python3.7/site-packages/pandas/core/indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3.21968761e-03, 2.89252523e-04, 1.01761514e-04, ...,\n",
       "        6.29579688e-07, 2.96900471e-05, 3.03201432e-06], dtype=float32),\n",
       "               prob      word  rank\n",
       " 56    1.858872e-01       one     0\n",
       " 429   1.034543e-01     thing     1\n",
       " 32    6.308804e-02        is     2\n",
       " 51    2.303730e-02      this     3\n",
       " 185   2.060782e-02      here     4\n",
       " ...            ...       ...   ...\n",
       " 7634  3.872345e-09     lowry  7899\n",
       " 6555  3.739657e-09   martins  7900\n",
       " 7747  3.690924e-09  seasoned  7901\n",
       " 7541  2.270597e-09   parades  7902\n",
       " 7127  2.186263e-09      pubs  7903\n",
       " \n",
       " [7904 rows x 3 columns],\n",
       "        rank  prob   entropy  num_tokens_in_context  bert_token_id\n",
       " 26826   NaN   NaN  7.120152                      5          26826)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no context ft2: Fine-tuned BERT: with speaker labels\n",
    "transfomers_bert_completions.get_stats_for_failure(\n",
    "    all_tokens_phono, 16764425, ft2_bertMaskedLM, ft2_tokenizer,\n",
    "    ft2_softmax_mask, None, use_speaker_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02469595e-02 9.94500704e-03 8.72355234e-03 ... 5.15204647e-07\n",
      " 1.10446425e-04 4.00284216e-06]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1.02469595e-02, 9.94500704e-03, 8.72355234e-03, ...,\n",
       "         5.15204647e-07, 1.10446425e-04, 4.00284216e-06]], dtype=float32),\n",
       " [              prob        word  rank\n",
       "  19    5.266874e-02           t     0\n",
       "  18    2.626214e-02           s     1\n",
       "  25    1.559725e-02         the     2\n",
       "  7     1.444659e-02           h     3\n",
       "  80    1.397641e-02          no     4\n",
       "  ...            ...         ...   ...\n",
       "  7630  2.317104e-07       racks  7899\n",
       "  7398  2.180164e-07     nonstop  7900\n",
       "  7396  2.086807e-07      hinges  7901\n",
       "  7804  1.627102e-07       scuba  7902\n",
       "  7334  1.254817e-07  yourselves  7903\n",
       "  \n",
       "  [7904 rows x 3 columns]],\n",
       "     rank      prob   entropy  num_tokens_in_context  bert_token_id  \\\n",
       " 42   582  0.000242  9.730298                      1             42   \n",
       " \n",
       "     mask_position  token  utterance_id  \n",
       " 42              0  mommy      16759315  )"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.get_stats_for_success(all_tokens_phono, 16759315, \n",
    "    ft2_bertMaskedLM, ft2_tokenizer, ft2_softmax_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'look', '!', '[SEP]', 'you', 'see', 'the', 'ball', '.', '[SEP]', 'what', 'is', 'it', '?', '[SEP]', '[MASK]', '!', '[SEP]']\n",
      "[2.9447689e-04 1.9055986e-05 9.5565183e-06 ... 2.8018499e-07 2.2896807e-06\n",
      " 3.3121651e-07]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([2.9447689e-04, 1.9055986e-05, 9.5565183e-06, ..., 2.8018499e-07,\n",
       "        2.2896807e-06, 3.3121651e-07], dtype=float32),\n",
       "               prob       word  rank\n",
       " 265   7.059029e-01       look     0\n",
       " 1847  2.771682e-02        hey     1\n",
       " 80    2.366572e-02         no     2\n",
       " 630   2.194207e-02         oh     3\n",
       " 73    2.063640e-02      there     4\n",
       " ...            ...        ...   ...\n",
       " 6905  1.132056e-08   tempered  7899\n",
       " 6420  1.007555e-08    prefers  7900\n",
       " 4958  8.470574e-09     cooled  7901\n",
       " 3372  7.183141e-09  addressed  7902\n",
       " 7801  5.356558e-09   thanking  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.bert_completions(\n",
    "    \"look ! [SEP] you see the ball . [SEP] what is it? [SEP] [MASK] !\",\n",
    "    adult_bertMaskedLM,\n",
    "    adult_tokenizer,\n",
    "    adult_softmax_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[MASK]', '!', '[SEP]']\n",
      "[5.2099419e-04 1.9024061e-04 1.3560688e-04 ... 2.0262019e-06 4.0520654e-06\n",
      " 5.2389805e-07]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.2099419e-04, 1.9024061e-04, 1.3560688e-04, ..., 2.0262019e-06,\n",
       "        4.0520654e-06, 5.2389805e-07], dtype=float32),\n",
       "               prob      word  rank\n",
       " 180   1.421626e-01        go     0\n",
       " 1847  1.083946e-01       hey     1\n",
       " 336   2.970560e-02      help     2\n",
       " 2042  2.592897e-02        ha     3\n",
       " 6667  2.522623e-02     yahoo     4\n",
       " ...            ...       ...   ...\n",
       " 6347  1.924245e-08   coating  7899\n",
       " 7727  1.762578e-08    paving  7900\n",
       " 4892  1.688352e-08   weighed  7901\n",
       " 5142  9.923831e-09  repaired  7902\n",
       " 5557  7.355099e-09  weighing  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.bert_completions(\n",
    "    \"[MASK] !\",\n",
    "    adult_bertMaskedLM,\n",
    "    adult_tokenizer,\n",
    "    adult_softmax_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'look', '!', '[SEP]', 'you', 'see', 'the', 'ball', '.', '[SEP]', 'what', 'is', 'it', '?', '[SEP]', '[MASK]', '!', '[SEP]']\n",
      "[3.3722995e-04 8.0243672e-06 1.3456638e-05 ... 2.2696716e-09 3.2982490e-07\n",
      " 6.3708242e-08]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3.3722995e-04, 8.0243672e-06, 1.3456638e-05, ..., 2.2696716e-09,\n",
       "        3.2982490e-07, 6.3708242e-08], dtype=float32),\n",
       "               prob       word  rank\n",
       " 265   8.214665e-01       look     0\n",
       " 1116  6.919919e-02       ball     1\n",
       " 630   1.057040e-02         oh     2\n",
       " 1002  8.318879e-03      watch     3\n",
       " 163   4.932548e-03        see     4\n",
       " ...            ...        ...   ...\n",
       " 7659  7.309568e-11    sockets  7899\n",
       " 4542  6.387398e-11      petty  7900\n",
       " 2645  6.073569e-11    greatly  7901\n",
       " 2786  5.940642e-11  preferred  7902\n",
       " 5559  3.651145e-11     vowels  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.bert_completions(\n",
    "    \"look ! [SEP] you see the ball . [SEP] what is it? [SEP] [MASK] !\",\n",
    "    ft1_bertMaskedLM,\n",
    "    ft1_tokenizer, \n",
    "    ft1_softmax_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[MASK]', '!', '[SEP]']\n",
      "[6.6175396e-03 9.3328045e-04 3.8436559e-04 ... 1.5088195e-06 2.5173698e-05\n",
      " 8.6116580e-07]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([6.6175396e-03, 9.3328045e-04, 3.8436559e-04, ..., 1.5088195e-06,\n",
       "        2.5173698e-05, 8.6116580e-07], dtype=float32),\n",
       "               prob       word  rank\n",
       " 180   5.293307e-02         go     0\n",
       " 630   4.451400e-02         oh     1\n",
       " 80    2.914032e-02         no     2\n",
       " 2534  2.634110e-02         ah     3\n",
       " 3567  2.466197e-02       boom     4\n",
       " ...            ...        ...   ...\n",
       " 3585  2.473545e-08       isle  7899\n",
       " 7659  1.701468e-08    sockets  7900\n",
       " 4743  1.565106e-08    leisure  7901\n",
       " 5189  1.538653e-08  greenwich  7902\n",
       " 1929  1.454352e-08    assumed  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.bert_completions(\n",
    "    \"[MASK] !\",\n",
    "    ft1_bertMaskedLM,\n",
    "    ft1_tokenizer,\n",
    "    ft1_softmax_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[cgv]', 'look', '!', '[cgv]', '[SEP]', 'you', 'see', 'the', 'ball', '.', '[SEP]', '[cgv]', 'what', 'is', 'it', '?', '[SEP]', '[cgv]', '[MASK]', '!', '[SEP]']\n",
      "[1.3582357e-04 4.7675712e-06 8.8429761e-06 ... 9.6583508e-10 7.7967492e-08\n",
      " 4.7733621e-08]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.3582357e-04, 4.7675712e-06, 8.8429761e-06, ..., 9.6583508e-10,\n",
       "        7.7967492e-08, 4.7733621e-08], dtype=float32),\n",
       "               prob     word  rank\n",
       " 265   9.245384e-01     look     0\n",
       " 1116  1.886555e-02     ball     1\n",
       " 80    6.116604e-03       no     2\n",
       " 630   5.484046e-03       oh     3\n",
       " 163   4.926789e-03      see     4\n",
       " ...            ...      ...   ...\n",
       " 7557  1.748249e-11     sima  7899\n",
       " 6898  1.746130e-11  herring  7900\n",
       " 7127  1.483380e-11     pubs  7901\n",
       " 7659  1.388839e-11  sockets  7902\n",
       " 6964  1.164501e-11  annette  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.bert_completions(\n",
    "    \"[cgv] look ! [cgv] [SEP] you see the ball . [SEP] [cgv] what is it? [SEP] [cgv] [MASK] !\",\n",
    "    ft2_bertMaskedLM,\n",
    "    ft2_tokenizer,\n",
    "    ft2_softmax_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_context = \"you can play it . [SEP] build the blocks . [SEP] look . [SEP] what is this [MASK] . [SEP] wee . [SEP] what ? [SEP] alright just a little . . .\"\n",
    "minimal_context = \"what is this [MASK] .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'you', 'can', 'play', 'it', '.', '[SEP]', 'build', 'the', 'blocks', '.', '[SEP]', 'look', '.', '[SEP]', 'what', 'is', 'this', '[MASK]', '.', '[SEP]', 'wee', '.', '[SEP]', 'what', '?', '[SEP]', 'alright', 'just', 'a', 'little', '.', '.', '.', '[SEP]']\n",
      "[4.1206350e-04 4.4401662e-05 1.1446742e-05 ... 3.1579634e-06 2.1047217e-05\n",
      " 1.9096753e-07]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([4.1206350e-04, 4.4401662e-05, 1.1446742e-05, ..., 3.1579634e-06,\n",
       "        2.1047217e-05, 1.9096753e-07], dtype=float32),\n",
       "               prob       word  rank\n",
       " 1849  1.846767e-01      stuff     0\n",
       " 429   1.394682e-01      thing     1\n",
       " 82    1.224868e-01      about     2\n",
       " 1632  4.064887e-02       shit     3\n",
       " 823   4.016055e-02        guy     4\n",
       " ...            ...        ...   ...\n",
       " 4802  8.735933e-09     whites  7899\n",
       " 3627  4.555039e-09    centres  7900\n",
       " 7072  3.839028e-09    graders  7901\n",
       " 4745  2.761369e-09     thirds  7902\n",
       " 6726  2.212416e-09  crossings  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.bert_completions(full_context, adult_bertMaskedLM,\n",
    "    adult_tokenizer, adult_softmax_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'is', 'this', '[MASK]', '.', '[SEP]']\n",
      "[5.7512279e-05 4.6954942e-06 1.7185309e-06 ... 1.9388933e-06 7.3790943e-06\n",
      " 2.9924431e-07]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.7512279e-05, 4.6954942e-06, 1.7185309e-06, ..., 1.9388933e-06,\n",
       "        7.3790943e-06, 2.9924431e-07], dtype=float32),\n",
       "               prob      word  rank\n",
       " 179   7.037868e-01     place     0\n",
       " 429   6.678923e-02     thing     1\n",
       " 165   1.722324e-02       man     2\n",
       " 823   1.432071e-02       guy     3\n",
       " 5387  1.370183e-02  nonsense     4\n",
       " ...            ...       ...   ...\n",
       " 6446  1.696233e-09  forwards  7899\n",
       " 7072  1.578553e-09   graders  7900\n",
       " 4302  1.565044e-09      laps  7901\n",
       " 6001  1.560177e-09     wills  7902\n",
       " 4745  8.996724e-10    thirds  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.bert_completions(minimal_context, adult_bertMaskedLM,\n",
    "    adult_tokenizer, adult_softmax_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'you', 'can', 'play', 'it', '.', '[SEP]', 'build', 'the', 'blocks', '.', '[SEP]', 'look', '.', '[SEP]', 'what', 'is', 'this', '[MASK]', '.', '[SEP]', 'wee', '.', '[SEP]', 'what', '?', '[SEP]', 'alright', 'just', 'a', 'little', '.', '.', '.', '[SEP]']\n",
      "[4.8073740e-03 9.6353717e-05 2.3748234e-04 ... 1.4534255e-07 1.1587276e-05\n",
      " 1.5726483e-06]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([4.8073740e-03, 9.6353717e-05, 2.3748234e-04, ..., 1.4534255e-07,\n",
       "        1.1587276e-05, 1.5726483e-06], dtype=float32),\n",
       "               prob      word  rank\n",
       " 429   1.337474e-01     thing     0\n",
       " 56    1.279441e-01       one     1\n",
       " 185   4.643393e-02      here     2\n",
       " 3809  4.023589e-02    fraser     3\n",
       " 176   3.672646e-02    called     4\n",
       " ...            ...       ...   ...\n",
       " 2645  1.122958e-09   greatly  7899\n",
       " 7241  7.916627e-10   braking  7900\n",
       " 6205  7.453299e-10     spurs  7901\n",
       " 7866  4.192773e-10    condor  7902\n",
       " 5985  2.194643e-10  turnpike  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.bert_completions(full_context, ft1_bertMaskedLM,\n",
    "    ft1_tokenizer, ft1_softmax_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'is', 'this', '[MASK]', '.', '[SEP]']\n",
      "[1.2034437e-02 9.0256333e-04 1.4439654e-04 ... 5.4023002e-07 2.6916116e-05\n",
      " 6.8373361e-06]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.2034437e-02, 9.0256333e-04, 1.4439654e-04, ..., 5.4023002e-07,\n",
       "        2.6916116e-05, 6.8373361e-06], dtype=float32),\n",
       "               prob      word  rank\n",
       " 56    9.787571e-02       one     0\n",
       " 429   8.345397e-02     thing     1\n",
       " 32    5.292996e-02        is     2\n",
       " 185   4.436808e-02      here     3\n",
       " 34    3.257630e-02       for     4\n",
       " ...            ...       ...   ...\n",
       " 5126  6.021823e-09  airports  7899\n",
       " 7241  6.009501e-09   braking  7900\n",
       " 7866  3.241802e-09    condor  7901\n",
       " 5583  2.310708e-09   motions  7902\n",
       " 5679  1.696319e-09    leaked  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.bert_completions(minimal_context, ft1_bertMaskedLM,\n",
    "    ft1_tokenizer, ft1_softmax_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define minimal and full context with [cgv] and [chi] items\n",
    "full_context = \"[cgv] you can play it . [SEP] [cgv] build the blocks . [SEP] [cgv] look . [SEP] [cgv] what is this [MASK] . [SEP] [chi] wee . [SEP] [cgv] what ? [SEP] [cgv]  alright just a little . . .\"\n",
    "minimal_context = \"[cgv] what is this [MASK] .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[cgv]', 'what', 'is', 'this', '[MASK]', '.', '[SEP]']\n",
      "[1.8574581e-03 2.2755688e-04 1.6224546e-04 ... 1.9756760e-07 2.6747892e-05\n",
      " 4.2586885e-06]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.8574581e-03, 2.2755688e-04, 1.6224546e-04, ..., 1.9756760e-07,\n",
       "        2.6747892e-05, 4.2586885e-06], dtype=float32),\n",
       "               prob     word  rank\n",
       " 56    7.246278e-02      one     0\n",
       " 3880  6.886909e-02      huh     1\n",
       " 185   5.969979e-02     here     2\n",
       " 429   5.437483e-02    thing     3\n",
       " 34    2.839113e-02      for     4\n",
       " ...            ...      ...   ...\n",
       " 7100  2.665498e-09  boasted  7899\n",
       " 7518  2.574831e-09  snooker  7900\n",
       " 6205  2.376044e-09    spurs  7901\n",
       " 7541  1.321016e-09  parades  7902\n",
       " 7127  1.075959e-09     pubs  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.bert_completions(minimal_context, ft2_bertMaskedLM,\n",
    "    ft2_tokenizer, ft2_softmax_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[cgv]', 'you', 'can', 'play', 'it', '.', '[SEP]', '[cgv]', 'build', 'the', 'blocks', '.', '[SEP]', '[cgv]', 'look', '.', '[SEP]', '[cgv]', 'what', 'is', 'this', '[MASK]', '.', '[SEP]', '[chi]', 'wee', '.', '[SEP]', '[cgv]', 'what', '?', '[SEP]', '[cgv]', 'alright', 'just', 'a', 'little', '.', '.', '.', '[SEP]']\n",
      "[4.4540415e-04 1.3089532e-05 1.0229954e-04 ... 1.0216678e-07 1.2149234e-05\n",
      " 3.1916500e-06]\n",
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([4.4540415e-04, 1.3089532e-05, 1.0229954e-04, ..., 1.0216678e-07,\n",
       "        1.2149234e-05, 3.1916500e-06], dtype=float32),\n",
       "               prob       word  rank\n",
       " 176   1.486644e-01     called     0\n",
       " 429   9.965955e-02      thing     1\n",
       " 3853  9.522507e-02    darling     2\n",
       " 185   9.165560e-02       here     3\n",
       " 56    5.568583e-02        one     4\n",
       " ...            ...        ...   ...\n",
       " 2617  6.267783e-10    credits  7899\n",
       " 4581  5.322159e-10      fangs  7900\n",
       " 6474  3.398692e-10  passports  7901\n",
       " 6555  1.880796e-10    martins  7902\n",
       " 7541  9.861655e-11    parades  7903\n",
       " \n",
       " [7904 rows x 3 columns])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomers_bert_completions.bert_completions(full_context, ft2_bertMaskedLM,\n",
    "    ft2_tokenizer, ft2_softmax_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Successes and Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'transfomers_bert_completions' from '/home/stephan/notebooks/child-directed-listening/transfomers_bert_completions.py'>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transfomers_bert_completions\n",
    "imp.reload(transfomers_bert_completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/notebooks/child-directed-listening/lib/python3.7/site-packages/pandas/core/indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-280-1cd9d4a2153b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m transfomers_bert_completions.get_stats_for_failure(\n\u001b[1;32m      4\u001b[0m     \u001b[0mall_tokens_phono\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m17280349\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madult_bertMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madult_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madult_softmax_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     5, use_speaker_labels=False)\n\u001b[0m",
      "\u001b[0;32m~/notebooks/child-directed-listening/transfomers_bert_completions.py\u001b[0m in \u001b[0;36mget_stats_for_failure\u001b[0;34m(all_tokens, selected_utt_id, bertMaskedLM, tokenizer, softmax_mask, context_width_in_utts, use_speaker_labels)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m#print('GPU retrieval time: '+str(time.time() - t2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m#print('Total time: '+str(time.time() - t1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_completions_for_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbertMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Empty tokens for utterance '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_utt_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/transfomers_bert_completions.py\u001b[0m in \u001b[0;36mget_completions_for_mask\u001b[0;34m(utt_df, true_word, bertMaskedLM, tokenizer, softmax_mask)\u001b[0m\n\u001b[1;32m    110\u001b[0m     gloss_with_mask =  tokenizer.convert_tokens_to_ids(['[CLS]']\n\u001b[1;32m    111\u001b[0m         ) + utt_df.token_id.tolist() + tokenizer.convert_tokens_to_ids(['[SEP]'])    \n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mpriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_completions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgloss_with_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbertMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrue_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompletions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/transfomers_bert_completions.py\u001b[0m in \u001b[0;36mbert_completions\u001b[0;34m(text, model, tokenizer, softmax_mask)\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;31m# Predict all tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m       \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msoftmax_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, masked_lm_labels)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n\u001b[0;32m--> 862\u001b[0;31m                                        output_all_encoded_layers=False)\n\u001b[0m\u001b[1;32m    863\u001b[0m         \u001b[0mprediction_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0mextended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10000.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mwords_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "#### Here is a MWE to pick back up\n",
    "\n",
    "transfomers_bert_completions.get_stats_for_failure(\n",
    "    all_tokens_phono, 17280349, adult_bertMaskedLM, adult_tokenizer, adult_softmax_mask,\n",
    "    5, use_speaker_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the args\n",
    "all_tokens = all_tokens_phono\n",
    "selected_utt_id = 17280349\n",
    "bertMaskedLM = adult_bertMaskedLM\n",
    "tokenizer = adult_tokenizer\n",
    "softmax_mask = adult_softmax_mask\n",
    "context_width_in_utts = 2\n",
    "use_speaker_labels =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/notebooks/child-directed-listening/lib/python3.7/site-packages/pandas/core/indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>token_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3083434</th>\n",
       "      <td>[chi]</td>\n",
       "      <td>30522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083435</th>\n",
       "      <td>[MASK]</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083436</th>\n",
       "      <td>.</td>\n",
       "      <td>1012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          token  token_id\n",
       "3083434   [chi]     30522\n",
       "3083435  [MASK]       103\n",
       "3083436       .      1012"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt_df = all_tokens.loc[all_tokens.id == selected_utt_id]\n",
    "utt_df.loc[utt_df.token == 'yyy','token'] = '[MASK]'\n",
    "utt_df.loc[utt_df.token == '[MASK]','token_id'] = 103\n",
    "utt_df[['token','token_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[chi]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-326-9b02cc40e910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# check if yy is in the vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mft2_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"[chi]\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '[chi]'"
     ]
    }
   ],
   "source": [
    "# check if yy is in the vocab\n",
    "ft2_tokenizer.vocab[\"[chi]\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ft2_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neither tokeinzer has xxx or yyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_transcript_id = utt_df.iloc[0].transcript_id\n",
    "\n",
    "this_seq_utt_id = utt_df.iloc[0].seq_utt_id\n",
    "before_utt_df = all_tokens.loc[(all_tokens.seq_utt_id < this_seq_utt_id) & (all_tokens.seq_utt_id > (this_seq_utt_id - (context_width_in_utts+1))) & (all_tokens['transcript_id'] == this_transcript_id)]\n",
    "\n",
    "after_utt_df = all_tokens.loc[(all_tokens.seq_utt_id > this_seq_utt_id) & (all_tokens.seq_utt_id < this_seq_utt_id + (context_width_in_utts + 1)) & (all_tokens.transcript_id == this_transcript_id)]\n",
    "\n",
    "# ready to use before_utt_df and after_utt_df    \n",
    "sep_row = pd.DataFrame({'token':['[SEP]'], 'token_id':tokenizer.convert_tokens_to_ids(['[SEP]'])})\n",
    "\n",
    "if before_utt_df.shape[0] > 0:\n",
    "    before_by_sent = [x[1] for x in before_utt_df.groupby(['seq_utt_id'])]    \n",
    "    i = n = 1\n",
    "    while i < len(before_by_sent):\n",
    "        before_by_sent.insert(i, sep_row)\n",
    "        i += (n+1)\n",
    "    before_by_sent_df = pd.concat(before_by_sent)\n",
    "else:\n",
    "    before_by_sent_df = pd.DataFrame()\n",
    "\n",
    "if after_utt_df.shape[0] > 0:\n",
    "    after_by_sent = [x[1] for x in after_utt_df.groupby(['seq_utt_id'])]    \n",
    "    i = n = 1\n",
    "    while i < len(after_by_sent):\n",
    "        after_by_sent.insert(i, sep_row)\n",
    "        i += (n+1)\n",
    "    after_by_sent_df = pd.concat(after_by_sent)\n",
    "else:\n",
    "    after_by_sent_df = pd.DataFrame()\n",
    "\n",
    "utt_df = pd.concat([before_by_sent_df, sep_row, utt_df, sep_row, after_by_sent_df])\n",
    "\n",
    "# if preserve_errors:\n",
    "#     #convert @ back to yyy for context items\n",
    "#     utt_df.loc[utt_df.token == '@','token'] = 'yyy'\n",
    "#     utt_df.loc[utt_df.token == 'yyy','token_id'] = tokenizer.convert_tokens_to_ids(['yyy'])[0]\n",
    "#     #convert @ back to xxx for context items\n",
    "#     utt_df.loc[utt_df.token == '$','token'] = 'xxx'\n",
    "#     utt_df.loc[utt_df.token == 'xxx','token_id'] = tokenizer.convert_tokens_to_ids(['xxx'])[0]\n",
    "\n",
    "if np.sum(utt_df.token == '[MASK]') > 1:\n",
    "    print('Multiple masks in the surrounding context')\n",
    "    import pdb\n",
    "    pdb.set_trace() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>token_id</th>\n",
       "      <th>bert_token_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3083429</th>\n",
       "      <td>yyy</td>\n",
       "      <td>30524</td>\n",
       "      <td>3083429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083430</th>\n",
       "      <td>.</td>\n",
       "      <td>1012</td>\n",
       "      <td>3083430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083432</th>\n",
       "      <td>yyy</td>\n",
       "      <td>30524</td>\n",
       "      <td>3083432.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083433</th>\n",
       "      <td>.</td>\n",
       "      <td>1012</td>\n",
       "      <td>3083433.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083435</th>\n",
       "      <td>[MASK]</td>\n",
       "      <td>103</td>\n",
       "      <td>3083435.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083436</th>\n",
       "      <td>.</td>\n",
       "      <td>1012</td>\n",
       "      <td>3083436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083438</th>\n",
       "      <td>bee</td>\n",
       "      <td>10506</td>\n",
       "      <td>3083438.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083439</th>\n",
       "      <td>##p</td>\n",
       "      <td>2361</td>\n",
       "      <td>3083439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083440</th>\n",
       "      <td>bee</td>\n",
       "      <td>10506</td>\n",
       "      <td>3083440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083441</th>\n",
       "      <td>##p</td>\n",
       "      <td>2361</td>\n",
       "      <td>3083441.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083442</th>\n",
       "      <td>bee</td>\n",
       "      <td>10506</td>\n",
       "      <td>3083442.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083443</th>\n",
       "      <td>##p</td>\n",
       "      <td>2361</td>\n",
       "      <td>3083443.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083444</th>\n",
       "      <td>bee</td>\n",
       "      <td>10506</td>\n",
       "      <td>3083444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083445</th>\n",
       "      <td>##p</td>\n",
       "      <td>2361</td>\n",
       "      <td>3083445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083446</th>\n",
       "      <td>bee</td>\n",
       "      <td>10506</td>\n",
       "      <td>3083446.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083447</th>\n",
       "      <td>##p</td>\n",
       "      <td>2361</td>\n",
       "      <td>3083447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083448</th>\n",
       "      <td>bee</td>\n",
       "      <td>10506</td>\n",
       "      <td>3083448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083449</th>\n",
       "      <td>##p</td>\n",
       "      <td>2361</td>\n",
       "      <td>3083449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083450</th>\n",
       "      <td>.</td>\n",
       "      <td>1012</td>\n",
       "      <td>3083450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083452</th>\n",
       "      <td>bee</td>\n",
       "      <td>10506</td>\n",
       "      <td>3083452.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083453</th>\n",
       "      <td>##p</td>\n",
       "      <td>2361</td>\n",
       "      <td>3083453.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083454</th>\n",
       "      <td>bee</td>\n",
       "      <td>10506</td>\n",
       "      <td>3083454.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083455</th>\n",
       "      <td>##p</td>\n",
       "      <td>2361</td>\n",
       "      <td>3083455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083456</th>\n",
       "      <td>bee</td>\n",
       "      <td>10506</td>\n",
       "      <td>3083456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083457</th>\n",
       "      <td>##p</td>\n",
       "      <td>2361</td>\n",
       "      <td>3083457.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083458</th>\n",
       "      <td>bee</td>\n",
       "      <td>10506</td>\n",
       "      <td>3083458.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083459</th>\n",
       "      <td>##p</td>\n",
       "      <td>2361</td>\n",
       "      <td>3083459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083460</th>\n",
       "      <td>.</td>\n",
       "      <td>1012</td>\n",
       "      <td>3083460.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          token  token_id  bert_token_id\n",
       "3083429     yyy     30524      3083429.0\n",
       "3083430       .      1012      3083430.0\n",
       "0         [SEP]       102            NaN\n",
       "3083432     yyy     30524      3083432.0\n",
       "3083433       .      1012      3083433.0\n",
       "0         [SEP]       102            NaN\n",
       "3083435  [MASK]       103      3083435.0\n",
       "3083436       .      1012      3083436.0\n",
       "0         [SEP]       102            NaN\n",
       "3083438     bee     10506      3083438.0\n",
       "3083439     ##p      2361      3083439.0\n",
       "3083440     bee     10506      3083440.0\n",
       "3083441     ##p      2361      3083441.0\n",
       "3083442     bee     10506      3083442.0\n",
       "3083443     ##p      2361      3083443.0\n",
       "3083444     bee     10506      3083444.0\n",
       "3083445     ##p      2361      3083445.0\n",
       "3083446     bee     10506      3083446.0\n",
       "3083447     ##p      2361      3083447.0\n",
       "3083448     bee     10506      3083448.0\n",
       "3083449     ##p      2361      3083449.0\n",
       "3083450       .      1012      3083450.0\n",
       "0         [SEP]       102            NaN\n",
       "3083452     bee     10506      3083452.0\n",
       "3083453     ##p      2361      3083453.0\n",
       "3083454     bee     10506      3083454.0\n",
       "3083455     ##p      2361      3083455.0\n",
       "3083456     bee     10506      3083456.0\n",
       "3083457     ##p      2361      3083457.0\n",
       "3083458     bee     10506      3083458.0\n",
       "3083459     ##p      2361      3083459.0\n",
       "3083460       .      1012      3083460.0"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt_df = utt_df.loc[~utt_df.token.isin(['[chi]','[cgv]'])][['token', 'token_id','bert_token_id']]\n",
    "utt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-297-c929d38f55c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransfomers_bert_completions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_completions_for_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft1_bertMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft1_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/notebooks/child-directed-listening/transfomers_bert_completions.py\u001b[0m in \u001b[0;36mget_completions_for_mask\u001b[0;34m(utt_df, true_word, bertMaskedLM, tokenizer, softmax_mask)\u001b[0m\n\u001b[1;32m    110\u001b[0m     gloss_with_mask =  tokenizer.convert_tokens_to_ids(['[CLS]']\n\u001b[1;32m    111\u001b[0m         ) + utt_df.token_id.tolist() + tokenizer.convert_tokens_to_ids(['[SEP]'])    \n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mpriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_completions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgloss_with_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbertMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrue_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompletions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/transfomers_bert_completions.py\u001b[0m in \u001b[0;36mbert_completions\u001b[0;34m(text, model, tokenizer, softmax_mask)\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;31m# Predict all tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m       \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msoftmax_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, masked_lm_labels)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n\u001b[0;32m--> 862\u001b[0;31m                                        output_all_encoded_layers=False)\n\u001b[0m\u001b[1;32m    863\u001b[0m         \u001b[0mprediction_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0mextended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10000.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mwords_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "transfomers_bert_completions.get_completions_for_mask(utt_df, None, ft1_bertMaskedLM, ft1_tokenizer, softmax_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_stats_for_success() got multiple values for argument 'use_speaker_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-26d684d19589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m transfomers_bert_completions.get_stats_for_success(all_tokens_phono, 16759315, adult_bertMaskedLM, \n\u001b[0;32m----> 2\u001b[0;31m         adult_tokenizer, adult_softmax_mask, 'score', None, use_speaker_labels=False, preserve_errors=True)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_stats_for_success() got multiple values for argument 'use_speaker_labels'"
     ]
    }
   ],
   "source": [
    "transfomers_bert_completions.get_stats_for_success(all_tokens_phono, 16759315, adult_bertMaskedLM, \n",
    "        adult_tokenizer, adult_softmax_mask, 'score', None, use_speaker_labels=False, preserve_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_stats_for_failure() got an unexpected keyword argument 'preserve_errors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-36306deaa38e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m transfomers_bert_completions.get_stats_for_failure(\n\u001b[1;32m      3\u001b[0m     \u001b[0mall_tokens_phono\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m17280276\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft1_bertMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft1_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft1_softmax_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     5, use_speaker_labels=False, preserve_errors=True)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_stats_for_failure() got an unexpected keyword argument 'preserve_errors'"
     ]
    }
   ],
   "source": [
    "# no context, ft1: Fine-tuned BERT: don't use speaker labels\n",
    "transfomers_bert_completions.get_stats_for_failure(\n",
    "    all_tokens_phono, 17280276, ft1_bertMaskedLM, ft1_tokenizer, ft1_softmax_mask,\n",
    "    5, use_speaker_labels=False, preserve_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no context, ft1: Fine-tuned BERT: don't use speaker labels\n",
    "transfomers_bert_completions.get_stats_for_success(all_tokens_phono, 16759315, ft1_bertMaskedLM, \n",
    "        ft1_tokenizer, ft1_softmax_mask, 'score', 5, use_speaker_labels=False, preserve_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft2: Fine-tuned BERT: with speaker labels\n",
    "transfomers_bert_completions.get_stats_for_failure(\n",
    "    all_tokens_phono, 17280276, ft2_bertMaskedLM, ft2_tokenizer,\n",
    "    ft2_softmax_mask, 5, use_speaker_labels=True,\n",
    "    preserve_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-aedccc00913f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ft2: Fine-tuned BERT: with speaker labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m transfomers_bert_completions.get_stats_for_success(all_tokens_phono, 16759315, \n\u001b[0;32m----> 3\u001b[0;31m ft2_bertMaskedLM, ft2_tokenizer, ft2_softmax_mask, 5, use_speaker_labels=True)\n\u001b[0m",
      "\u001b[0;32m~/notebooks/child-directed-listening/transfomers_bert_completions.py\u001b[0m in \u001b[0;36mget_stats_for_success\u001b[0;34m(all_tokens, selected_utt_id, bertMaskedLM, tokenizer, softmax_mask, context_width_in_utts, use_speaker_labels)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         this_priors, this_completions, this_stats = get_completions_for_mask(utt_df_local, \n\u001b[0;32m--> 322\u001b[0;31m             utt_df.iloc[mask_position].token, bertMaskedLM, tokenizer, softmax_mask) \n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mthis_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask_position'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/transfomers_bert_completions.py\u001b[0m in \u001b[0;36mget_completions_for_mask\u001b[0;34m(utt_df, true_word, bertMaskedLM, tokenizer, softmax_mask)\u001b[0m\n\u001b[1;32m    110\u001b[0m     gloss_with_mask =  tokenizer.convert_tokens_to_ids(['[CLS]']\n\u001b[1;32m    111\u001b[0m         ) + utt_df.token_id.tolist() + tokenizer.convert_tokens_to_ids(['[SEP]'])    \n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mpriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_completions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgloss_with_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbertMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrue_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompletions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/transfomers_bert_completions.py\u001b[0m in \u001b[0;36mbert_completions\u001b[0;34m(text, model, tokenizer, softmax_mask)\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;31m# Predict all tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m       \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msoftmax_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, masked_lm_labels)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n\u001b[0;32m--> 862\u001b[0;31m                                        output_all_encoded_layers=False)\n\u001b[0m\u001b[1;32m    863\u001b[0m         \u001b[0mprediction_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0mextended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10000.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mwords_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/child-directed-listening/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# ft2: Fine-tuned BERT: with speaker labels\n",
    "transfomers_bert_completions.get_stats_for_success(all_tokens_phono, 16759315, \n",
    "ft2_bertMaskedLM, ft2_tokenizer, ft2_softmax_mask, 5, use_speaker_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve from selected utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 1000 success utterances and 1000 failure utterances\n",
    "selected_success_utts = np.random.choice(success_utts.utterance_id, 500, replace=False)\n",
    "selected_yyy_utts = np.random.choice(yyy_utts.utterance_id, 1000, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_scores = transfomers_bert_completions.compare_successes_failures_unigram_model(\n",
    "    all_tokens_phono,\n",
    "    selected_success_utts,\n",
    "    selected_yyy_utts,\n",
    "    adult_tokenizer,\n",
    "    adult_softmax_mask,\n",
    "    'data/chi_vocab.csv',\n",
    "    initial_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_scores_flat = transfomers_bert_completions.compare_successes_failures_unigram_model(\n",
    "    all_tokens_phono,\n",
    "    selected_success_utts,\n",
    "    selected_yyy_utts,\n",
    "    adult_tokenizer,\n",
    "    adult_softmax_mask,\n",
    "    None,\n",
    "    initial_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is going on with \"Each\" probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM as trans_BertForMaskedLM\n",
    "from transformers import BertTokenizer as trans_BertTokenizer\n",
    " \n",
    "wwm = trans_BertForMaskedLM.from_pretrained('bert-large-uncased-whole-word-masking')\n",
    "wwm.eval()\n",
    "wwm_tokenizer = trans_BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 30519, 30520, 30521])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wwm_softmax_mask, wwm_vocab = transfomers_bert_completions.get_softmax_mask(wwm_tokenizer, cmu_2syl_inchildes.word)\n",
    "full_softmax_mask = np.argwhere(np.ones(len(wwm_tokenizer.vocab))).flatten()\n",
    "full_softmax_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[MASK]', 'nonfiction', 'book', 'has', 'a', 'call', 'number', 'on', 'its', 'spine', '.', '[SEP]']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-1ecb7abbbbed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"[MASK] nonfiction book has a call number on its spine .\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwwm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwwm_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     full_softmax_mask)\n\u001b[0m",
      "\u001b[0;32m~/notebooks/child-directed-listening/transfomers_bert_completions.py\u001b[0m in \u001b[0;36mbert_completions\u001b[0;34m(text, model, tokenizer, softmax_mask)\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m   \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msoftmax_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m   \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msoftmax_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "completions = transfomers_bert_completions.bert_completions(\n",
    "    \"[MASK] nonfiction book has a call number on its spine .\", wwm, \n",
    "    wwm_tokenizer, full_softmax_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the off-the shelf adult model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[MASK]', 'nonfiction', 'book', 'has', 'a', 'call', 'number', 'on', 'its', 'spine', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "completions = transfomers_bert_completions.bert_completions(\n",
    "    \"[MASK] nonfiction book has a call number on its spine .\", adult_bertMaskedLM, \n",
    "    adult_tokenizer, full_softmax_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>word</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>0.418243</td>\n",
       "      <td>each</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>0.382689</td>\n",
       "      <td>every</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>0.099194</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.052032</td>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>0.022856</td>\n",
       "      <td>any</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>0.010374</td>\n",
       "      <td>this</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>0.006768</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>0.000921</td>\n",
       "      <td>some</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>0.000679</td>\n",
       "      <td>most</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>0.000631</td>\n",
       "      <td>one</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          prob   word  rank\n",
       "2169  0.418243   each     0\n",
       "2296  0.382689  every     1\n",
       "1037  0.099194      a     2\n",
       "1996  0.052032    the     3\n",
       "2151  0.022856    any     4\n",
       "2023  0.010374   this     5\n",
       "2053  0.006768     no     6\n",
       "2070  0.000921   some     7\n",
       "2087  0.000679   most     8\n",
       "2028  0.000631    one     9"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions[1].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "child-listening-env",
   "language": "python",
   "name": "child-listening-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
