{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import transformers_bert_completions, load_splits, load_models\n",
    "from utils_model_sampling import sample_across_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changed this notebook to use the +/- 20 context newly generated versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change the tokenizer from model output2 to be dynamic if possible\n"
     ]
    }
   ],
   "source": [
    "eval_data = load_splits.load_eval_data_all('all', 'all')\n",
    "\n",
    "# Is it possible that success_utts already has the glosses? Need to check this\n",
    "\n",
    "success_utts = eval_data['success_utts']\n",
    "yyy_utts = eval_data['yyy_utts']\n",
    "all_tokens_phono = eval_data['phono']\n",
    "\n",
    "initial_vocab, cmu_in_initial_vocab = load_models.get_initial_vocab_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"pop pop pop pop | another one another one | I don't want to | apple apple apple apple | a red a pink | oh A B c | Mom what is that | a green a blue | beep beep beep beep | a b a b | one two three four | did you did you | one two one two | one two one two | b e a o | what is this two | no no no no | a cars a cars | a two a two | beep beep beep beep | one two one one | a tree a tree | way up over there | Mom Mommy Mom Mommy | where's Daddy where's Dada | hi Gigi hi Gigi | a ball a ball | a four a four | another one another one | mah mah mah mah | what's that what's this | what's that a ball | five five five five | no no Mommy no | mah muh mm mm | one one two three | a green a green | a green a green | a square a square | I did a square | another one another one | no no no no | no no no no | another one another one | a ball a ball | a ball a ball | red circle what's that | a ball a ball | a four a five | Mommy I did it | look at the green | Mommy can you fix | I try to push | Mommy help me with | Mom help me with | Mommy help me with | right there this one | Mommy help me with | that way like that | here's the white one | wee I play white | yellow yellow yellow yellow | Mommy help me with | one two one two | it's in the ball | in the truck please | Mommy want open this | one two one two | Telly here is Telly | I want a blue | one two three four | white one yes white | I want it again | slide it this way | cook cook with Mommy | yeah yeah a blue | I don't want it | this in the bowl | tweet tweet tweet tweet | oh look there's five | hello rabbit hello rabbit | what prize is it | up up the hill | Andrew Alexander Alexander Alexander | the one the one | here Mommy Mommy box | all done what's this | there's a pink purple | help me with more | hit the a bat | no no help me | ooh it's a car | ah Mommy bear please | Daddy cook for Daddy | one two three four | pink green pink blue | Mommy do it please | and a blue banana | camera is up there | I don't wanna Mommy\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success_utts_with_gloss = success_utts.copy() # Already merged with gloss in new Providence notebook\n",
    "success_utts_with_gloss['num_tokens'] = [len(x.split(' ')) for x in success_utts_with_gloss['gloss']]\n",
    "\n",
    "\n",
    "' | '.join(success_utts_with_gloss.loc[success_utts_with_gloss.num_tokens ==4 ].gloss[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = success_utts_with_gloss.loc[success_utts_with_gloss.gloss=='I want to read'].utterance_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['token', 'id', 'gloss', 'transcript_id', 'utterance_order',\n",
       "       'speaker_code', 'target_child_name', 'target_child_age', 'type',\n",
       "       'punct', 'speaker_code_simple', 'gloss_with_punct', 'token_id',\n",
       "       'seq_utt_id', 'actual_phonology', 'model_phonology', 'bert_token_id',\n",
       "       'model_phonology_clean', 'actual_phonology_clean',\n",
       "       'model_phonology_no_dia', 'actual_phonology_no_dia', 'cv_raw',\n",
       "       'cv_collapsed', 'num_vowels', 'in_vocab', 'success_token', 'yyy_token',\n",
       "       'partition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens_phono.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gloss</th>\n",
       "      <th>actual_phonology_no_dia</th>\n",
       "      <th>model_phonology_no_dia</th>\n",
       "      <th>id</th>\n",
       "      <th>bert_token_id</th>\n",
       "      <th>utterance_order</th>\n",
       "      <th>transcript_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1032327</th>\n",
       "      <td>I want to read</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>16928243</td>\n",
       "      <td>1032327</td>\n",
       "      <td>310</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032328</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>ɑə</td>\n",
       "      <td>ɑə</td>\n",
       "      <td>16928243</td>\n",
       "      <td>1032328</td>\n",
       "      <td>310</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032329</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>wɑn</td>\n",
       "      <td>wɑnt</td>\n",
       "      <td>16928243</td>\n",
       "      <td>1032329</td>\n",
       "      <td>310</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032330</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>də</td>\n",
       "      <td>tu</td>\n",
       "      <td>16928243</td>\n",
       "      <td>1032330</td>\n",
       "      <td>310</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032331</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>wid</td>\n",
       "      <td>ɹid</td>\n",
       "      <td>16928243</td>\n",
       "      <td>1032331</td>\n",
       "      <td>310</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032332</th>\n",
       "      <td>I want to read</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>16928243</td>\n",
       "      <td>1032332</td>\n",
       "      <td>310</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067443</th>\n",
       "      <td>I want to read</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>16933485</td>\n",
       "      <td>1067443</td>\n",
       "      <td>631</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067444</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>ɑə</td>\n",
       "      <td>ɑə</td>\n",
       "      <td>16933485</td>\n",
       "      <td>1067444</td>\n",
       "      <td>631</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067445</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>wɑn</td>\n",
       "      <td>wɑnt</td>\n",
       "      <td>16933485</td>\n",
       "      <td>1067445</td>\n",
       "      <td>631</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067446</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>də</td>\n",
       "      <td>tu</td>\n",
       "      <td>16933485</td>\n",
       "      <td>1067446</td>\n",
       "      <td>631</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067447</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>wid</td>\n",
       "      <td>ɹid</td>\n",
       "      <td>16933485</td>\n",
       "      <td>1067447</td>\n",
       "      <td>631</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067448</th>\n",
       "      <td>I want to read</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>16933485</td>\n",
       "      <td>1067448</td>\n",
       "      <td>631</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070652</th>\n",
       "      <td>I want to read</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>16934000</td>\n",
       "      <td>1070652</td>\n",
       "      <td>666</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070653</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>ɑə</td>\n",
       "      <td>ɑə</td>\n",
       "      <td>16934000</td>\n",
       "      <td>1070653</td>\n",
       "      <td>666</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070654</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>wɑn</td>\n",
       "      <td>wɑnt</td>\n",
       "      <td>16934000</td>\n",
       "      <td>1070654</td>\n",
       "      <td>666</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070655</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>də</td>\n",
       "      <td>tu</td>\n",
       "      <td>16934000</td>\n",
       "      <td>1070655</td>\n",
       "      <td>666</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070656</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>wid</td>\n",
       "      <td>ɹid</td>\n",
       "      <td>16934000</td>\n",
       "      <td>1070656</td>\n",
       "      <td>666</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070657</th>\n",
       "      <td>I want to read</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>16934000</td>\n",
       "      <td>1070657</td>\n",
       "      <td>666</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281594</th>\n",
       "      <td>I want to read</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>16962364</td>\n",
       "      <td>1281594</td>\n",
       "      <td>1614</td>\n",
       "      <td>42330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281595</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>ɑə</td>\n",
       "      <td>ɑə</td>\n",
       "      <td>16962364</td>\n",
       "      <td>1281595</td>\n",
       "      <td>1614</td>\n",
       "      <td>42330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281596</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>wɑn</td>\n",
       "      <td>wɑnt</td>\n",
       "      <td>16962364</td>\n",
       "      <td>1281596</td>\n",
       "      <td>1614</td>\n",
       "      <td>42330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281597</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>də</td>\n",
       "      <td>tu</td>\n",
       "      <td>16962364</td>\n",
       "      <td>1281597</td>\n",
       "      <td>1614</td>\n",
       "      <td>42330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281598</th>\n",
       "      <td>I want to read</td>\n",
       "      <td>wid</td>\n",
       "      <td>ɹid</td>\n",
       "      <td>16962364</td>\n",
       "      <td>1281598</td>\n",
       "      <td>1614</td>\n",
       "      <td>42330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281599</th>\n",
       "      <td>I want to read</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>16962364</td>\n",
       "      <td>1281599</td>\n",
       "      <td>1614</td>\n",
       "      <td>42330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  gloss actual_phonology_no_dia model_phonology_no_dia  \\\n",
       "1032327  I want to read                                                  \n",
       "1032328  I want to read                      ɑə                     ɑə   \n",
       "1032329  I want to read                     wɑn                   wɑnt   \n",
       "1032330  I want to read                      də                     tu   \n",
       "1032331  I want to read                     wid                    ɹid   \n",
       "1032332  I want to read                                                  \n",
       "1067443  I want to read                                                  \n",
       "1067444  I want to read                      ɑə                     ɑə   \n",
       "1067445  I want to read                     wɑn                   wɑnt   \n",
       "1067446  I want to read                      də                     tu   \n",
       "1067447  I want to read                     wid                    ɹid   \n",
       "1067448  I want to read                                                  \n",
       "1070652  I want to read                                                  \n",
       "1070653  I want to read                      ɑə                     ɑə   \n",
       "1070654  I want to read                     wɑn                   wɑnt   \n",
       "1070655  I want to read                      də                     tu   \n",
       "1070656  I want to read                     wid                    ɹid   \n",
       "1070657  I want to read                                                  \n",
       "1281594  I want to read                                                  \n",
       "1281595  I want to read                      ɑə                     ɑə   \n",
       "1281596  I want to read                     wɑn                   wɑnt   \n",
       "1281597  I want to read                      də                     tu   \n",
       "1281598  I want to read                     wid                    ɹid   \n",
       "1281599  I want to read                                                  \n",
       "\n",
       "               id  bert_token_id  utterance_order  transcript_id  \n",
       "1032327  16928243        1032327              310          42336  \n",
       "1032328  16928243        1032328              310          42336  \n",
       "1032329  16928243        1032329              310          42336  \n",
       "1032330  16928243        1032330              310          42336  \n",
       "1032331  16928243        1032331              310          42336  \n",
       "1032332  16928243        1032332              310          42336  \n",
       "1067443  16933485        1067443              631          42336  \n",
       "1067444  16933485        1067444              631          42336  \n",
       "1067445  16933485        1067445              631          42336  \n",
       "1067446  16933485        1067446              631          42336  \n",
       "1067447  16933485        1067447              631          42336  \n",
       "1067448  16933485        1067448              631          42336  \n",
       "1070652  16934000        1070652              666          42336  \n",
       "1070653  16934000        1070653              666          42336  \n",
       "1070654  16934000        1070654              666          42336  \n",
       "1070655  16934000        1070655              666          42336  \n",
       "1070656  16934000        1070656              666          42336  \n",
       "1070657  16934000        1070657              666          42336  \n",
       "1281594  16962364        1281594             1614          42330  \n",
       "1281595  16962364        1281595             1614          42330  \n",
       "1281596  16962364        1281596             1614          42330  \n",
       "1281597  16962364        1281597             1614          42330  \n",
       "1281598  16962364        1281598             1614          42330  \n",
       "1281599  16962364        1281599             1614          42330  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens_phono.loc[all_tokens_phono.id.isin(selected)][['gloss','actual_phonology_no_dia',\n",
    " 'model_phonology_no_dia', 'id','bert_token_id','utterance_order','transcript_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find new test examples- successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change the initial tokenizer to be based on latest trained models, eventually.\n",
      "********* CHECKING THE TOKENIZATION *****\n",
      "For model id all/all/with_tags/0_context/childes\n",
      "['[CLS]', '[chi]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', '[cgv]', 'back', 'on', 'the', 'table', 'if', 'you', 'wanna', 'finish', 'it', '.', '[SEP]']\n",
      "********* CHECKING THE TOKENIZATION *****\n",
      "For model id all/all/with_tags/20_context/childes\n",
      "['[CLS]', '[chi]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', '[cgv]', 'back', 'on', 'the', 'table', 'if', 'you', 'wanna', 'finish', 'it', '.', '[SEP]']\n",
      "********* CHECKING THE TOKENIZATION *****\n",
      "For model id all/all/no_tags/0_context/childes\n",
      "['[CLS]', '[chi]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', '[cgv]', 'back', 'on', 'the', 'table', 'if', 'you', 'wanna', 'finish', 'it', '.', '[SEP]']\n",
      "********* CHECKING THE TOKENIZATION *****\n",
      "For model id all/all/no_tags/20_context/childes\n",
      "['[CLS]', '[chi]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', '[cgv]', 'back', 'on', 'the', 'table', 'if', 'you', 'wanna', 'finish', 'it', '.', '[SEP]']\n",
      "********* CHECKING THE TOKENIZATION *****\n",
      "For model id age/young/with_tags/0_context/childes\n",
      "['[CLS]', '[chi]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', '[cgv]', 'back', 'on', 'the', 'table', 'if', 'you', 'wanna', 'finish', 'it', '.', '[SEP]']\n",
      "********* CHECKING THE TOKENIZATION *****\n",
      "For model id age/young/with_tags/20_context/childes\n",
      "['[CLS]', '[chi]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', '[cgv]', 'back', 'on', 'the', 'table', 'if', 'you', 'wanna', 'finish', 'it', '.', '[SEP]']\n",
      "********* CHECKING THE TOKENIZATION *****\n",
      "For model id age/young/no_tags/0_context/childes\n",
      "['[CLS]', '[chi]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', '[cgv]', 'back', 'on', 'the', 'table', 'if', 'you', 'wanna', 'finish', 'it', '.', '[SEP]']\n",
      "********* CHECKING THE TOKENIZATION *****\n",
      "For model id age/young/no_tags/20_context/childes\n",
      "['[CLS]', '[chi]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', '[cgv]', 'back', 'on', 'the', 'table', 'if', 'you', 'wanna', 'finish', 'it', '.', '[SEP]']\n",
      "********* CHECKING THE TOKENIZATION *****\n",
      "For model id age/old/with_tags/0_context/childes\n",
      "['[CLS]', '[chi]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', '[cgv]', 'back', 'on', 'the', 'table', 'if', 'you', 'wanna', 'finish', 'it', '.', '[SEP]']\n",
      "********* CHECKING THE TOKENIZATION *****\n",
      "For model id age/old/with_tags/20_context/childes\n",
      "['[CLS]', '[chi]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', '[cgv]', 'back', 'on', 'the', 'table', 'if', 'you', 'wanna', 'finish', 'it', '.', '[SEP]']\n",
      "********* CHECKING THE TOKENIZATION *****\n",
      "For model id age/old/no_tags/0_context/childes\n",
      "['[CLS]', '[chi]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', '[cgv]', 'back', 'on', 'the', 'table', 'if', 'you', 'wanna', 'finish', 'it', '.', '[SEP]']\n",
      "********* CHECKING THE TOKENIZATION *****\n",
      "For model id age/old/no_tags/20_context/childes\n",
      "['[CLS]', '[chi]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', '[cgv]', 'back', 'on', 'the', 'table', 'if', 'you', 'wanna', 'finish', 'it', '.', '[SEP]']\n",
      "all/all/no_tags/20_context/childes\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_optimal_beta_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1dc9422ed855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mthis_model_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0moptimal_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_optimal_beta_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_model_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_model_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     this_scoring = sample_across_models.sample_across_models([test_idx], \n\u001b[1;32m     30\u001b[0m         success_utts, yyy_utts, all_tokens_phono, models, initial_vocab, cmu_in_initial_vocab, beta_values=[optimal_beta])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_optimal_beta_value' is not defined"
     ]
    }
   ],
   "source": [
    "all_models = load_models.get_model_dict()\n",
    "\n",
    "# CDL + Context +/- 20 is needed\n",
    "# BERT + Context +/- 20 is needed\n",
    "# Childes on train data.\n",
    "\n",
    "# How to load properly with sample across models?\n",
    "which_models = [\n",
    "    'all/all/no_tags/20_context/childes',\n",
    "    'all/all/no_tags/20_context/bert',\n",
    "    'all/all/no_tags/0_context/data_unigram',\n",
    "]\n",
    "\n",
    "test_idx = 16928243\n",
    "\n",
    "scores_across_models = []\n",
    "\n",
    "for model_id in which_models:\n",
    "\n",
    "    print(model_id)\n",
    "    \n",
    "    args_extract = model_id.split('/')\n",
    "    this_split, this_dataset = args_extract[0], args_extract[1]\n",
    "    this_model_type = args_extract[-1]\n",
    "    \n",
    "    this_model_dict = all_models[model_id]\n",
    "    \n",
    "    optimal_beta = get_optimal_beta_value(this_split, this_dataset, this_model_dict, this_model_type)\n",
    "    this_scoring = sample_across_models.sample_across_models([test_idx], \n",
    "        success_utts, yyy_utts, all_tokens_phono, models, initial_vocab, cmu_in_initial_vocab, beta_values=[optimal_beta])\n",
    "    \n",
    "    scores_across_models.append(this_scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Written to match load_models.query_model_title\n",
    "default_args = {\n",
    "    'split' : 'all',\n",
    "    'dataset' : 'all', \n",
    "    'is_tags' : False,\n",
    "    'context_num' : 20,\n",
    "}\n",
    "\n",
    "# Need to rename childes unigram -> to just unigram\n",
    "\n",
    "childes_all_title = load_models.query_model_title(model_type = 'childes', **default_args)\n",
    "adult_all_title = load_models.query_model_title(model_type = 'adult', **default_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_example = scores_across_models.loc[(scores_across_models.model == childes_all_title) &\n",
    "    (scores_across_models.token == 'read')][['model','highest_posterior_words','highest_posterior_probabilities',\n",
    "    'highest_prior_words','highest_prior_probabilities', 'prior_surprisal','token']]\n",
    "success_example\n",
    "\n",
    "words = success_example.iloc[0].highest_prior_words.split(' ')\n",
    "probs = [float(x) for x in success_example.iloc[0].highest_prior_probabilities.split(' ')]\n",
    "' '.join([words[i]+' ('+str(np.round(probs[i],2))+')' for i in range(len(words))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_example = scores_across_models.loc[(scores_across_models.model == childes_all_title) &\n",
    "    (scores_across_models.token == 'read')][['model','highest_posterior_words','highest_posterior_probabilities',\n",
    "    'highest_prior_words','highest_prior_probabilities', 'prior_surprisal','token']]\n",
    "success_example\n",
    "\n",
    "words = success_example.iloc[0].highest_posterior_words.split(' ')\n",
    "probs = [float(x) for x in success_example.iloc[0].highest_posterior_probabilities.split(' ')]\n",
    "' '.join([words[i]+' ('+str(np.round(probs[i],2))+')' for i in range(len(words))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_example = scores_across_models.loc[(scores_across_models.model == adult_all_title) &\n",
    "    (scores_across_models.token == 'read')][['model','highest_posterior_words','highest_posterior_probabilities',\n",
    "    'highest_prior_words','highest_prior_probabilities', 'prior_surprisal','token']]\n",
    "success_example\n",
    "\n",
    "words = success_example.iloc[0].highest_prior_words.split(' ')\n",
    "probs = [float(x) for x in success_example.iloc[0].highest_prior_probabilities.split(' ')]\n",
    "' '.join([words[i]+' ('+str(np.round(probs[i],2))+')' for i in range(len(words))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = success_example.iloc[0].highest_posterior_words.split(' ')\n",
    "probs = [float(x) for x in success_example.iloc[0].highest_posterior_probabilities.split(' ')]\n",
    "' '.join([words[i]+' ('+str(np.round(probs[i],2))+')' for i in range(len(words))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_example = scores_across_models.loc[(scores_across_models.model == unigram_title) &\n",
    "    (scores_across_models.token == 'read')][['model','highest_posterior_words','highest_posterior_probabilities',\n",
    "    'highest_prior_words','highest_prior_probabilities', 'prior_surprisal','token']]\n",
    "success_example\n",
    "\n",
    "words = success_example.iloc[0].highest_prior_words.split(' ')\n",
    "probs = [float(x) for x in success_example.iloc[0].highest_prior_probabilities.split(' ')]\n",
    "' '.join([words[i]+' ('+str(np.round(probs[i],2))+')' for i in range(len(words))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = success_example.iloc[0].highest_posterior_words.split(' ')\n",
    "probs = [float(x) for x in success_example.iloc[0].highest_posterior_probabilities.split(' ')]\n",
    "' '.join([words[i]+' ('+str(np.round(probs[i],2))+')' for i in range(len(words))]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New extraction section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " utt_glosses.loc[(utt_glosses.transcript_id == target_transcript_id) &\n",
    "                (utt_glosses.utterance_order.isin(range(310-2, 310+2)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy_utts_with_gloss = yyy_utts.merge(utt_glosses[['id','gloss']], left_on=['utterance_id'], right_on=['id'])\n",
    "yyy_utts_with_gloss['num_tokens'] = [len(x.split(' ')) for x in yyy_utts_with_gloss['gloss']]\n",
    "' | '.join(yyy_utts_with_gloss.loc[success_utts_with_gloss.num_tokens ==4 ].gloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = yyy_utts_with_gloss.loc[yyy_utts_with_gloss.gloss=='you make your yyy'].utterance_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_phono.loc[all_tokens_phono.id.isin(selected)][['gloss','actual_phonology_no_dia',\n",
    " 'model_phonology_no_dia', 'id','bert_token_id','utterance_order','transcript_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_across_models = sample_across_models.sample_across_models( [16813515], success_utts,\n",
    "    yyy_utts, all_tokens_phono, models, initial_vocab, cmu_in_initial_vocab, beta_values=[3.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is 16813515 already available in the models? Because you will have to refactor to have it be per model, calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_across_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy_example = scores_across_models.loc[(scores_across_models.model == 'CHILDES BERT, 20 utts context')][['model','highest_posterior_words','highest_posterior_probabilities',\n",
    "    'highest_prior_words','highest_prior_probabilities', 'prior_surprisal','token']]\n",
    "yyy_example\n",
    "\n",
    "words = yyy_example.iloc[0].highest_prior_words.split(' ')\n",
    "probs = [float(x) for x in yyy_example.iloc[0].highest_prior_probabilities.split(' ')]\n",
    "' '.join([words[i]+' ('+str(np.round(probs[i],2))+')' for i in range(len(words))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = yyy_example.iloc[0].highest_posterior_words.split(' ')\n",
    "probs = [float(x) for x in yyy_example.iloc[0].highest_posterior_probabilities.split(' ')]\n",
    "' '.join([words[i]+' ('+str(np.round(probs[i],2))+')' for i in range(len(words))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy_example = scores_across_models.loc[(scores_across_models.model == 'Adult BERT, 20 utts context')][['model','highest_posterior_words','highest_posterior_probabilities',\n",
    "    'highest_prior_words','highest_prior_probabilities', 'prior_surprisal','token']]\n",
    "yyy_example\n",
    "\n",
    "words = yyy_example.iloc[0].highest_prior_words.split(' ')\n",
    "probs = [float(x) for x in yyy_example.iloc[0].highest_prior_probabilities.split(' ')]\n",
    "' '.join([words[i]+' ('+str(np.round(probs[i],2))+')' for i in range(len(words))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = yyy_example.iloc[0].highest_posterior_words.split(' ')\n",
    "probs = [float(x) for x in yyy_example.iloc[0].highest_posterior_probabilities.split(' ')]\n",
    "' '.join([words[i]+' ('+str(np.round(probs[i],2))+')' for i in range(len(words))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy_example = scores_across_models.loc[(scores_across_models.model == 'CHILDES Unigram')][['model','highest_posterior_words','highest_posterior_probabilities',\n",
    "    'highest_prior_words','highest_prior_probabilities', 'prior_surprisal','token']]\n",
    "yyy_example\n",
    "\n",
    "words = yyy_example.iloc[0].highest_prior_words.split(' ')\n",
    "probs = [float(x) for x in yyy_example.iloc[0].highest_prior_probabilities.split(' ')]\n",
    "' '.join([words[i]+' ('+str(np.round(probs[i],2))+')' for i in range(len(words))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = yyy_example.iloc[0].highest_posterior_words.split(' ')\n",
    "probs = [float(x) for x in yyy_example.iloc[0].highest_posterior_probabilities.split(' ')]\n",
    "' '.join([words[i]+' ('+str(np.round(probs[i],2))+')' for i in range(len(words))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " utt_glosses.loc[(utt_glosses.transcript_id == 42253) &\n",
    "                (utt_glosses.utterance_order.isin(range(112-3, 112+3)))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "child-listening-env",
   "language": "python",
   "name": "child-listening-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
