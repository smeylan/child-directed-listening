{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import childespy\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os.path import join, exists\n",
    "np.random.seed(0)\n",
    "\n",
    "# Important: Run this cell only once per \"restart runtime\"\n",
    "# for reproducibility of random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import data_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## General description of splits (this notebook addresses the \"all\" and \"age\" splits)\n",
    " \n",
    " **All split**\n",
    "    \n",
    "- The normal train and validation from the original query on CHILDES in \"Generate data to fine-tune\" notebook.\n",
    "- Train, Val chosen 80/20 randomly using unique transcript IDs only.\n",
    "- Doesn't have xxx or yyy.\n",
    "\n",
    "**Age split**\n",
    "\n",
    "- Split on <=36 months for young, and rest for old.\n",
    "- Do all split process.\n",
    "- Doesn't have xxx or yyy.\n",
    "\n",
    "**Child split**\n",
    "- Split on 6 children as found in Generate Phonological Analysis. See notebook \"Generate child data for finetuning\".\n",
    "- Has 200 yyy and 200 successes for validation, randomly select from UNIQUE TRANSCRIPT IDS ONLY (you need to ensure this in the code). The rest is train.\n",
    "- Doesn't have xxx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Train, Val for All and Age splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all of the North American and British English adult and child utterances without xxx or yyy\n",
    "#concatenate them at the the transcript level\n",
    "#hold out 20% for validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using current database version: '2020.1'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>collection_name</th>\n",
       "      <th>data_source</th>\n",
       "      <th>collection_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>Garvey</td>\n",
       "      <td>Eng-NA</td>\n",
       "      <td>CHILDES</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>Valian</td>\n",
       "      <td>Eng-NA</td>\n",
       "      <td>CHILDES</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>Bernstein</td>\n",
       "      <td>Eng-NA</td>\n",
       "      <td>CHILDES</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>Clark</td>\n",
       "      <td>Eng-NA</td>\n",
       "      <td>CHILDES</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36</td>\n",
       "      <td>PetersonMcCabe</td>\n",
       "      <td>Eng-NA</td>\n",
       "      <td>CHILDES</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>221</td>\n",
       "      <td>Wells</td>\n",
       "      <td>Eng-UK</td>\n",
       "      <td>CHILDES</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>222</td>\n",
       "      <td>Gathburn</td>\n",
       "      <td>Eng-UK</td>\n",
       "      <td>CHILDES</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>223</td>\n",
       "      <td>Nuffield</td>\n",
       "      <td>Eng-UK</td>\n",
       "      <td>CHILDES</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>224</td>\n",
       "      <td>Lara</td>\n",
       "      <td>Eng-UK</td>\n",
       "      <td>CHILDES</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>225</td>\n",
       "      <td>Belfast</td>\n",
       "      <td>Eng-UK</td>\n",
       "      <td>CHILDES</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id            name collection_name data_source  collection_id\n",
       "1    32          Garvey          Eng-NA     CHILDES              2\n",
       "2    33          Valian          Eng-NA     CHILDES              2\n",
       "3    34       Bernstein          Eng-NA     CHILDES              2\n",
       "4    35           Clark          Eng-NA     CHILDES              2\n",
       "5    36  PetersonMcCabe          Eng-NA     CHILDES              2\n",
       "..  ...             ...             ...         ...            ...\n",
       "57  221           Wells          Eng-UK     CHILDES             12\n",
       "58  222        Gathburn          Eng-UK     CHILDES             12\n",
       "59  223        Nuffield          Eng-UK     CHILDES             12\n",
       "60  224            Lara          Eng-UK     CHILDES             12\n",
       "61  225         Belfast          Eng-UK     CHILDES             12\n",
       "\n",
       "[61 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora = childespy.get_sql_query('select * from corpus where \\\n",
    "collection_name in (\"Eng-NA\", \"Eng-UK\") and data_source = \"CHILDES\"')\n",
    "corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "childes_datasets = \",\".join([str(x) for x in corpora.collection_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate = False #True\n",
    "if regenerate:\n",
    "    utt_glosses = childespy.get_sql_query('select gloss, transcript_id, id, \\\n",
    "    utterance_order, speaker_code, target_child_name, target_child_age, type from utterance where collection_name in (\"Eng-NA\", \"Eng-UK\") \\\n",
    "    and collection_id in ('+childes_datasets+') and speaker_code in (\"MOT\", \"FAT\",\"CHI\")' , db_version = \"2020.1\")\n",
    "    utt_glosses.to_csv('csv/utt_glosses.csv', index=False)\n",
    "else: \n",
    "    utt_glosses = pd.read_csv('csv/utt_glosses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.data_gen' from '/home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/utils/data_gen.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from utils import data_gen\n",
    "importlib.reload(data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_split_gen(raw_data, split_name, dataset_name, base_dir, verbose = False):\n",
    "    \n",
    "    if not exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "    \n",
    "    print('Beginning split gen call:', split_name, dataset_name)\n",
    "    \n",
    "    fill_punct_val = None # For behavior of finetune notebook.\n",
    "    cleaned_utt_glosses = data_gen.prep_utt_glosses_for_split(utt_glosses, fill_punct_val = fill_punct_val,\n",
    "                                                              verbose = True)\n",
    "    \n",
    "    # Be careful of saving cleaned_utt_glosses -- it will convert the 'tokens' attribute to a string,\n",
    "    # so it can't be used directly with save_vocab or token frequencies afterwards.\n",
    "    \n",
    "    tok_freq, chi_tok_freq = data_gen.save_vocab(cleaned_utt_glosses, 'all', 'all', base_dir)\n",
    "\n",
    "    split_glosses_df = data_gen.split_glosses_shuffle(cleaned_utt_glosses, 'all', 'all', base_dir, val_ratio = 0.8)\n",
    "    \n",
    "    return split_glosses_df, tok_freq, chi_tok_freq\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"all\" split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning split gen call: all all\n",
      "Cell 232 output (4205071, 7)\n",
      "Cell 233 output (3959952, 7)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-37adfdc5ac78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mall_utt_glosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutt_glosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Important because data_gen functions often mutate, not copy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mall_split_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexec_split_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_utt_glosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# expectations for verbose output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ca794b73a47b>\u001b[0m in \u001b[0;36mexec_split_gen\u001b[0;34m(raw_data, split_name, dataset_name, base_dir, verbose)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfill_punct_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# For behavior of finetune notebook.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     cleaned_utt_glosses = data_gen.prep_utt_glosses_for_split(utt_glosses, fill_punct_val = fill_punct_val,\n\u001b[0;32m---> 10\u001b[0;31m                                                               verbose = True)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcleaned_utt_glosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cleaned_utt_glosses_my.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chompsky/childes/child_listening_continuation/child-directed-listening/utils/data_gen.py\u001b[0m in \u001b[0;36mprep_utt_glosses_for_split\u001b[0;34m(data, fill_punct_val, verbose)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cell 233 output'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_glosses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cell 269'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgloss_with_punct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chompsky/childes/child_listening_continuation/child-directed-listening/utils/data_gen.py\u001b[0m in \u001b[0;36mclean_glosses\u001b[0;34m(data, fill_punct_val, verbose)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# Cell 237\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     data['punct'] = [punct_for_type[x] if x in punct_for_type else fill_punct_val\n\u001b[0;32m--> 122\u001b[0;31m                         for x in data.type ]\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cell 238'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chompsky/childes/child_listening_continuation/child-directed-listening/utils/data_gen.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# Cell 237\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     data['punct'] = [punct_for_type[x] if x in punct_for_type else fill_punct_val\n\u001b[0m\u001b[1;32m    122\u001b[0m                         for x in data.type ]\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For now maintain in separate cells because you want to check things, can merge into a single dictionary later.\n",
    "# This is kind of slow -- how to monitor progress? Probably because of all of the copies\n",
    "\n",
    "import importlib\n",
    "importlib.reload(data_gen)\n",
    "\n",
    "base_path = 'refactored'\n",
    "\n",
    "all_utt_glosses = utt_glosses.copy() # Important because data_gen functions often mutate, not copy.\n",
    "all_split_df, _, _ = exec_split_gen(all_utt_glosses, 'all', 'all', base_path)\n",
    "\n",
    "# expectations for verbose output.\n",
    "# 232 should be (4205071, 7) (because you added two extra fields)\n",
    "# 233 likewise is (3959952, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"age\" split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the original utterances by age first. \n",
    "# Split point is 36 months\n",
    "\n",
    "mask = utt_glosses['target_child_age'] <= 36 * 30.5\n",
    "\n",
    "# Implied that target_child_age is in days,\n",
    "# and 30.5 days/month is used in the original Generalized Phonological analysis.\n",
    "\n",
    "young_df = utt_glosses[mask].copy()\n",
    "old_df = utt_glosses[~mask].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#young_split_df, _, _ = exec_split_gen(young_df, 'age', 'young')\n",
    "#old_split_df, _, _ = exec_split_gen(old_df, 'age', 'old')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick checks for consistency with the full notebook (on \"all\" split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The fix gloss should now match?\n",
    "\n",
    "import os\n",
    "from os.path import join, exists\n",
    "\n",
    "# Need to check where are the differences in pandas dataframes?\n",
    "\n",
    "retrieve_path = data_gen.get_split_folder('all', 'all', base_path)\n",
    "tok = pd.read_csv(join(retrieve_path, 'vocab.csv'))\n",
    "chi_tok = pd.read_csv(join(retrieve_path, 'chi_vocab.csv'))\n",
    "\n",
    "# This was definitely re-run from scratch. You need to check your vocab.csv code? What is the generation of the tokens like?\n",
    "\n",
    "orig_tok = pd.read_csv('data/vocab.csv') # Saved by the original notebook in your run.\n",
    "orig_chi_tok = pd.read_csv('data/chi_vocab.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nan_filler = 'is_an_nan_need_to_fill'\n",
    "orig_tok_sorted = orig_tok.sort_values('word').reset_index(drop = True).fillna(nan_filler)\n",
    "tok_sorted = tok.sort_values('word').reset_index(drop = True).fillna(nan_filler)\n",
    "\n",
    "orig_tok_sorted.equals(tok_sorted)\n",
    "\n",
    "orig_tok_sorted.head()\n",
    "\n",
    "assert all(orig_tok_sorted['word'] == tok_sorted['word'])\n",
    "assert all(orig_tok_sorted['count'] == tok_sorted['count'])\n",
    "\n",
    "# Note that if you compare the dataframes directly, like:\n",
    "# print(orig_tok_sorted[orig_tok_sorted != tok_sorted])\n",
    "# print(tok_sorted[orig_tok_sorted != tok_sorted])\n",
    "# This is not going to work because of the Unnamed: 0 column. But the actual contents are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking if the dataframes are the same after the first cleaning\n",
    "\n",
    "base_path = 'refactored'\n",
    "\n",
    "my_out = pd.read_csv(join(base_path, 'cleaned_utt_glosses_my.csv'))\n",
    "his_out = pd.read_csv('data/cleaned_utt_glosses_meylan.csv')\n",
    "\n",
    "my_out.sort_values('id')\n",
    "his_out.sort_values('id')\n",
    "\n",
    "filler = 'fill_this_nan______'\n",
    "\n",
    "my_out = my_out.fillna(filler)\n",
    "his_out = his_out.fillna(filler)\n",
    "\n",
    "for field in my_out.columns:\n",
    "    if field not in his_out.columns: continue\n",
    "    if not all(my_out[field] == his_out[field]):\n",
    "        print(field)\n",
    "        print(my_out[my_out[field] != his_out[field]])\n",
    "        break # Why?\n",
    "        \n",
    "    # Seems like they are actually the same\n",
    "    # So long as \"NaN\" glosses are filled with some value to be equivalent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "child-listening-env",
   "language": "python",
   "name": "child-listening-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
