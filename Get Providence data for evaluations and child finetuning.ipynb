{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepares Providence data\n",
    "\n",
    "Evaluation set for all and age splits.\n",
    "\n",
    "Both the finetuning and evaluation set (split) for child finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# 7/22/21: https://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# end cite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "import rpy2.robjects.lib.ggplot2 as ggplot2\n",
    "import childespy\n",
    "import numpy as np\n",
    "import os\n",
    "import imp\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import re\n",
    "import unicodedata\n",
    "import scipy.stats\n",
    "import copy\n",
    "from string import punctuation\n",
    "\n",
    "import config\n",
    "np.random.seed(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import split_gen, data_cleaning, load_splits, load_models, data_cleaning, transformers_bert_completions\n",
    "from utils_child import child_split_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and clean the source of individual utterance samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communicative success: how many no-xxx, no-yyy child  utterances are in Providence? \n",
    "# Communicative failures: how many one-yyy, no-xxx child utterances are in Providence?\n",
    "# Subset to instances that are monosyllabic later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using current database version: '2020.1'.\n",
      "\n",
      "R[write to console]: Using supported database version: '2020.1'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pvd_idx = childespy.get_sql_query('select * from corpus where name = \"Providence\"').iloc[0]['id']\n",
    "\n",
    "phono_glosses = childespy.get_sql_query('select gloss, target_child_name, target_child_age, \\\n",
    "    speaker_code, actual_phonology, model_phonology, transcript_id, utterance_id, \\\n",
    "    token_order, corpus_name, collection_name, language from token where \\\n",
    "    actual_phonology != \"\" and model_phonology != \"\" and collection_name = \"Eng-NA\" \\\n",
    "    and corpus_id = '+str(pvd_idx) ,\n",
    "        db_version = \"2020.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providence    396621\n",
      "Name: corpus_name, dtype: int64\n",
      "*           26736\n",
      "ə              10\n",
      "(.)             7\n",
      "aɪ              4\n",
      "pitched         2\n",
      "            ...  \n",
      "dulɪ            1\n",
      "vɛnt            1\n",
      "neɪniwɑə        1\n",
      "mɪ              1\n",
      "bi              1\n",
      "Name: actual_phonology, Length: 76, dtype: int64\n",
      "ɛ         3206\n",
      "ʌ         2132\n",
      "ɪ         1881\n",
      "ə          512\n",
      "o          507\n",
      "          ... \n",
      "paʊs         1\n",
      "pʰɑkə        1\n",
      "fɔləbi       1\n",
      "ɪɛː          1\n",
      "aɪsɛ         1\n",
      "Name: actual_phonology, Length: 30293, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if config.verbose: \n",
    "    print(phono_glosses.corpus_name.value_counts())\n",
    "    print(phono_glosses.loc[phono_glosses.gloss == 'xxx'].actual_phonology.value_counts())\n",
    "    print(phono_glosses.loc[phono_glosses.gloss == 'yyy'].actual_phonology.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_phono = phono_glosses.loc[(phono_glosses.speaker_code == 'CHI') & \n",
    "    (phono_glosses.target_child_age < (365*5))]\n",
    "\n",
    "def count_transmission_errors(utt_vector, error_codes):\n",
    "    return(np.sum([x in error_codes for x in  utt_vector]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31457, 3)\n",
      "(83880, 3)\n"
     ]
    }
   ],
   "source": [
    "xxxs_per_utt = chi_phono.groupby('utterance_id').gloss.agg(\n",
    "    lambda x: count_transmission_errors(x, ['xxx'])).reset_index()\n",
    "xxxs_per_utt.columns = ['utterance_id', 'num_xxx']\n",
    "yyys_per_utt = chi_phono.groupby('utterance_id').gloss.agg(\n",
    "    lambda x: count_transmission_errors(x, ['yyy'])).reset_index()\n",
    "yyys_per_utt.columns = ['utterance_id', 'num_yyy']\n",
    "failures_per_utt = xxxs_per_utt.merge(yyys_per_utt)\n",
    "\n",
    "raw_yyy_utts = failures_per_utt.loc[(failures_per_utt.num_xxx == 0) &  (failures_per_utt.num_yyy == 1)]\n",
    "\n",
    "if config.verbose: print(raw_yyy_utts.shape)\n",
    "\n",
    "raw_success_utts = failures_per_utt.loc[(failures_per_utt.num_xxx == 0) &  \n",
    "    (failures_per_utt.num_yyy == 0)]\n",
    "\n",
    "if config.verbose: print(raw_success_utts.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and clean Providence data \n",
    "\n",
    "Corresponds to: 4 | Prep Utterances / Tokens for BERT,\n",
    "    in the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using current database version: '2020.1'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the index of the Providence corpus\n",
    "pvd_idx = childespy.get_sql_query('select * from corpus where name = \"Providence\"').iloc[0]['id']\n",
    "\n",
    "# Load utterances from the Providence corpus from childs-db\n",
    "\n",
    "if config.regenerate:\n",
    "    raw_utt_glosses = childespy.get_sql_query('select gloss, transcript_id, id, \\\n",
    "    utterance_order, speaker_code, target_child_name, target_child_age, type from utterance where corpus_id = '+str(pvd_idx) ,\n",
    "        db_version = \"2020.1\")\n",
    "    raw_utt_glosses.to_csv('csv/pvd_utt_glosses.csv', index=False)\n",
    "else: \n",
    "    raw_utt_glosses = pd.read_csv('csv/pvd_utt_glosses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "declarative                   335678\n",
      "question                       84707\n",
      "imperative_emphatic            15954\n",
      "trail off                      12351\n",
      "self interruption               6658\n",
      "interruption                    2928\n",
      "self interruption question       825\n",
      "trail off question               650\n",
      "interruption question            304\n",
      "quotation precedes                 3\n",
      "question exclamation               2\n",
      "broken for coding                  1\n",
      "Name: type, dtype: int64\n",
      "Cell 238 gloss                where do you want me to go\n",
      "transcript_id                             42204\n",
      "id                                     16759250\n",
      "utterance_order                               1\n",
      "speaker_code                                OPE\n",
      "target_child_name                          Alex\n",
      "target_child_age                          514.0\n",
      "type                                   question\n",
      "punct                                         ?\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for_chi_phono_utts = raw_utt_glosses.copy() # Avoid cleaning the glosses for the utt_glosses twice (see prep code for child splits)\n",
    "utt_glosses = data_cleaning.clean_glosses(for_chi_phono_utts)\n",
    "\n",
    "if config.verbose: utt_glosses[utt_glosses.id == 17280964]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7904, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cmu_2syl_inchildes = load_models.get_cmu_dict_info()\n",
    "\n",
    "# tokenize with the most extensive tokenizer, which is the one used for model #2\n",
    "\n",
    "initial_tokenizer = load_models.get_meylan_original_model(with_tags = True)['tokenizer']\n",
    "\n",
    "initial_tokenizer.add_tokens(['yyy','xxx']) #must maintain xxx and yyy for alignment,\n",
    "# otherwwise, BERT tokenizer will try to separate these into x #x and #x and y #y #y\n",
    "inital_vocab_mask, initial_vocab = transformers_bert_completions.get_softmax_mask(initial_tokenizer,\n",
    "    cmu_2syl_inchildes.word)\n",
    "\n",
    "# confirm yyy treated as a separate character\n",
    "assert initial_tokenizer.tokenize('this is a yyy.') == ['this', 'is', 'a', 'yyy', '.']\n",
    "\n",
    "cmu_in_initial_vocab = cmu_2syl_inchildes.loc[cmu_2syl_inchildes.word.isin(initial_vocab)]\n",
    "\n",
    "if config.verbose: print(cmu_in_initial_vocab.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  token        id                                         gloss  \\\n",
      "0           0  [cgv]  16759250                    where do you want me to go   \n",
      "1           1  where  16759250                    where do you want me to go   \n",
      "2           2     do  16759250                    where do you want me to go   \n",
      "3           3    you  16759250                    where do you want me to go   \n",
      "4           4   want  16759250                    where do you want me to go   \n",
      "5           5     me  16759250                    where do you want me to go   \n",
      "6           6     to  16759250                    where do you want me to go   \n",
      "7           7     go  16759250                    where do you want me to go   \n",
      "8           8      ?  16759250                    where do you want me to go   \n",
      "9           9  [cgv]  16759261  anywhere you'll feel comfortable um anywhere   \n",
      "\n",
      "   transcript_id  utterance_order speaker_code target_child_name  \\\n",
      "0          42204                1          OPE              Alex   \n",
      "1          42204                1          OPE              Alex   \n",
      "2          42204                1          OPE              Alex   \n",
      "3          42204                1          OPE              Alex   \n",
      "4          42204                1          OPE              Alex   \n",
      "5          42204                1          OPE              Alex   \n",
      "6          42204                1          OPE              Alex   \n",
      "7          42204                1          OPE              Alex   \n",
      "8          42204                1          OPE              Alex   \n",
      "9          42204                2          MOT              Alex   \n",
      "\n",
      "   target_child_age         type punct speaker_code_simple  \\\n",
      "0             514.0     question     ?               [CGV]   \n",
      "1             514.0     question     ?               [CGV]   \n",
      "2             514.0     question     ?               [CGV]   \n",
      "3             514.0     question     ?               [CGV]   \n",
      "4             514.0     question     ?               [CGV]   \n",
      "5             514.0     question     ?               [CGV]   \n",
      "6             514.0     question     ?               [CGV]   \n",
      "7             514.0     question     ?               [CGV]   \n",
      "8             514.0     question     ?               [CGV]   \n",
      "9             514.0  declarative     .               [CGV]   \n",
      "\n",
      "                                    gloss_with_punct  \n",
      "0                  [CGV] where do you want me to go?  \n",
      "1                  [CGV] where do you want me to go?  \n",
      "2                  [CGV] where do you want me to go?  \n",
      "3                  [CGV] where do you want me to go?  \n",
      "4                  [CGV] where do you want me to go?  \n",
      "5                  [CGV] where do you want me to go?  \n",
      "6                  [CGV] where do you want me to go?  \n",
      "7                  [CGV] where do you want me to go?  \n",
      "8                  [CGV] where do you want me to go?  \n",
      "9  [CGV] anywhere you'll feel comfortable um anyw...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build a dataframe of tokens \n",
    "# this is slow, because tokenization is slow\n",
    "def inflate (row):\n",
    "    tokens = initial_tokenizer.tokenize(row['gloss_with_punct'])\n",
    "    return(pd.DataFrame({'token':tokens, 'id':row['id']}) )\n",
    "\n",
    "if config.regenerate:\n",
    "    all_tokens = pd.concat([inflate(x) for x in utt_glosses.to_dict('records')])\n",
    "    all_tokens = all_tokens.merge(utt_glosses)\n",
    "    all_tokens.to_csv('csv/pvd_utt_glosses_inflated.csv')\n",
    "\n",
    "else:\n",
    "    all_tokens = pd.read_csv('csv/pvd_utt_glosses_inflated.csv', na_filter=False)\n",
    "\n",
    "if config.verbose: print(all_tokens.iloc[0:10])\n",
    "\n",
    "# Assign a token_id (integer in the BERT vocabulary). \n",
    "# Because these are from the tokenized utterances, there is no correpsondence \n",
    "# with childes-db token ids\n",
    "all_tokens['token_id'] = initial_tokenizer.convert_tokens_to_ids(all_tokens['token'])\n",
    "# assigns utterances a 0-indexed index column\n",
    "all_tokens['seq_utt_id'] = all_tokens['id'].astype('category').cat.codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add back IPA, syllable structure, and child ages for child productions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the token-level data, esp phonology\n",
    "\n",
    "if config.regenerate:\n",
    "\n",
    "    # get token-level information for Providence\n",
    "    pvd_chi_tokens = childespy.get_sql_query('select gloss, target_child_name, target_child_age, \\\n",
    "    speaker_code, actual_phonology, model_phonology, transcript_id, utterance_id, \\\n",
    "    token_order from token where speaker_code = \"CHI\" and corpus_id = '+str(pvd_idx),\n",
    "        db_version = \"2020.1\")\n",
    "    pvd_chi_tokens['gloss'] = [data_cleaning.fix_gloss(x) for x in pvd_chi_tokens.gloss]\n",
    "    \n",
    "    # prep the tokens generated from segmenting the utterances\n",
    "    all_tokens_test = copy.deepcopy(all_tokens) \n",
    "\n",
    "    # initialize the fields that need to be populated\n",
    "    all_tokens_test['actual_phonology'] = ''\n",
    "    all_tokens_test['model_phonology'] = ''\n",
    "    all_tokens_test['target_child_age'] = np.nan\n",
    "    \n",
    "    # get a set of unique utterances\n",
    "    _, idx = np.unique(all_tokens_test.id, return_index=True)\n",
    "    all_utt_indices = all_tokens_test.id[np.sort(idx)]\n",
    "    \n",
    "    # For fast retrieval of IPA, split pvd_chi_tokens into a dictionary\n",
    "    pvd_chi_tokens_list = pvd_chi_tokens.groupby(['utterance_id'])\n",
    "    pvd_chi_tokens_dict = dict(zip(\n",
    "        [x[0] for x in pvd_chi_tokens_list], \n",
    "        [x[1] for x in pvd_chi_tokens_list], \n",
    "    ))\n",
    "    \n",
    "    # For fast retrival of BERT tokenization\n",
    "    all_tokens_test_list = all_tokens_test.groupby(['id'])\n",
    "    all_tokens_test_dict = dict(zip(\n",
    "        [x[0] for x in all_tokens_test_list], \n",
    "        [x[1] for x in all_tokens_test_list], \n",
    "    ))\n",
    "        \n",
    "    # Augment the tokens from all_tokens with the IPA from pvd_chi_tokens \n",
    "    rvs = [] \n",
    "    utts_to_retrieve = raw_yyy_utts.utterance_id.to_list() + raw_success_utts.utterance_id.to_list()\n",
    "    i=-1\n",
    "    for utt_index in all_utt_indices: #utts_to_retrieve: #[16760331]:       \n",
    "        i+=1\n",
    "        if i % int(len(all_utt_indices) / 100) == 0:\n",
    "            print(str(np.round((i / (len(all_utt_indices)) * 100),2))+'% complete...')    \n",
    "            # should learn to use tqdm instead\n",
    "        if utt_index in utts_to_retrieve:        \n",
    "            utt_df = copy.deepcopy(all_tokens_test_dict[utt_index])\n",
    "            utt_df['model_phonology'] = transfomers_bert_completions.augment_with_ipa(\n",
    "              utt_df, pvd_chi_tokens_dict[utt_index],initial_tokenizer, 'model_phonology')\n",
    "            utt_df['actual_phonology'] = transfomers_bert_completions.augment_with_ipa(\n",
    "              utt_df, pvd_chi_tokens_dict[utt_index],initial_tokenizer, 'actual_phonology')\n",
    "            utt_df['target_child_age'] = pvd_chi_tokens_dict[utt_index].iloc[0].target_child_age    \n",
    "            rvs.append(utt_df)  \n",
    "        else:\n",
    "            rvs.append(all_tokens_test_dict[utt_index])  \n",
    "            \n",
    "    # get the resulting augmented forms back into a dataframe\n",
    "    all_tokens_phono = pd.concat(rvs)\n",
    "    \n",
    "    # add a unique identifier to the BERT tokens\n",
    "    all_tokens_phono['bert_token_id'] = range(all_tokens_phono.shape[0])\n",
    "    \n",
    "    #save the results\n",
    "    all_tokens_phono.to_pickle('csv/pvd_utt_glosses_phono_inflated.pkl')\n",
    "else:\n",
    "    all_tokens_phono = pd.read_pickle('csv/pvd_utt_glosses_phono_inflated.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IPA map\n",
    "phone_map_df = pd.read_csv('phon/phon_map_populated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          token actual_phonology model_phonology\n",
      "42        mommy              ɑmɪ           mɑmiː\n",
      "81          yyy                ʌ               *\n",
      "170         wee              wiː             wiː\n",
      "173         yyy               aʊ               *\n",
      "201         wee              wiː             wiː\n",
      "...         ...              ...             ...\n",
      "3083588  nobody           nobɑɾi        noʊbɑdiː\n",
      "3083589   hates             heɪs           heɪts\n",
      "3083594      oh                o              oʊ\n",
      "3083595     why              waɪ             waɪ\n",
      "3083596    lick              liʔ             lɪk\n",
      "\n",
      "[254517 rows x 3 columns]\n",
      "  arpa ipa c_or_v\n",
      "0   AA   ɑ      v\n",
      "1   AE   æ      v\n",
      "2   AH   ə      v\n",
      "3   AO   ɔ      v\n",
      "4   AW  aʊ      v\n"
     ]
    }
   ],
   "source": [
    "if config.verbose:\n",
    "    # Inspect the IPA\n",
    "    print(all_tokens_phono.loc[all_tokens_phono.actual_phonology != ''][['token','actual_phonology','model_phonology']])\n",
    "    print(phone_map_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phone_remap(x):\n",
    "    return(x.replace(\"ː\",\"\").replace('ʌ','ə')\n",
    ".replace('ɪ','ə').replace('ɔ','ɑ').replace('a','ɑ').replace('o','oʊ').replace('˞','').replace('ʰ',\n",
    "    ''). replace('r','ɹ')).replace('\\\\^','').replace('\\\\ ̃','').replace(' ̩','').replace('^',''\n",
    ").replace('ʙ','b').replace('(','').replace(')','').replace('.','').replace('ch','ʧ'\n",
    ").replace('c','k').replace('g','ɡ').replace('y','j').replace('ʁ','ɹ')\n",
    "\n",
    "def strip_accents(string, accents=('COMBINING ACUTE ACCENT', \n",
    "    'COMBINING GRAVE ACCENT', 'COMBINING TILDE', 'COMBINING VERTICAL LINE BELOW',\n",
    "    'COMBINING SHORT STROKE OVERLAY')):\n",
    "    accents = set(map(unicodedata.lookup, accents))\n",
    "    chars = [c for c in unicodedata.normalize('NFD', string) if c not in accents]\n",
    "    return unicodedata.normalize('NFC', ''.join(chars))\n",
    "\n",
    "cv_map = dict(zip(phone_map_df['ipa'], phone_map_df['c_or_v']))\n",
    "cv_map['o'] = 'v' \n",
    "cv_map['ɜ'] = 'v'\n",
    "cv_map['e'] = 'v'\n",
    "cv_map['ʔ'] = 'c'\n",
    "cv_map['ɾ'] = 'c'\n",
    "cv_map['ɲ'] = 'c'\n",
    "cv_map['x'] = 'c'\n",
    "cv_map['ɱ'] = 'c'\n",
    "cv_map['ɣ'] = 'c'\n",
    "\n",
    "def cv_mapper(x, cv_map):\n",
    "    try:\n",
    "        return(cv_map[x])\n",
    "    except:\n",
    "        raise ValueError(x)\n",
    "\n",
    "if config.regenerate:    \n",
    "\n",
    "    # Do the same excludes as were used to identify appropriate utterances\n",
    "    excludes = ['*','(.)','(..)', '(...)','(....)','(.....)']\n",
    "    all_tokens_phono.loc[all_tokens_phono.actual_phonology.isin(excludes),'actual_phonology'] =''\n",
    "    all_tokens_phono.loc[all_tokens_phono.actual_phonology.str.contains('V'),'actual_phonology'] =''\n",
    "    \n",
    "    # remap phonology from narrow phonetic transcription to broad phonological transcription\n",
    "    all_tokens_phono['model_phonology_clean'] = [phone_remap(x) for x in all_tokens_phono['model_phonology']]\n",
    "    all_tokens_phono['actual_phonology_clean'] = [phone_remap(x) for x in all_tokens_phono['actual_phonology']]\n",
    "\n",
    "    # remove any non-combining diacritical marks\n",
    "    all_tokens_phono['model_phonology_no_dia'] = [strip_accents(x) for x in \\\n",
    "    all_tokens_phono['model_phonology_clean']]\n",
    "    all_tokens_phono['actual_phonology_no_dia'] = [strip_accents(x) for x in \\\n",
    "    all_tokens_phono['actual_phonology_clean']]\n",
    "    \n",
    "    # Compute the number of non-contiguous vowels.\n",
    "    # slightly different than the cmu vowel computation ---\n",
    "    # because here we are computing it directly from IPA\n",
    "    all_tokens_phono['cv_raw'] = [''.join([cv_mapper(x, cv_map) for x in list(y)]) if y != '' else '' for y in all_tokens_phono['actual_phonology_no_dia']]    \n",
    "    all_tokens_phono['cv_collapsed']  = [re.sub(r'(.)\\1+', r'\\1', str(x)) if x != '' else '' for x in all_tokens_phono['cv_raw']]\n",
    "    all_tokens_phono['num_vowels'] = [np.sum(np.array(list(x)) == 'v') if x !='' else np.nan for x in all_tokens_phono['cv_collapsed']]\n",
    "    all_tokens_phono.to_pickle('csv/pvd_utt_glosses_phono_cleaned_inflated.pkl')\n",
    "else:\n",
    "    all_tokens_phono = pd.read_pickle('csv/pvd_utt_glosses_phono_cleaned_inflated.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gloss</th>\n",
       "      <th>seq_utt_id</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>0</td>\n",
       "      <td>[cgv]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>0</td>\n",
       "      <td>where</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>0</td>\n",
       "      <td>do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>0</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>0</td>\n",
       "      <td>want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>0</td>\n",
       "      <td>me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>0</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>0</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        gloss  seq_utt_id  token\n",
       "0  where do you want me to go           0  [cgv]\n",
       "1  where do you want me to go           0  where\n",
       "2  where do you want me to go           0     do\n",
       "3  where do you want me to go           0    you\n",
       "4  where do you want me to go           0   want\n",
       "5  where do you want me to go           0     me\n",
       "6  where do you want me to go           0     to\n",
       "7  where do you want me to go           0     go\n",
       "8  where do you want me to go           0      ?"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens_phono[all_tokens_phono.id == 16759250][['gloss', 'seq_utt_id', 'token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42             ɑmə\n",
      "81               ə\n",
      "170             wi\n",
      "173             ɑʊ\n",
      "201             wi\n",
      "            ...   \n",
      "3083588    noʊbɑɾi\n",
      "3083589       heəs\n",
      "3083594         oʊ\n",
      "3083595        wɑə\n",
      "3083596        liʔ\n",
      "Name: actual_phonology_no_dia, Length: 254440, dtype: object\n",
      "(3083625, 24)\n"
     ]
    }
   ],
   "source": [
    "if config.verbose:\n",
    "    # Why no actual phonology?\n",
    "    print(all_tokens_phono.loc[all_tokens_phono.actual_phonology_no_dia != '']['actual_phonology_no_dia'])\n",
    "    print(all_tokens_phono.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the tokens that can be evaluated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_utt_ids = set(raw_success_utts['utterance_id']) \n",
    "initial_vocab_set = set(initial_vocab)\n",
    "yyy_utt_ids = set(raw_yyy_utts['utterance_id'])\n",
    "all_tokens_phono['in_vocab'] = all_tokens_phono['token'].isin(initial_vocab_set)\n",
    "all_tokens_phono['success_token'] = [x in successful_utt_ids for x in \n",
    "    all_tokens_phono['id']]\n",
    "all_tokens_phono['yyy_token'] = [x in yyy_utt_ids for x in \n",
    "    all_tokens_phono['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'b' 'c' ... 'hideout' 'pudding' 'stalks']\n",
      "(3083625, 27)\n"
     ]
    }
   ],
   "source": [
    "if config.verbose:\n",
    "    print(initial_vocab)\n",
    "    print(all_tokens_phono.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the subset of success and failure utterances that have transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_phono['partition'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188212, 28)\n",
      "         token        id               gloss  transcript_id  utterance_order  \\\n",
      "42       mommy  16759315               Mommy          42204                6   \n",
      "170        wee  16759467                 wee          42204               24   \n",
      "201        wee  16759501                 wee          42204               28   \n",
      "239        woo  16759549                 woo          42204               33   \n",
      "743      ernie  16759752               Ernie          42204               58   \n",
      "...        ...       ...                 ...            ...              ...   \n",
      "3083575   help  17280891                help          42569              752   \n",
      "3083589  hates  17280946  nobody hates Simba          42569              755   \n",
      "3083594     oh  17280964   oh why lick hippo          42569              756   \n",
      "3083595    why  17280964   oh why lick hippo          42569              756   \n",
      "3083596   lick  17280964   oh why lick hippo          42569              756   \n",
      "\n",
      "        speaker_code target_child_name  target_child_age               type  \\\n",
      "42               CHI              Alex          514.0000        declarative   \n",
      "170              CHI              Alex          514.0000        declarative   \n",
      "201              CHI              Alex          514.0000        declarative   \n",
      "239              CHI              Alex          514.0000        declarative   \n",
      "743              CHI              Alex          514.0000        declarative   \n",
      "...              ...               ...               ...                ...   \n",
      "3083575          CHI           William         1212.0625        declarative   \n",
      "3083589          CHI           William         1212.0625        declarative   \n",
      "3083594          CHI           William         1212.0625  self interruption   \n",
      "3083595          CHI           William         1212.0625  self interruption   \n",
      "3083596          CHI           William         1212.0625  self interruption   \n",
      "\n",
      "        punct  ... actual_phonology_clean model_phonology_no_dia  \\\n",
      "42          .  ...                    ɑmə                   mɑmi   \n",
      "170         .  ...                     wi                     wi   \n",
      "201         .  ...                     wi                     wi   \n",
      "239         .  ...                      u                     wu   \n",
      "743         .  ...                      ɛ                   əɹni   \n",
      "...       ...  ...                    ...                    ...   \n",
      "3083575     .  ...                    ɛlp                   hɛlp   \n",
      "3083589     .  ...                   heəs                  heəts   \n",
      "3083594     .  ...                     oʊ                    oʊʊ   \n",
      "3083595     .  ...                    wɑə                    wɑə   \n",
      "3083596     .  ...                    liʔ                    lək   \n",
      "\n",
      "         actual_phonology_no_dia  cv_raw cv_collapsed num_vowels  in_vocab  \\\n",
      "42                           ɑmə     vcv          vcv        2.0      True   \n",
      "170                           wi      cv           cv        1.0      True   \n",
      "201                           wi      cv           cv        1.0      True   \n",
      "239                            u       v            v        1.0      True   \n",
      "743                            ɛ       v            v        1.0      True   \n",
      "...                          ...     ...          ...        ...       ...   \n",
      "3083575                      ɛlp     vcc           vc        1.0      True   \n",
      "3083589                     heəs    cvvc          cvc        1.0      True   \n",
      "3083594                       oʊ      vv            v        1.0      True   \n",
      "3083595                      wɑə     cvv           cv        1.0      True   \n",
      "3083596                      liʔ     cvc          cvc        1.0      True   \n",
      "\n",
      "        success_token yyy_token partition  \n",
      "42               True     False   success  \n",
      "170              True     False   success  \n",
      "201              True     False   success  \n",
      "239              True     False   success  \n",
      "743              True     False   success  \n",
      "...               ...       ...       ...  \n",
      "3083575          True     False   success  \n",
      "3083589          True     False   success  \n",
      "3083594          True     False   success  \n",
      "3083595          True     False   success  \n",
      "3083596          True     False   success  \n",
      "\n",
      "[188212 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "success_tokens = all_tokens_phono.loc[(all_tokens_phono['success_token']) & \n",
    "    (all_tokens_phono['num_vowels'] <= 2) ]\n",
    "all_tokens_phono.loc[(all_tokens_phono['success_token']) & \n",
    "    (all_tokens_phono['num_vowels'] <= 2), 'partition'] = 'success'     \n",
    "\n",
    "if config.verbose:\n",
    "    print(success_tokens.shape)\n",
    "    print(all_tokens_phono.loc[(all_tokens_phono['success_token']) & \n",
    "    (all_tokens_phono['num_vowels'] <= 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy_tokens = all_tokens_phono.loc[(all_tokens_phono['yyy_token']) & \n",
    "(all_tokens_phono['token'] == 'yyy') & (all_tokens_phono.num_vowels <= 2) ]\n",
    "all_tokens_phono.loc[(all_tokens_phono['yyy_token']) & \n",
    "(all_tokens_phono['token'] == 'yyy') & (all_tokens_phono.num_vowels <= 2),'partition'] = 'yyy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27693, 28)\n",
      "none       2867720\n",
      "success     188212\n",
      "yyy          27693\n",
      "Name: partition, dtype: int64\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "if config.verbose:\n",
    "    print(yyy_tokens.shape)\n",
    "    print(all_tokens_phono.partition.value_counts())\n",
    "    print(initial_tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_phono.loc[all_tokens_phono.token == 'xxx','token_id'] = initial_tokenizer.unk_token_id\n",
    "all_tokens_phono.loc[all_tokens_phono.token == 'yyy','token_id'] = initial_tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional section from 6 | Prevalence of Successes and Failures Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to augment successes/failures with information on age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwong/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/nwong/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing gloss df augmentation, 0.0% complete.\n",
      "Computing gloss df augmentation, 11.9218% complete.\n",
      "Computing gloss df augmentation, 23.8436% complete.\n",
      "Computing gloss df augmentation, 35.7654% complete.\n",
      "Computing gloss df augmentation, 47.6872% complete.\n",
      "Computing gloss df augmentation, 59.609% complete.\n",
      "Computing gloss df augmentation, 71.5308% complete.\n",
      "Computing gloss df augmentation, 83.4526% complete.\n",
      "Computing gloss df augmentation, 95.3743% complete.\n",
      "Computing gloss df augmentation, 0.0% complete.\n",
      "Computing gloss df augmentation, 31.7894% complete.\n",
      "Computing gloss df augmentation, 63.5789% complete.\n",
      "Computing gloss df augmentation, 95.3683% complete.\n",
      "1.5    9919\n",
      "2.0    7261\n",
      "1.0    6693\n",
      "2.5    4895\n",
      "3.0    2097\n",
      "3.5     414\n",
      "0.5     167\n",
      "4.0      11\n",
      "Name: year, dtype: int64\n",
      "2.0    22432\n",
      "2.5    21194\n",
      "1.5    16798\n",
      "3.0    12564\n",
      "1.0     6697\n",
      "3.5     3683\n",
      "4.0      379\n",
      "0.5      133\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get number of tokens per age\n",
    "# Warnings were present in the original code\n",
    "\n",
    "raw_success_utts['set'] = 'success'\n",
    "raw_yyy_utts['set'] = 'failure'\n",
    "\n",
    "utt_age = chi_phono.groupby('utterance_id').target_child_age.agg(np.unique).reset_index()\n",
    "\n",
    "# Additional attributes needed for the text split.\n",
    "utt_name = chi_phono.groupby('utterance_id').target_child_name.agg(np.unique).reset_index()\n",
    "utt_transcript = chi_phono.groupby('utterance_id').transcript_id.agg(np.unique).reset_index()\n",
    "# Manually asserted that speaker code is always CHI for all of chi_phono, so OK to set it directly to CHI later.\n",
    "\n",
    "inter_success_utts = raw_success_utts.copy()\n",
    "inter_yyy_utts = raw_yyy_utts.copy()\n",
    "\n",
    "for add_attr in [utt_age, utt_name, utt_transcript]:\n",
    "    inter_success_utts = inter_success_utts.merge(add_attr, on = 'utterance_id')\n",
    "    inter_yyy_utts = inter_yyy_utts.merge(add_attr, on = 'utterance_id')\n",
    "\n",
    "# Merge the glosses separately because they aren't the same for both the successes and the yyy.\n",
    "#Generate the glosses per utterance id\n",
    "utt_gloss_save_success = data_cleaning.gloss_df_augmentation(chi_phono, raw_success_utts.utterance_id)\n",
    "utt_gloss_save_yyy = data_cleaning.gloss_df_augmentation(chi_phono, raw_yyy_utts.utterance_id)\n",
    "    \n",
    "success_utts = inter_success_utts.merge(utt_gloss_save_success, on = 'utterance_id')\n",
    "yyy_utts = inter_yyy_utts.merge(utt_gloss_save_yyy, on = 'utterance_id')\n",
    "\n",
    "utts_with_ages = pd.concat([success_utts, yyy_utts])\n",
    "\n",
    "assert len(set(utts_with_ages['utterance_id'])) == utts_with_ages.shape[0],\\\n",
    "\"Make sure that the utterance id is a unique identifier for the observations in the yyy and success dataframes\"\n",
    "assert len(set(utt_age['utterance_id'])) == utt_age.shape[0],\\\n",
    "\"Make sure that the utterance id is a unique identifier for the observations in the utt_age dataframe\"\n",
    "\n",
    "# Changed from the original: the merged dfs don't have the same order of utterance id immediately,\n",
    "# so am now merging on utterance id\n",
    "\n",
    "utts_with_ages['year'] = .5*np.floor(utts_with_ages['target_child_age'] / (365. /2) ) \n",
    "\n",
    "if config.verbose:\n",
    "    print(utts_with_ages.loc[utts_with_ages.set == 'failure'].year.value_counts())\n",
    "    print(utts_with_ages.loc[utts_with_ages.set == 'success'].year.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% complete\n",
      "8.670244587599816% complete\n",
      "17.340489175199632% complete\n",
      "26.010733762799447% complete\n",
      "34.680978350399265% complete\n",
      "43.351222937999076% complete\n",
      "52.021467525598894% complete\n",
      "60.69171211319871% complete\n",
      "69.36195670079853% complete\n",
      "78.03220128839835% complete\n",
      "86.70244587599815% complete\n",
      "95.37269046359798% complete\n",
      "Asserts passed.\n"
     ]
    }
   ],
   "source": [
    "# A cell that acts as a check for the utt_age, utt_name augumentations.\n",
    "\n",
    "# Check that every utterance_id is matched to its right age in yyy/success dataframe\n",
    "# This is a valid method because all of the utterance IDs are unique per dataframe.\n",
    "\n",
    "for i in range(utts_with_ages.shape[0]):\n",
    "    \n",
    "    if i % 10000 == 0: print(f'{(i / utts_with_ages.shape[0]) * 100.0}% complete')\n",
    "    this_entry = utts_with_ages.iloc[i]\n",
    "    this_id = this_entry['utterance_id']\n",
    "    \n",
    "    keys_to_check = ['target_child_age', 'target_child_name', 'transcript_id', 'gloss']\n",
    "    \n",
    "    cross_entry = chi_phono[chi_phono['utterance_id'] == this_id]\n",
    "    # Why is the cross value actually still a string?\n",
    "    # Where is it converted to non-string -- is there a way to convert it to non string?\n",
    "\n",
    "    for key in keys_to_check:\n",
    "        \n",
    "        this_value = this_entry[key]\n",
    "    \n",
    "        \n",
    "        if key == 'gloss' and ' ' in this_entry['gloss']:\n",
    "            # If this utterance is multiple tokens,\n",
    "            # you will have to match across multiple entries in chi_phono and join them to make the gloss.\n",
    "            # For example, idx = 5 using utts_with_ages indexing\n",
    "            \n",
    "            # what this checks for\n",
    "            # 1) you got the right pieces of the gloss\n",
    "            # 2) they are in the right token order\n",
    "            \n",
    "            formatted_cross = list(cross_entry[key])\n",
    "            \n",
    "            assert list(cross_entry['token_order']) == list(range(1, 1 + cross_entry.shape[0])),\\\n",
    "            \"Cross entry was not sliced in ascending token order, so gloss order of words is wrong.\"\n",
    "            \n",
    "            \n",
    "            assert this_value == ' '.join(formatted_cross), f'if, at index: {i}, key: {key}, real: {this_value}, cross: {formatted_cross}'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # The item to be matched is a single value or string.\n",
    "            # Applies to everything but the multiple token gloss.\n",
    "            # If the gloss of the utt_with_ages is multiple tokens,\n",
    "            # then it will still match to multiple locations in chi_phono tokens.\n",
    "            # However, because it's not the gloss attribute itself, the child attribute\n",
    "            # should be repeated across all of those entries.\n",
    "            \n",
    "            if cross_entry.shape[0] == 1:\n",
    "                cross_single_val = cross_entry[key].item()\n",
    "            else:\n",
    "                cross_set = list(set(cross_entry[key]))\n",
    "                assert len(cross_set) == 1\n",
    "                cross_single_val = cross_set[0]\n",
    "                \n",
    "            assert this_value == cross_single_val, f'else, at index: {i}, key: {key}, real: {this_value}, cross: {cross_entry[key]}'\n",
    "\n",
    "print('Asserts passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the samples and splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/pvd_all_tokens_phono_for_eval_before_child.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-efe134af6cbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Below: For debugging only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mall_tokens_phono\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pvd_all_tokens_phono_for_eval_before_child.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# This is not the same as pvd_utt_glosses_phono_cleaned_inflated.pkl', avoid this name to avoid confusion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(self, path, compression, protocol, storage_options)\u001b[0m\n\u001b[1;32m   2866\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2867\u001b[0m             \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2868\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2869\u001b[0m         )\n\u001b[1;32m   2870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(obj, filepath_or_buffer, compression, protocol, storage_options)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     ) as handles:\n\u001b[1;32m     97\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/pvd_all_tokens_phono_for_eval_before_child.pkl'"
     ]
    }
   ],
   "source": [
    "# Partition all_tokens_phono into a \"val\" and \"eval\" phase for use in the age/old, age/young, and all/all evaluations.\n",
    "\n",
    "# Go ahead and limit all_tokens_phono to successes only.\n",
    "\n",
    "phono_pool = all_tokens_phono[all_tokens_phono.success_token | all_tokens_phono.yyy_token]\n",
    "\n",
    "split_attr = 'transcript_id'\n",
    "phono_val_idxs, phono_eval_idxs = split_gen.determine_split_idxs(phono_pool, split_attr, 0.5)\n",
    "phono_phase_data = dict()\n",
    "\n",
    "for phase, idx_set in zip(['val', 'eval'], [phono_val_idxs, phono_eval_idxs]):\n",
    "    # It's on transcript_id, not actual idx, so this is OK.\n",
    "    # all_tokens_phono will receive the val/eval phase marking where it applies.\n",
    "     \n",
    "    this_phase_data, all_tokens_phono = split_gen.assign_and_find_phase_data(phase, split_attr, idx_set, all_tokens_phono)\n",
    "    phono_phase_data[phase] = this_phase_data\n",
    "\n",
    "all_tokens_phono = data_cleaning.augment_target_child_year(all_tokens_phono)\n",
    "\n",
    "# Below: For debugging only\n",
    "all_tokens_phono.to_pickle(join(config.eval_dir, 'pvd_all_tokens_phono_for_eval_before_child.pkl'))\n",
    "\n",
    "# This is not the same as pvd_utt_glosses_phono_cleaned_inflated.pkl', avoid this name to avoid confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting age split data\n",
      "Processing all all\n",
      "{'val'}\n",
      "Resampling for: beta, all, all, age: None, phase: val\n",
      "\tbeta sample (5000, 1)\n",
      "Processing age young\n",
      "{'val'}\n",
      "Resampling for: beta, age, young, age: None, phase: val\n",
      "\tbeta sample (5000, 1)\n",
      "Processing age old\n",
      "{'val'}\n",
      "Resampling for: beta, age, old, age: None, phase: val\n",
      "\tbeta sample (5000, 1)\n",
      "successes\n",
      "{'val'}\n",
      "Resampling for: models_across_time, None, None, age: 1.0, phase: val\n",
      "age sample (3787, 1)\n",
      "failures\n",
      "{'val'}\n",
      "Resampling for: models_across_time, None, None, age: 1.0, phase: val\n",
      "age sample (3173, 1)\n",
      "successes\n",
      "{'eval'}\n",
      "Resampling for: models_across_time, None, None, age: 1.0, phase: eval\n",
      "age sample (2910, 1)\n",
      "failures\n",
      "{'eval'}\n",
      "Resampling for: models_across_time, None, None, age: 1.0, phase: eval\n",
      "age sample (3520, 1)\n",
      "successes\n",
      "{'val'}\n",
      "Resampling for: models_across_time, None, None, age: 2.0, phase: val\n",
      "age sample (5000, 1)\n",
      "failures\n",
      "{'val'}\n",
      "Resampling for: models_across_time, None, None, age: 2.0, phase: val\n",
      "age sample (3669, 1)\n",
      "successes\n",
      "{'eval'}\n",
      "Resampling for: models_across_time, None, None, age: 2.0, phase: eval\n",
      "age sample (5000, 1)\n",
      "failures\n",
      "{'eval'}\n",
      "Resampling for: models_across_time, None, None, age: 2.0, phase: eval\n",
      "age sample (3592, 1)\n",
      "successes\n",
      "{'val'}\n",
      "Resampling for: models_across_time, None, None, age: 3.0, phase: val\n",
      "age sample (5000, 1)\n",
      "failures\n",
      "{'val'}\n",
      "Resampling for: models_across_time, None, None, age: 3.0, phase: val\n",
      "age sample (1311, 1)\n",
      "successes\n",
      "{'eval'}\n",
      "Resampling for: models_across_time, None, None, age: 3.0, phase: eval\n",
      "age sample (5000, 1)\n",
      "failures\n",
      "{'eval'}\n",
      "Resampling for: models_across_time, None, None, age: 3.0, phase: eval\n",
      "age sample (786, 1)\n",
      "successes\n",
      "{'val'}\n",
      "Resampling for: models_across_time, None, None, age: 2.5, phase: val\n",
      "age sample (5000, 1)\n",
      "failures\n",
      "{'val'}\n",
      "Resampling for: models_across_time, None, None, age: 2.5, phase: val\n",
      "age sample (2458, 1)\n",
      "successes\n",
      "{'eval'}\n",
      "Resampling for: models_across_time, None, None, age: 2.5, phase: eval\n",
      "age sample (5000, 1)\n",
      "failures\n",
      "{'eval'}\n",
      "Resampling for: models_across_time, None, None, age: 2.5, phase: eval\n",
      "age sample (2437, 1)\n",
      "successes\n",
      "{'val'}\n",
      "Resampling for: models_across_time, None, None, age: 1.5, phase: val\n",
      "age sample (5000, 1)\n",
      "failures\n",
      "{'val'}\n",
      "Resampling for: models_across_time, None, None, age: 1.5, phase: val\n",
      "age sample (4531, 1)\n",
      "successes\n",
      "{'eval'}\n",
      "Resampling for: models_across_time, None, None, age: 1.5, phase: eval\n",
      "age sample (5000, 1)\n",
      "failures\n",
      "{'eval'}\n",
      "Resampling for: models_across_time, None, None, age: 1.5, phase: eval\n",
      "age sample (5000, 1)\n",
      "successes\n",
      "{'val'}\n",
      "Resampling for: models_across_time, None, None, age: 3.5, phase: val\n",
      "age sample (1333, 1)\n",
      "failures\n",
      "{'val'}\n",
      "Resampling for: models_across_time, None, None, age: 3.5, phase: val\n",
      "age sample (161, 1)\n",
      "successes\n",
      "{'eval'}\n",
      "Resampling for: models_across_time, None, None, age: 3.5, phase: eval\n",
      "age sample (2350, 1)\n",
      "failures\n",
      "{'eval'}\n",
      "Resampling for: models_across_time, None, None, age: 3.5, phase: eval\n",
      "age sample (253, 1)\n"
     ]
    }
   ],
   "source": [
    "# What to do here? Resampling for 10 for now -- in order to make sure child scripts run fine.\n",
    "\n",
    "print('Getting age split data')\n",
    "\n",
    "young_phono, old_phono = split_gen.get_age_split_data(all_tokens_phono)\n",
    "\n",
    "phono_pool = [\n",
    "    all_tokens_phono,\n",
    "    young_phono,\n",
    "    old_phono\n",
    "]\n",
    "\n",
    "model_args = [('all', 'all'), ('age', 'young'), ('age', 'old')]\n",
    "\n",
    "for (split_name, dataset_name), this_phono_raw in zip(model_args, phono_pool):\n",
    "    \n",
    "    print('Processing', split_name, dataset_name)\n",
    "    phono_phase = this_phono_raw[this_phono_raw.phase == 'val']\n",
    "\n",
    "    # age = None means don't filter on a given age\n",
    "    result_beta_sample = load_splits.sample_successes('beta', split_name, dataset_name, None, phono_phase, 'val')        \n",
    "\n",
    "    print('\\tbeta sample', result_beta_sample.shape)\n",
    "\n",
    "\n",
    "# Dropping ages 0.5 and 4.0 because of data sparsity.\n",
    "# -- for 4.0 there is only one transcript so it's not possible to do a val/eval split.\n",
    "# -- for 1.0 it's possible to have a sample size of 1 or 0, which is too unstable.\n",
    "\n",
    "used_ages = list(set(all_tokens_phono['year'].dropna()))\n",
    "for age in used_ages[1:-1]:\n",
    "    \n",
    "    for phase in ['val', 'eval']:\n",
    "        \n",
    "        for sample_func in [load_splits.sample_successes, load_splits.sample_yyy]:\n",
    "\n",
    "            if sample_func == load_splits.sample_successes:\n",
    "                print('successes')\n",
    "            else:\n",
    "                print('failures')\n",
    "\n",
    "            phono_phase = all_tokens_phono[all_tokens_phono.phase == phase]\n",
    "            this_age_sample = sample_func('models_across_time', None, None, age, phono_phase, phase)        \n",
    "\n",
    "            print('age sample', this_age_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Child work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the all_tokens_phono type information to the utterance_ids\n",
    "# So that they are accessible by success_utts/yyy_utts.\n",
    "\n",
    "# This is also the pool, actually \n",
    "# Note that filtering may be easier at this point.\n",
    "# Or some punct. won't be available\n",
    "\n",
    "id_type_pairs = list( { (t_id, t_type) for t_id, t_type in zip(all_tokens_phono['id'], all_tokens_phono['type']) } )\n",
    "visited_ids = set()\n",
    "\n",
    "id2type = dict(id_type_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check for above\n",
    "\n",
    "visited_ids = set()\n",
    "for t_id, t_type in id_type_pairs:\n",
    "    \n",
    "    # Every id corresponds to exactly one type\n",
    "    assert t_id not in visited_ids\n",
    "    visited_ids.add(t_id)\n",
    "    \n",
    "assert visited_ids == set(all_tokens_phono['id']), \"Not all ids were visited.\"\n",
    "\n",
    "print('Asserts passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disjoint assert assumptions passed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Below are checks needed to ensure that disjoint splitting scheme is truly disjoint across train/val,\n",
    "# even with separate yyy/successes assignments.\n",
    "\n",
    "unique_success_ids = set(success_utts['utterance_id'])\n",
    "unique_yyy_ids = set(yyy_utts['utterance_id'])\n",
    "\n",
    "assert len(unique_success_ids & unique_yyy_ids) == 0,\\\n",
    "\"Overlap in utterance id exists between successes and yyy utterances.\"\n",
    "assert len(unique_success_ids) == success_utts.shape[0], \"Utterance ids are not unique in success_utts dataframe.\"\n",
    "assert len(unique_yyy_ids) == yyy_utts.shape[0], \"Utterance ids are not unique in yyy_utts dataframe.\"\n",
    "\n",
    "print('Disjoint assert assumptions passed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 232 output (83880, 10)\n",
      "Cell 233 output (83880, 11)\n",
      "declarative                   67290\n",
      "question                      10044\n",
      "imperative_emphatic            3375\n",
      "trail off                      1985\n",
      "interruption                    564\n",
      "self interruption               403\n",
      "trail off question              125\n",
      "interruption question            53\n",
      "self interruption question       41\n",
      "Name: type, dtype: int64\n",
      "Cell 238 utterance_id            16759315\n",
      "num_xxx                        0\n",
      "num_yyy                        0\n",
      "set                      success\n",
      "target_child_age           514.0\n",
      "target_child_name           Alex\n",
      "transcript_id              42204\n",
      "gloss                      Mommy\n",
      "type                 declarative\n",
      "speaker_code                 CHI\n",
      "contains_error             False\n",
      "punct                          .\n",
      "Name: 0, dtype: object\n",
      "Cell 269 0    [CHI] mommy.\n",
      "1      [CHI] wee.\n",
      "2      [CHI] wee.\n",
      "3      [CHI] woo.\n",
      "4    [CHI] ernie.\n",
      "Name: gloss_with_punct, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter out any utterances that aren't in all_tokens_phono.id\n",
    "\n",
    "filtered_id = all_tokens_phono[all_tokens_phono.success_token | all_tokens_phono.yyy_token].id\n",
    "\n",
    "success_child_pool = load_splits.get_utts_from_ids(success_utts, filtered_id)\n",
    "yyy_child_pool = load_splits.get_utts_from_ids(yyy_utts, filtered_id)\n",
    "\n",
    "# You need the speaker code to use the prep_utt_glosses function\n",
    "assert all(chi_phono.speaker_code == 'CHI') # The source of all of the utts_with_ages, success_utts\n",
    "\n",
    "assign_type = lambda this_id : id2type[this_id]\n",
    "\n",
    "# Recover the punctuation.\n",
    "for df in [success_child_pool, yyy_child_pool]:\n",
    "    df['type'] = list(map(assign_type, df['utterance_id']))\n",
    "    df['speaker_code'] = ['CHI' for _ in range(df.shape[0])]\n",
    "\n",
    "child_names = set(success_child_pool['target_child_name'])\n",
    "\n",
    "split_attr = 'transcript_id'\n",
    "\n",
    "\n",
    "# Prep finetuning data for text file writing.\n",
    "success_child_pool = data_cleaning.prep_utt_glosses(success_child_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: William\n",
      "\tWriting beta samples for phase val, to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/child/William/success_utts_beta_5000_val.csv, sample size: (5000, 1)\n",
      "\tWriting beta samples for phase eval, to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/child/William/success_utts_beta_5000_eval.csv, sample size: (5000, 1)\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/William/train.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/William/train_no_tags.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/William/val.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/William/val_no_tags.txt\n",
      "Processing: Violet\n",
      "\tWriting beta samples for phase val, to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/child/Violet/success_utts_beta_5000_val.csv, sample size: (5000, 1)\n",
      "\tWriting beta samples for phase eval, to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/child/Violet/success_utts_beta_5000_eval.csv, sample size: (5000, 1)\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Violet/train.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Violet/train_no_tags.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Violet/val.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Violet/val_no_tags.txt\n",
      "Processing: Naima\n",
      "\tWriting beta samples for phase val, to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/child/Naima/success_utts_beta_5000_val.csv, sample size: (5000, 1)\n",
      "\tWriting beta samples for phase eval, to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/child/Naima/success_utts_beta_5000_eval.csv, sample size: (5000, 1)\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Naima/train.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Naima/train_no_tags.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Naima/val.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Naima/val_no_tags.txt\n",
      "Processing: Lily\n",
      "\tWriting beta samples for phase val, to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/child/Lily/success_utts_beta_5000_val.csv, sample size: (5000, 1)\n",
      "\tWriting beta samples for phase eval, to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/child/Lily/success_utts_beta_5000_eval.csv, sample size: (5000, 1)\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Lily/train.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Lily/train_no_tags.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Lily/val.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Lily/val_no_tags.txt\n",
      "Processing: Alex\n",
      "\tWriting beta samples for phase val, to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/child/Alex/success_utts_beta_5000_val.csv, sample size: (5000, 1)\n",
      "\tWriting beta samples for phase eval, to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/child/Alex/success_utts_beta_5000_eval.csv, sample size: (5000, 1)\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Alex/train.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Alex/train_no_tags.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Alex/val.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Alex/val_no_tags.txt\n",
      "Processing: Ethan\n",
      "\tWriting beta samples for phase val, to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/child/Ethan/success_utts_beta_5000_val.csv, sample size: (5000, 1)\n",
      "\tWriting beta samples for phase eval, to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/child/Ethan/success_utts_beta_5000_eval.csv, sample size: (5000, 1)\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Ethan/train.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Ethan/train_no_tags.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Ethan/val.txt\n",
      "File written to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/data/new_splits/child/Ethan/val_no_tags.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "split_attr = 'transcript_id'\n",
    "\n",
    "for name in child_names:\n",
    "    \n",
    "    print(f'Processing: {name}')\n",
    "    this_child_phono = all_tokens_phono[all_tokens_phono.target_child_name == name]\n",
    "    \n",
    "    this_success_phono = this_child_phono[this_child_phono.success_token]\n",
    "    this_yyy_phono = this_child_phono[this_child_phono.yyy_token]\n",
    "    \n",
    "    this_partition_folder = split_gen.get_split_folder('child', name, config.data_dir)\n",
    " \n",
    "    # Split successes\n",
    "    train_success_idxs_1, val_success_idxs = child_split_gen.split_train_eval_idxs(this_success_phono, split_attr, 'val')\n",
    "    train_success_idxs_2, eval_success_idxs =  child_split_gen.split_train_eval_idxs(this_success_phono, split_attr, 'eval')\n",
    "    \n",
    "    # Split yyy\n",
    "    _, val_yyy_idxs = child_split_gen.split_train_eval_idxs(this_yyy_phono, split_attr, 'val')\n",
    "    _, eval_yyy_idxs =  child_split_gen.split_train_eval_idxs(this_yyy_phono, split_attr, 'eval')\n",
    "    \n",
    "    # Combine the proper indices into their phases\n",
    "    \n",
    "    train_idxs = np.concatenate([train_success_idxs_1, train_success_idxs_2])\n",
    "    eval_idxs = np.concatenate([eval_success_idxs, eval_yyy_idxs])\n",
    "    val_idxs = np.concatenate([val_yyy_idxs, eval_yyy_idxs])\n",
    "    \n",
    "    # Write the new split information to all_tokens_phono\n",
    "    list_idxs = [train_idxs, val_idxs, eval_idxs]\n",
    "    list_child_phases = [f'{name}_{phase}' for phase in ['train', 'val', 'eval']]\n",
    "    \n",
    "    for idx_set, phase_name in zip(list_idxs, list_child_phases):\n",
    "        \n",
    "        # Make a new attribute for all_tokens_phono parallel to phase (which is the val/eval split defined above)\n",
    "        _, all_tokens_phono = split_gen.assign_and_find_phase_data(phase_name, split_attr, idx_set, all_tokens_phono, phase_label = 'phase_child')\n",
    "    \n",
    "    # Sample beta from the respective splits val/eval\n",
    "    # and not the same as the evaluation transcripts selected above.\n",
    "    \n",
    "    train_pool = all_tokens_phono[all_tokens_phono.phase_child == f'{name}_train']\n",
    "    \n",
    "    val_sample = child_split_gen.get_beta_idxs(train_pool, 'transcript_id', 'val')\n",
    "    eval_sample = child_split_gen.get_beta_idxs(train_pool, 'transcript_id', 'eval')\n",
    "    \n",
    "    \n",
    "    for phase, sample in zip(['val', 'eval'], [val_sample, eval_sample]):\n",
    "        this_path = load_splits.get_sample_path('success', 'beta', 'child', name, eval_phase = phase)\n",
    "        sample.to_csv(this_path)\n",
    "        \n",
    "        print(f'\\tWriting beta samples for phase {phase}, to {this_path}, sample size: {sample.shape}')\n",
    "    \n",
    "    # Write the train and val to the text files for training.\n",
    "    \n",
    "    val_pool = all_tokens_phono[all_tokens_phono.phase_child == f'{name}_val']\n",
    "    \n",
    "    for phase, phase_data in zip(['train', 'val'], [train_pool, val_pool]):\n",
    "        split_gen.write_partition(phase, phase_data, this_partition_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write final all_tokens_phono with all split information to the proper place.\n",
    "all_tokens_phono.to_pickle(join(config.eval_dir, 'pvd_all_tokens_phono_for_eval.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "child-listening-env",
   "language": "python",
   "name": "child-listening-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
