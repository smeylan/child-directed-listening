{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepares Providence data\n",
    "\n",
    "Evaluation set for all and age splits.\n",
    "\n",
    "Both the finetuning and evaluation set (split) for child finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "import rpy2.robjects.lib.ggplot2 as ggplot2\n",
    "import childespy\n",
    "import numpy as np\n",
    "import os\n",
    "import imp\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import re\n",
    "import unicodedata\n",
    "import scipy.stats\n",
    "import copy\n",
    "from string import punctuation\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import transfomers_bert_completions, split_gen, data_cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and clean the source of individual utterance samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communicative success: how many no-xxx, no-yyy child  utterances are in Providence? \n",
    "# Communicative failures: how many one-yyy, no-xxx child utterances are in Providence?\n",
    "# Subset to instances that are monosyllabic later\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using current database version: '2020.1'.\n",
      "\n",
      "R[write to console]: Using supported database version: '2020.1'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pvd_idx = childespy.get_sql_query('select * from corpus where name = \"Providence\"').iloc[0]['id']\n",
    "\n",
    "phono_glosses = childespy.get_sql_query('select gloss, target_child_name, target_child_age, \\\n",
    "    speaker_code, actual_phonology, model_phonology, transcript_id, utterance_id, \\\n",
    "    token_order, corpus_name, collection_name, language from token where \\\n",
    "    actual_phonology != \"\" and model_phonology != \"\" and collection_name = \"Eng-NA\" \\\n",
    "    and corpus_id = '+str(pvd_idx) ,\n",
    "        db_version = \"2020.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providence    396621\n",
      "Name: corpus_name, dtype: int64\n",
      "*       26736\n",
      "ə          10\n",
      "(.)         7\n",
      "aɪ          4\n",
      "ən          2\n",
      "        ...  \n",
      "mɪ          1\n",
      "hɛʔ         1\n",
      "kjɛ         1\n",
      "dɪ          1\n",
      "hɛjə        1\n",
      "Name: actual_phonology, Length: 76, dtype: int64\n",
      "ɛ         3206\n",
      "ʌ         2132\n",
      "ɪ         1881\n",
      "ə          512\n",
      "o          507\n",
      "          ... \n",
      "ʃævəsk       1\n",
      "tuitə        1\n",
      "mʌtɑd        1\n",
      "pɑbæ         1\n",
      "iæ̃          1\n",
      "Name: actual_phonology, Length: 30293, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if verbose: \n",
    "    print(phono_glosses.corpus_name.value_counts())\n",
    "    print(phono_glosses.loc[phono_glosses.gloss == 'xxx'].actual_phonology.value_counts())\n",
    "    print(phono_glosses.loc[phono_glosses.gloss == 'yyy'].actual_phonology.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_phono = phono_glosses.loc[(phono_glosses.speaker_code == 'CHI') & \n",
    "    (phono_glosses.target_child_age < (365*5))]\n",
    "\n",
    "def count_transmission_errors(utt_vector, error_codes):\n",
    "    return(np.sum([x in error_codes for x in  utt_vector]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gloss', 'target_child_name', 'target_child_age', 'speaker_code',\n",
       "       'actual_phonology', 'model_phonology', 'transcript_id', 'utterance_id',\n",
       "       'token_order', 'corpus_name', 'collection_name', 'language'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_phono.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31457, 3)\n",
      "(83880, 3)\n"
     ]
    }
   ],
   "source": [
    "xxxs_per_utt = chi_phono.groupby('utterance_id').gloss.agg(\n",
    "    lambda x: count_transmission_errors(x, ['xxx'])).reset_index()\n",
    "xxxs_per_utt.columns = ['utterance_id', 'num_xxx']\n",
    "yyys_per_utt = chi_phono.groupby('utterance_id').gloss.agg(\n",
    "    lambda x: count_transmission_errors(x, ['yyy'])).reset_index()\n",
    "yyys_per_utt.columns = ['utterance_id', 'num_yyy']\n",
    "failures_per_utt = xxxs_per_utt.merge(yyys_per_utt)\n",
    "\n",
    "raw_yyy_utts = failures_per_utt.loc[(failures_per_utt.num_xxx == 0) &  (failures_per_utt.num_yyy == 1)]\n",
    "\n",
    "if verbose: print(raw_yyy_utts.shape)\n",
    "\n",
    "raw_success_utts = failures_per_utt.loc[(failures_per_utt.num_xxx == 0) &  \n",
    "    (failures_per_utt.num_yyy == 0)]\n",
    "\n",
    "if verbose: print(raw_success_utts.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor below to the other notebook -- load based on cached successes instead? Unsure if there is a dependency there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214239, 12)\n",
      "1          ɑmɪ\n",
      "3          wiː\n",
      "4          wiː\n",
      "5           uː\n",
      "52           ɛ\n",
      "          ... \n",
      "396606       o\n",
      "396607     waɪ\n",
      "396608     liʔ\n",
      "396609       ɪ\n",
      "396610    hɪpo\n",
      "Name: actual_phonology, Length: 214239, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Probably move this to the other analysis notebook and use caching instead on the data.\n",
    "\n",
    "\n",
    "tokens_from_errorless_utts = chi_phono.loc[chi_phono.utterance_id.isin(raw_success_utts.utterance_id)]\n",
    "#exclude un-transcribed tokens and syllabically transcribed tokens\n",
    "excludes = ['*','(.)','(..)', '(...)','(....)','(.....)']\n",
    "tokens_from_errorless_utts = tokens_from_errorless_utts.loc[~(tokens_from_errorless_utts.actual_phonology.isin(excludes) |\n",
    "    tokens_from_errorless_utts.model_phonology.isin(excludes))]\n",
    "\n",
    "if verbose:\n",
    "    print(tokens_from_errorless_utts.shape)\n",
    "    print(tokens_from_errorless_utts.actual_phonology)\n",
    "\n",
    "# 31,457 transmission errors (from 31,457 utterances)\n",
    "# 214,239 transmission successes (from 83,880 utterances)\n",
    "# this will be further decreased later by the need to test monosyllabic forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and clean Providence data \n",
    "\n",
    "Corresponds to: 4 | Prep Utterances / Tokens for BERT,\n",
    "    in the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_models, data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using current database version: '2020.1'.\n",
      "\n",
      "R[write to console]: Using supported database version: '2020.1'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regenerate = True\n",
    "verbose = True\n",
    "\n",
    "# Get the index of the Providence corpus\n",
    "pvd_idx = childespy.get_sql_query('select * from corpus where name = \"Providence\"').iloc[0]['id']\n",
    "\n",
    "# Load utterances from the Providence corpus from childs-db\n",
    "\n",
    "if regenerate:\n",
    "    utt_glosses = childespy.get_sql_query('select gloss, transcript_id, id, \\\n",
    "    utterance_order, speaker_code, target_child_name, target_child_age, type from utterance where corpus_id = '+str(pvd_idx) ,\n",
    "        db_version = \"2020.1\")\n",
    "    utt_glosses.to_csv('csv/pvd_utt_glosses.csv', index=False)\n",
    "else: \n",
    "    utt_glosses = pd.read_csv('csv/pvd_utt_glosses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_glosses = data_cleaning.clean_glosses(utt_glosses, '.')\n",
    "\n",
    "if verbose: utt_glosses[utt_glosses.id == 17280964]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7904, 8)\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(load_models)\n",
    "\n",
    "root_dir = '/home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/'\n",
    "\n",
    "cmu_2syl_inchildes = load_models.get_cmu_dict_info(root_dir = root_dir)\n",
    "\n",
    "# tokenize with the most extensive tokenizer, which is the one used for model #2\n",
    "\n",
    "initial_tokenizer = load_models.get_meylan_original_model(with_tags = True, root_dir = root_dir)['tokenizer']\n",
    "\n",
    "initial_tokenizer.add_tokens(['yyy','xxx']) #must maintain xxx and yyy for alignment,\n",
    "# otherwwise, BERT tokenizer will try to separate these into x #x and #x and y #y #y\n",
    "inital_vocab_mask, initial_vocab = transfomers_bert_completions.get_softmax_mask(initial_tokenizer,\n",
    "    cmu_2syl_inchildes.word)\n",
    "\n",
    "# confirm yyy treated as a separate character\n",
    "assert initial_tokenizer.tokenize('this is a yyy.') == ['this', 'is', 'a', 'yyy', '.']\n",
    "\n",
    "cmu_in_initial_vocab = cmu_2syl_inchildes.loc[cmu_2syl_inchildes.word.isin(initial_vocab)]\n",
    "\n",
    "if verbose: print(cmu_in_initial_vocab.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   token        id                                         gloss  \\\n",
      "0  [cgv]  16759250                    where do you want me to go   \n",
      "1  where  16759250                    where do you want me to go   \n",
      "2     do  16759250                    where do you want me to go   \n",
      "3    you  16759250                    where do you want me to go   \n",
      "4   want  16759250                    where do you want me to go   \n",
      "5     me  16759250                    where do you want me to go   \n",
      "6     to  16759250                    where do you want me to go   \n",
      "7     go  16759250                    where do you want me to go   \n",
      "8      ?  16759250                    where do you want me to go   \n",
      "9  [cgv]  16759261  anywhere you'll feel comfortable um anywhere   \n",
      "\n",
      "   transcript_id  utterance_order speaker_code target_child_name  \\\n",
      "0          42204                1          OPE              Alex   \n",
      "1          42204                1          OPE              Alex   \n",
      "2          42204                1          OPE              Alex   \n",
      "3          42204                1          OPE              Alex   \n",
      "4          42204                1          OPE              Alex   \n",
      "5          42204                1          OPE              Alex   \n",
      "6          42204                1          OPE              Alex   \n",
      "7          42204                1          OPE              Alex   \n",
      "8          42204                1          OPE              Alex   \n",
      "9          42204                2          MOT              Alex   \n",
      "\n",
      "   target_child_age         type punct speaker_code_simple  \\\n",
      "0             514.0     question     ?               [CGV]   \n",
      "1             514.0     question     ?               [CGV]   \n",
      "2             514.0     question     ?               [CGV]   \n",
      "3             514.0     question     ?               [CGV]   \n",
      "4             514.0     question     ?               [CGV]   \n",
      "5             514.0     question     ?               [CGV]   \n",
      "6             514.0     question     ?               [CGV]   \n",
      "7             514.0     question     ?               [CGV]   \n",
      "8             514.0     question     ?               [CGV]   \n",
      "9             514.0  declarative     .               [CGV]   \n",
      "\n",
      "                                    gloss_with_punct  \n",
      "0                  [CGV] where do you want me to go?  \n",
      "1                  [CGV] where do you want me to go?  \n",
      "2                  [CGV] where do you want me to go?  \n",
      "3                  [CGV] where do you want me to go?  \n",
      "4                  [CGV] where do you want me to go?  \n",
      "5                  [CGV] where do you want me to go?  \n",
      "6                  [CGV] where do you want me to go?  \n",
      "7                  [CGV] where do you want me to go?  \n",
      "8                  [CGV] where do you want me to go?  \n",
      "9  [CGV] anywhere you'll feel comfortable um anyw...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build a dataframe of tokens \n",
    "# this is slow, because tokenization is slow\n",
    "def inflate (row):\n",
    "    tokens = initial_tokenizer.tokenize(row['gloss_with_punct'])\n",
    "    return(pd.DataFrame({'token':tokens, 'id':row['id']}) )\n",
    "\n",
    "regenerate = True\n",
    "if regenerate:\n",
    "    all_tokens = pd.concat([inflate(x) for x in utt_glosses.to_dict('records')])\n",
    "    all_tokens = all_tokens.merge(utt_glosses)\n",
    "    all_tokens.to_csv('csv/pvd_utt_glosses_inflated.csv')\n",
    "\n",
    "else:\n",
    "    all_tokens = pd.read_csv('csv/pvd_utt_glosses_inflated.csv', na_filter=False)\n",
    "\n",
    "if verbose: print(all_tokens.iloc[0:10])\n",
    "\n",
    "# Assign a token_id (integer in the BERT vocabulary). \n",
    "# Because these are from the tokenized utterances, there is no correpsondence \n",
    "# with childes-db token ids\n",
    "all_tokens['token_id'] = initial_tokenizer.convert_tokens_to_ids(all_tokens['token'])\n",
    "# assigns utterances a 0-indexed index column\n",
    "all_tokens['seq_utt_id'] = all_tokens['id'].astype('category').cat.codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add back IPA, syllable structure, and child ages for child productions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using supported database version: '2020.1'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% complete...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwong/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/pandas/core/frame.py:1554: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0% complete...\n",
      "2.0% complete...\n",
      "3.0% complete...\n",
      "4.0% complete...\n",
      "5.0% complete...\n",
      "6.0% complete...\n",
      "7.0% complete...\n",
      "8.0% complete...\n",
      "9.0% complete...\n",
      "10.0% complete...\n",
      "11.0% complete...\n",
      "12.0% complete...\n",
      "13.0% complete...\n",
      "14.0% complete...\n",
      "15.0% complete...\n",
      "16.0% complete...\n",
      "17.0% complete...\n",
      "18.0% complete...\n",
      "19.0% complete...\n",
      "20.0% complete...\n",
      "21.0% complete...\n",
      "22.0% complete...\n",
      "23.0% complete...\n",
      "24.0% complete...\n",
      "25.0% complete...\n",
      "26.0% complete...\n",
      "27.0% complete...\n",
      "28.0% complete...\n",
      "29.0% complete...\n",
      "30.0% complete...\n",
      "31.0% complete...\n",
      "32.0% complete...\n",
      "33.0% complete...\n",
      "34.0% complete...\n",
      "35.0% complete...\n",
      "36.0% complete...\n",
      "37.0% complete...\n",
      "37.99% complete...\n",
      "38.99% complete...\n",
      "39.99% complete...\n",
      "40.99% complete...\n",
      "41.99% complete...\n",
      "42.99% complete...\n",
      "43.99% complete...\n",
      "44.99% complete...\n",
      "45.99% complete...\n",
      "46.99% complete...\n",
      "47.99% complete...\n",
      "48.99% complete...\n",
      "49.99% complete...\n",
      "50.99% complete...\n",
      "51.99% complete...\n",
      "52.99% complete...\n",
      "53.99% complete...\n",
      "54.99% complete...\n",
      "55.99% complete...\n",
      "56.99% complete...\n",
      "57.99% complete...\n",
      "58.99% complete...\n",
      "59.99% complete...\n",
      "60.99% complete...\n",
      "61.99% complete...\n",
      "62.99% complete...\n",
      "63.99% complete...\n",
      "64.99% complete...\n",
      "65.99% complete...\n",
      "66.99% complete...\n"
     ]
    }
   ],
   "source": [
    "# get the token-level data, esp phonology\n",
    "\n",
    "if regenerate:\n",
    "\n",
    "    # get token-level information for Providence\n",
    "    pvd_chi_tokens = childespy.get_sql_query('select gloss, target_child_name, target_child_age, \\\n",
    "    speaker_code, actual_phonology, model_phonology, transcript_id, utterance_id, \\\n",
    "    token_order from token where speaker_code = \"CHI\" and corpus_id = '+str(pvd_idx),\n",
    "        db_version = \"2020.1\")\n",
    "    pvd_chi_tokens['gloss'] = [data_cleaning.fix_gloss(x) for x in pvd_chi_tokens.gloss]\n",
    "    \n",
    "    # prep the tokens generated from segmenting the utterances\n",
    "    all_tokens_test = copy.deepcopy(all_tokens) \n",
    "\n",
    "    # initialize the fields that need to be populated\n",
    "    all_tokens_test['actual_phonology'] = ''\n",
    "    all_tokens_test['model_phonology'] = ''\n",
    "    all_tokens_test['target_child_age'] = np.nan\n",
    "    \n",
    "    # get a set of unique utterances\n",
    "    _, idx = np.unique(all_tokens_test.id, return_index=True)\n",
    "    all_utt_indices = all_tokens_test.id[np.sort(idx)]\n",
    "    \n",
    "    # For fast retrieval of IPA, split pvd_chi_tokens into a dictionary\n",
    "    pvd_chi_tokens_list = pvd_chi_tokens.groupby(['utterance_id'])\n",
    "    pvd_chi_tokens_dict = dict(zip(\n",
    "        [x[0] for x in pvd_chi_tokens_list], \n",
    "        [x[1] for x in pvd_chi_tokens_list], \n",
    "    ))\n",
    "    \n",
    "    # For fast retrival of BERT tokenization\n",
    "    all_tokens_test_list = all_tokens_test.groupby(['id'])\n",
    "    all_tokens_test_dict = dict(zip(\n",
    "        [x[0] for x in all_tokens_test_list], \n",
    "        [x[1] for x in all_tokens_test_list], \n",
    "    ))\n",
    "        \n",
    "    # Augment the tokens from all_tokens with the IPA from pvd_chi_tokens \n",
    "    rvs = [] \n",
    "    utts_to_retrieve = raw_yyy_utts.utterance_id.to_list() + raw_success_utts.utterance_id.to_list()\n",
    "    i=-1\n",
    "    for utt_index in all_utt_indices: #utts_to_retrieve: #[16760331]:       \n",
    "        i+=1\n",
    "        if i % int(len(all_utt_indices) / 100) == 0:\n",
    "            print(str(np.round((i / (len(all_utt_indices)) * 100),2))+'% complete...')    \n",
    "            # should learn to use tqdm instead\n",
    "        if utt_index in utts_to_retrieve:        \n",
    "            utt_df = copy.deepcopy(all_tokens_test_dict[utt_index])\n",
    "            utt_df['model_phonology'] = transfomers_bert_completions.augment_with_ipa(\n",
    "              utt_df, pvd_chi_tokens_dict[utt_index],initial_tokenizer, 'model_phonology')\n",
    "            utt_df['actual_phonology'] = transfomers_bert_completions.augment_with_ipa(\n",
    "              utt_df, pvd_chi_tokens_dict[utt_index],initial_tokenizer, 'actual_phonology')\n",
    "            utt_df['target_child_age'] = pvd_chi_tokens_dict[utt_index].iloc[0].target_child_age    \n",
    "            rvs.append(utt_df)  \n",
    "        else:\n",
    "            rvs.append(all_tokens_test_dict[utt_index])  \n",
    "            \n",
    "    # get the resulting augmented forms back into a dataframe\n",
    "    all_tokens_phono = pd.concat(rvs)\n",
    "    \n",
    "    # add a unique identifier to the BERT tokens\n",
    "    all_tokens_phono['bert_token_id'] = range(all_tokens_phono.shape[0])\n",
    "    \n",
    "    #save the results\n",
    "    all_tokens_phono.to_pickle('csv/pvd_utt_glosses_phono_inflated.pkl')\n",
    "else:\n",
    "    all_tokens_phono = pd.read_pickle('csv/pvd_utt_glosses_phono_inflated.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IPA map\n",
    "phone_map_df = pd.read_csv('phon/phon_map_populated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    # Inspect the IPA\n",
    "    print(all_tokens_phono.loc[all_tokens_phono.actual_phonology != ''][['token','actual_phonology','model_phonology']])\n",
    "    print(phone_map_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phone_remap(x):\n",
    "    return(x.replace(\"ː\",\"\").replace('ʌ','ə')\n",
    ".replace('ɪ','ə').replace('ɔ','ɑ').replace('a','ɑ').replace('o','oʊ').replace('˞','').replace('ʰ',\n",
    "    ''). replace('r','ɹ')).replace('\\\\^','').replace('\\\\ ̃','').replace(' ̩','').replace('^',''\n",
    ").replace('ʙ','b').replace('(','').replace(')','').replace('.','').replace('ch','ʧ'\n",
    ").replace('c','k').replace('g','ɡ').replace('y','j').replace('ʁ','ɹ')\n",
    "\n",
    "def strip_accents(string, accents=('COMBINING ACUTE ACCENT', \n",
    "    'COMBINING GRAVE ACCENT', 'COMBINING TILDE', 'COMBINING VERTICAL LINE BELOW',\n",
    "    'COMBINING SHORT STROKE OVERLAY')):\n",
    "    accents = set(map(unicodedata.lookup, accents))\n",
    "    chars = [c for c in unicodedata.normalize('NFD', string) if c not in accents]\n",
    "    return unicodedata.normalize('NFC', ''.join(chars))\n",
    "\n",
    "cv_map = dict(zip(phone_map_df['ipa'], phone_map_df['c_or_v']))\n",
    "cv_map['o'] = 'v' \n",
    "cv_map['ɜ'] = 'v'\n",
    "cv_map['e'] = 'v'\n",
    "cv_map['ʔ'] = 'c'\n",
    "cv_map['ɾ'] = 'c'\n",
    "cv_map['ɲ'] = 'c'\n",
    "cv_map['x'] = 'c'\n",
    "cv_map['ɱ'] = 'c'\n",
    "cv_map['ɣ'] = 'c'\n",
    "\n",
    "def cv_mapper(x, cv_map):\n",
    "    try:\n",
    "        return(cv_map[x])\n",
    "    except:\n",
    "        raise ValueError(x)\n",
    "\n",
    "regenerate = True\n",
    "if regenerate:    \n",
    "\n",
    "    # Do the same excludes as were used to identify appropriate utterances\n",
    "    excludes = ['*','(.)','(..)', '(...)','(....)','(.....)']\n",
    "    all_tokens_phono.loc[all_tokens_phono.actual_phonology.isin(excludes),'actual_phonology'] =''\n",
    "    all_tokens_phono.loc[all_tokens_phono.actual_phonology.str.contains('V'),'actual_phonology'] =''\n",
    "    \n",
    "    # remap phonology from narrow phonetic transcription to broad phonological transcription\n",
    "    all_tokens_phono['model_phonology_clean'] = [phone_remap(x) for x in all_tokens_phono['model_phonology']]\n",
    "    all_tokens_phono['actual_phonology_clean'] = [phone_remap(x) for x in all_tokens_phono['actual_phonology']]\n",
    "\n",
    "    # remove any non-combining diacritical marks\n",
    "    all_tokens_phono['model_phonology_no_dia'] = [strip_accents(x) for x in \\\n",
    "    all_tokens_phono['model_phonology_clean']]\n",
    "    all_tokens_phono['actual_phonology_no_dia'] = [strip_accents(x) for x in \\\n",
    "    all_tokens_phono['actual_phonology_clean']]\n",
    "    \n",
    "    # Compute the number of non-contiguous vowels.\n",
    "    # slightly different than the cmu vowel computation ---\n",
    "    # because here we are computing it directly from IPA\n",
    "    all_tokens_phono['cv_raw'] = [''.join([cv_mapper(x, cv_map) for x in list(y)]) if y != '' else '' for y in all_tokens_phono['actual_phonology_no_dia']]    \n",
    "    all_tokens_phono['cv_collapsed']  = [re.sub(r'(.)\\1+', r'\\1', str(x)) if x != '' else '' for x in all_tokens_phono['cv_raw']]\n",
    "    all_tokens_phono['num_vowels'] = [np.sum(np.array(list(x)) == 'v') if x !='' else np.nan for x in all_tokens_phono['cv_collapsed']]\n",
    "    all_tokens_phono.to_pickle('csv/pvd_utt_glosses_phono_cleaned_inflated.pkl')\n",
    "else:\n",
    "    all_tokens_phono = pd.read_pickle('csv/pvd_utt_glosses_phono_cleaned_inflated.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    # Why no actual phonology?\n",
    "    print(all_tokens_phono.loc[all_tokens_phono.actual_phonology_no_dia != '']['actual_phonology_no_dia'])\n",
    "    print(all_tokens_phono.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the tokens that can be evaluated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_utt_ids = set(raw_success_utts['utterance_id'])\n",
    "initial_vocab_set = set(initial_vocab)\n",
    "yyy_utt_ids = set(raw_yyy_utts['utterance_id'])\n",
    "all_tokens_phono['in_vocab'] = all_tokens_phono['token'].isin(initial_vocab_set)\n",
    "all_tokens_phono['success_token'] = [x in successful_utt_ids for x in \n",
    "    all_tokens_phono['id']]\n",
    "all_tokens_phono['yyy_token'] = [x in yyy_utt_ids for x in \n",
    "    all_tokens_phono['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    print(initial_vocab)\n",
    "    print(all_tokens_phono.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the subset of success and failure utterances that have transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_phono['partition'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_tokens = all_tokens_phono.loc[(all_tokens_phono['success_token']) & \n",
    "    (all_tokens_phono['num_vowels'] <= 2) ]\n",
    "all_tokens_phono.loc[(all_tokens_phono['success_token']) & \n",
    "    (all_tokens_phono['num_vowels'] <= 2), 'partition'] = 'success'     \n",
    "\n",
    "if verbose:\n",
    "    print(success_tokens.shape)\n",
    "    print(all_tokens_phono.loc[(all_tokens_phono['success_token']) & \n",
    "    (all_tokens_phono['num_vowels'] <= 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy_tokens = all_tokens_phono.loc[(all_tokens_phono['yyy_token']) & \n",
    "(all_tokens_phono['token'] == 'yyy') & (all_tokens_phono.num_vowels <= 2) ]\n",
    "all_tokens_phono.loc[(all_tokens_phono['yyy_token']) & \n",
    "(all_tokens_phono['token'] == 'yyy') & (all_tokens_phono.num_vowels <= 2),'partition'] = 'yyy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    print(yyy_tokens.shape)\n",
    "    print(all_tokens_phono.partition.value_counts())\n",
    "    print(initial_tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional section from 6 | Prevalence of Successes and Failures Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to augment successes/failures with information on age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of tokens per age\n",
    "# Warnings were present in the original code\n",
    "\n",
    "raw_success_utts['set'] = 'success'\n",
    "raw_yyy_utts['set'] = 'failure'\n",
    "\n",
    "utt_age = chi_phono.groupby('utterance_id').target_child_age.agg(np.unique).reset_index()\n",
    "utt_name = chi_phono.groupby('utterance_id').target_child_name.agg(np.unique).reset_index()\n",
    "\n",
    "success_utts = raw_success_utts.merge(utt_age, on = 'utterance_id').merge(utt_name, on = 'utterance_id')\n",
    "yyy_utts = raw_yyy_utts.merge(utt_age, on = 'utterance_id').merge(utt_name, on = 'utterance_id')\n",
    "\n",
    "utts_with_ages = pd.concat([success_utts, yyy_utts]) # Added names for child split.\n",
    "\n",
    "assert len(set(utts_with_ages['utterance_id'])) == utts_with_ages.shape[0],\\\n",
    "\"Make sure that the utterance id is a unique identifier for the observations in the yyy and success dataframes\"\n",
    "assert len(set(utt_age['utterance_id'])) == utt_age.shape[0],\\\n",
    "\"Make sure that the utterance id is a unique identifier for the observations in the utt_age dataframe\"\n",
    "\n",
    "# Changed from the original: the merged dfs don't have the same order of utterance id immediately,\n",
    "# so am now merging on utterance id\n",
    "\n",
    "utts_with_ages['year'] = .5*np.floor(utts_with_ages['target_child_age'] / (365. /2) ) \n",
    "\n",
    "if verbose:\n",
    "    print(utts_with_ages.loc[utts_with_ages.set == 'failure'].year.value_counts())\n",
    "    print(utts_with_ages.loc[utts_with_ages.set == 'success'].year.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utts_with_ages.columns # Why is there \"target_child_age_x\" and such? Need to think about this.\n",
    "# There is an unexpected interaction... what is a better way to merge the information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A cell that acts as a check for the utt_age, utt_name augumentations.\n",
    "\n",
    "\n",
    "# Need below assumption for the name-based check.\n",
    "# Ensure that for every name in the original dataframe,\n",
    "# every utterance associated with that name\n",
    "# has the right association in the resultant dataframe,\n",
    "# and there are no extra associations in the resultant dataframe.\n",
    "\n",
    "assert len(set(utts_with_ages['utterance_id'])) == utts_with_ages.shape[0], \"utts with ages doesn't have unique IDs\"\n",
    "names = set(chi_phono['target_child_name'])\n",
    "\n",
    "for name in names:\n",
    "    \n",
    "    select_name_subset = lambda df : df[df['target_child_name'] == name]['utterance_id']\n",
    "    ids_for_name = set(select_name_subset(chi_phono))\n",
    "    utts_with_ages_ids = set(select_name_subset(utts_with_ages))\n",
    "    \n",
    "    assert ids_for_name == utts_with_ages_ids\n",
    "    \n",
    "    \n",
    "# Check that every utterance_id is matched to its right age in yyy/success dataframe\n",
    "# This is a valid method because all of the utterance IDs are unique per dataframe.\n",
    "\n",
    "for i in range(utts_with_ages.shape[0]):\n",
    "    \n",
    "    if i % 10000 == 0: print(f'{(i / utts_with_ages.shape[0]) * 100.0} complete')\n",
    "    this_entry = utts_with_ages.iloc[i]\n",
    "    this_id = this_entry['utterance_id']\n",
    "    this_age = this_entry['target_child_age']\n",
    "    \n",
    "    # Below is OK because of the assert.\n",
    "    cross_entry = utts_with_ages[utts_with_ages['utterance_id'] == this_id]\n",
    "    \n",
    "    assert cross_entry.shape[0] == 1\n",
    "    assert cross_entry['target_child_age'].item() == this_age, f'Error at index: {i}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This marks the end of the section before the model evaluations/queries beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_eval_data(data, filename, split_name, dataset_name, base_dir = 'eval/new_splits'):\n",
    "    \n",
    "    assert split_name in ['all', 'age', 'child'], \"Invalid split name. Must be one of {all, age, child}.\"\n",
    "    \n",
    "    # Saving based on a mask of a copy of a? Will this be a problem?\n",
    "    \n",
    "    save_path = split_gen.get_split_folder('all', 'all', base_dir)\n",
    "    save_location = join(save_path, filename)\n",
    "    data.to_pickle(save_location)\n",
    "    \n",
    "    print(f'Saved all/all evaluation data to {save_location}')\n",
    "    \n",
    "    return save_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phono_filename = 'pvd_utt_glosses_phono_cleaned_inflated.pkl'\n",
    "success_utts_filename = 'success_utts.csv'\n",
    "yyy_utts_filename = 'yyy_utts.csv'\n",
    "\n",
    "data_filenames = [phono_filename, success_utts_filename, yyy_utts_filename]\n",
    "\n",
    "# Use this line from the original code and load the above two CSVs for model inputs later:\n",
    "# utts_with_ages = pd.concat([success_utts, yyy_utts]).merge(utt_age)\n",
    "\n",
    "# for the input into the actual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save evaluation data for all split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for this_data, filename in zip([all_tokens_phono, success_utts, yyy_utts], data_filenames):\n",
    "    save_eval_data(this_data, filename, 'all', 'all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save evaluation data for age split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "young_tokens_phono, old_tokens_phono = split_gen.get_age_split_data(all_tokens_phono, months = 36)\n",
    "young_success_utts, old_success_utts = split_gen.get_age_split_data(success_utts, months = 36)\n",
    "young_yyy_utts, old_yyy_utts = split_gen.get_age_split_data(yyy_utts, months = 36)\n",
    "\n",
    "for this_data, filename in zip([old_tokens_phono, old_success_utts, old_yyy_utts], data_filenames):\n",
    "    save_eval_data(this_data, filename, 'age', 'old')\n",
    "    \n",
    "for this_data, filename in zip([young_tokens_phono, young_success_utts, young_yyy_utts], data_filenames):\n",
    "    save_eval_data(this_data, filename, 'age', 'young') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process finetuning and evaluation data for child split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note you'll have to restart runtime/re-gen all of this on random seed\n",
    "\n",
    "# For now, define a success as any utterance without any yyy and xxx.  \n",
    "# This is how it's defined in the implementation elswhere -- although technically the token itself should also be monosyllabic.\n",
    "\n",
    "child_names = set(success_utts['target_child_name'])\n",
    "for name in child_names:\n",
    "    child_success_utts = success_utts[success_utts['target_child_name'] == name]\n",
    "    \n",
    "    # Need to select 200 for the evaluation. How to do this per child?\n",
    "    \n",
    "    this_partition_folder = split_gen.get_split_folder('child', name, base_dir = 'data/new_splits')\n",
    "    val_idxs = split_gen.glosses_random_split(child_success_utts, val_num = 200)\n",
    "    \n",
    "    pooled_data = split_gen.write_data_partition_text(child_success_utts, this_partition_folder, val_idxs)\n",
    "    # Note: Here, partition = success or not\n",
    "    # phase = train or validation (this was changed from the original)\n",
    "    \n",
    "    print(name, child_success_utts.shape[0], 'number of examples')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "child-listening-env",
   "language": "python",
   "name": "child-listening-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
