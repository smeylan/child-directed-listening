{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepares Providence data\n",
    "\n",
    "Evaluation set for all and age splits.\n",
    "\n",
    "Both the finetuning and evaluation set (split) for child finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "import rpy2.robjects.lib.ggplot2 as ggplot2\n",
    "import childespy\n",
    "import numpy as np\n",
    "import os\n",
    "import imp\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import re\n",
    "import unicodedata\n",
    "import scipy.stats\n",
    "import copy\n",
    "from string import punctuation\n",
    "\n",
    "import config\n",
    "np.random.seed(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import split_gen, data_cleaning, load_splits, load_csvs, load_models, data_cleaning, transformers_bert_completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and clean the source of individual utterance samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communicative success: how many no-xxx, no-yyy child  utterances are in Providence? \n",
    "# Communicative failures: how many one-yyy, no-xxx child utterances are in Providence?\n",
    "# Subset to instances that are monosyllabic later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using current database version: '2020.1'.\n",
      "\n",
      "R[write to console]: Using supported database version: '2020.1'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pvd_idx = childespy.get_sql_query('select * from corpus where name = \"Providence\"').iloc[0]['id']\n",
    "\n",
    "phono_glosses = childespy.get_sql_query('select gloss, target_child_name, target_child_age, \\\n",
    "    speaker_code, actual_phonology, model_phonology, transcript_id, utterance_id, \\\n",
    "    token_order, corpus_name, collection_name, language from token where \\\n",
    "    actual_phonology != \"\" and model_phonology != \"\" and collection_name = \"Eng-NA\" \\\n",
    "    and corpus_id = '+str(pvd_idx) ,\n",
    "        db_version = \"2020.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.verbose: \n",
    "    print(phono_glosses.corpus_name.value_counts())\n",
    "    print(phono_glosses.loc[phono_glosses.gloss == 'xxx'].actual_phonology.value_counts())\n",
    "    print(phono_glosses.loc[phono_glosses.gloss == 'yyy'].actual_phonology.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_phono = phono_glosses.loc[(phono_glosses.speaker_code == 'CHI') & \n",
    "    (phono_glosses.target_child_age < (365*5))]\n",
    "\n",
    "def count_transmission_errors(utt_vector, error_codes):\n",
    "    return(np.sum([x in error_codes for x in  utt_vector]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxs_per_utt = chi_phono.groupby('utterance_id').gloss.agg(\n",
    "    lambda x: count_transmission_errors(x, ['xxx'])).reset_index()\n",
    "xxxs_per_utt.columns = ['utterance_id', 'num_xxx']\n",
    "yyys_per_utt = chi_phono.groupby('utterance_id').gloss.agg(\n",
    "    lambda x: count_transmission_errors(x, ['yyy'])).reset_index()\n",
    "yyys_per_utt.columns = ['utterance_id', 'num_yyy']\n",
    "failures_per_utt = xxxs_per_utt.merge(yyys_per_utt)\n",
    "\n",
    "raw_yyy_utts = failures_per_utt.loc[(failures_per_utt.num_xxx == 0) &  (failures_per_utt.num_yyy == 1)]\n",
    "\n",
    "if config.verbose: print(raw_yyy_utts.shape)\n",
    "\n",
    "raw_success_utts = failures_per_utt.loc[(failures_per_utt.num_xxx == 0) &  \n",
    "    (failures_per_utt.num_yyy == 0)]\n",
    "\n",
    "if config.verbose: print(raw_success_utts.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and clean Providence data \n",
    "\n",
    "Corresponds to: 4 | Prep Utterances / Tokens for BERT,\n",
    "    in the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using current database version: '2020.1'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the index of the Providence corpus\n",
    "pvd_idx = childespy.get_sql_query('select * from corpus where name = \"Providence\"').iloc[0]['id']\n",
    "\n",
    "# Load utterances from the Providence corpus from childs-db\n",
    "\n",
    "if config.regenerate:\n",
    "    raw_utt_glosses = childespy.get_sql_query('select gloss, transcript_id, id, \\\n",
    "    utterance_order, speaker_code, target_child_name, target_child_age, type from utterance where corpus_id = '+str(pvd_idx) ,\n",
    "        db_version = \"2020.1\")\n",
    "    raw_utt_glosses.to_csv('csv/pvd_utt_glosses.csv', index=False)\n",
    "else: \n",
    "    raw_utt_glosses = load_csvs.load_csv_with_lists('csv/pvd_utt_glosses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_chi_phono_utts = raw_utt_glosses.copy() # Avoid cleaning the glosses for the utt_glosses twice (see prep code for child splits)\n",
    "utt_glosses = data_cleaning.clean_glosses(for_chi_phono_utts, '.')\n",
    "\n",
    "if config.verbose: utt_glosses[utt_glosses.id == 17280964]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmu_2syl_inchildes = load_models.get_cmu_dict_info()\n",
    "\n",
    "# tokenize with the most extensive tokenizer, which is the one used for model #2\n",
    "\n",
    "initial_tokenizer = load_models.get_meylan_original_model(with_tags = True)['tokenizer']\n",
    "\n",
    "initial_tokenizer.add_tokens(['yyy','xxx']) #must maintain xxx and yyy for alignment,\n",
    "# otherwwise, BERT tokenizer will try to separate these into x #x and #x and y #y #y\n",
    "inital_vocab_mask, initial_vocab = transformers_bert_completions.get_softmax_mask(initial_tokenizer,\n",
    "    cmu_2syl_inchildes.word)\n",
    "\n",
    "# confirm yyy treated as a separate character\n",
    "assert initial_tokenizer.tokenize('this is a yyy.') == ['this', 'is', 'a', 'yyy', '.']\n",
    "\n",
    "cmu_in_initial_vocab = cmu_2syl_inchildes.loc[cmu_2syl_inchildes.word.isin(initial_vocab)]\n",
    "\n",
    "if config.verbose: print(cmu_in_initial_vocab.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build a dataframe of tokens \n",
    "# this is slow, because tokenization is slow\n",
    "def inflate (row):\n",
    "    tokens = initial_tokenizer.tokenize(row['gloss_with_punct'])\n",
    "    return(pd.DataFrame({'token':tokens, 'id':row['id']}) )\n",
    "\n",
    "if config.regenerate:\n",
    "    all_tokens = pd.concat([inflate(x) for x in utt_glosses.to_dict('records')])\n",
    "    all_tokens = all_tokens.merge(utt_glosses)\n",
    "    all_tokens.to_csv('csv/pvd_utt_glosses_inflated.csv')\n",
    "\n",
    "else:\n",
    "    all_tokens = load_csvs.load_csv_with_lists('csv/pvd_utt_glosses_inflated.csv', na_filter=False)\n",
    "\n",
    "if config.verbose: print(all_tokens.iloc[0:10])\n",
    "\n",
    "# Assign a token_id (integer in the BERT vocabulary). \n",
    "# Because these are from the tokenized utterances, there is no correpsondence \n",
    "# with childes-db token ids\n",
    "all_tokens['token_id'] = initial_tokenizer.convert_tokens_to_ids(all_tokens['token'])\n",
    "# assigns utterances a 0-indexed index column\n",
    "all_tokens['seq_utt_id'] = all_tokens['id'].astype('category').cat.codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add back IPA, syllable structure, and child ages for child productions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the token-level data, esp phonology\n",
    "\n",
    "if config.regenerate:\n",
    "\n",
    "    # get token-level information for Providence\n",
    "    pvd_chi_tokens = childespy.get_sql_query('select gloss, target_child_name, target_child_age, \\\n",
    "    speaker_code, actual_phonology, model_phonology, transcript_id, utterance_id, \\\n",
    "    token_order from token where speaker_code = \"CHI\" and corpus_id = '+str(pvd_idx),\n",
    "        db_version = \"2020.1\")\n",
    "    pvd_chi_tokens['gloss'] = [data_cleaning.fix_gloss(x) for x in pvd_chi_tokens.gloss]\n",
    "    \n",
    "    # prep the tokens generated from segmenting the utterances\n",
    "    all_tokens_test = copy.deepcopy(all_tokens) \n",
    "\n",
    "    # initialize the fields that need to be populated\n",
    "    all_tokens_test['actual_phonology'] = ''\n",
    "    all_tokens_test['model_phonology'] = ''\n",
    "    all_tokens_test['target_child_age'] = np.nan\n",
    "    \n",
    "    # get a set of unique utterances\n",
    "    _, idx = np.unique(all_tokens_test.id, return_index=True)\n",
    "    all_utt_indices = all_tokens_test.id[np.sort(idx)]\n",
    "    \n",
    "    # For fast retrieval of IPA, split pvd_chi_tokens into a dictionary\n",
    "    pvd_chi_tokens_list = pvd_chi_tokens.groupby(['utterance_id'])\n",
    "    pvd_chi_tokens_dict = dict(zip(\n",
    "        [x[0] for x in pvd_chi_tokens_list], \n",
    "        [x[1] for x in pvd_chi_tokens_list], \n",
    "    ))\n",
    "    \n",
    "    # For fast retrival of BERT tokenization\n",
    "    all_tokens_test_list = all_tokens_test.groupby(['id'])\n",
    "    all_tokens_test_dict = dict(zip(\n",
    "        [x[0] for x in all_tokens_test_list], \n",
    "        [x[1] for x in all_tokens_test_list], \n",
    "    ))\n",
    "        \n",
    "    # Augment the tokens from all_tokens with the IPA from pvd_chi_tokens \n",
    "    rvs = [] \n",
    "    utts_to_retrieve = raw_yyy_utts.utterance_id.to_list() + raw_success_utts.utterance_id.to_list()\n",
    "    i=-1\n",
    "    for utt_index in all_utt_indices: #utts_to_retrieve: #[16760331]:       \n",
    "        i+=1\n",
    "        if i % int(len(all_utt_indices) / 100) == 0:\n",
    "            print(str(np.round((i / (len(all_utt_indices)) * 100),2))+'% complete...')    \n",
    "            # should learn to use tqdm instead\n",
    "        if utt_index in utts_to_retrieve:        \n",
    "            utt_df = copy.deepcopy(all_tokens_test_dict[utt_index])\n",
    "            utt_df['model_phonology'] = transfomers_bert_completions.augment_with_ipa(\n",
    "              utt_df, pvd_chi_tokens_dict[utt_index],initial_tokenizer, 'model_phonology')\n",
    "            utt_df['actual_phonology'] = transfomers_bert_completions.augment_with_ipa(\n",
    "              utt_df, pvd_chi_tokens_dict[utt_index],initial_tokenizer, 'actual_phonology')\n",
    "            utt_df['target_child_age'] = pvd_chi_tokens_dict[utt_index].iloc[0].target_child_age    \n",
    "            rvs.append(utt_df)  \n",
    "        else:\n",
    "            rvs.append(all_tokens_test_dict[utt_index])  \n",
    "            \n",
    "    # get the resulting augmented forms back into a dataframe\n",
    "    all_tokens_phono = pd.concat(rvs)\n",
    "    \n",
    "    # add a unique identifier to the BERT tokens\n",
    "    all_tokens_phono['bert_token_id'] = range(all_tokens_phono.shape[0])\n",
    "    \n",
    "    #save the results\n",
    "    all_tokens_phono.to_pickle('csv/pvd_utt_glosses_phono_inflated.pkl')\n",
    "else:\n",
    "    all_tokens_phono = pd.read_pickle('csv/pvd_utt_glosses_phono_inflated.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IPA map\n",
    "phone_map_df = load_csvs.load_csv_with_lists('phon/phon_map_populated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.verbose:\n",
    "    # Inspect the IPA\n",
    "    print(all_tokens_phono.loc[all_tokens_phono.actual_phonology != ''][['token','actual_phonology','model_phonology']])\n",
    "    print(phone_map_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phone_remap(x):\n",
    "    return(x.replace(\"ː\",\"\").replace('ʌ','ə')\n",
    ".replace('ɪ','ə').replace('ɔ','ɑ').replace('a','ɑ').replace('o','oʊ').replace('˞','').replace('ʰ',\n",
    "    ''). replace('r','ɹ')).replace('\\\\^','').replace('\\\\ ̃','').replace(' ̩','').replace('^',''\n",
    ").replace('ʙ','b').replace('(','').replace(')','').replace('.','').replace('ch','ʧ'\n",
    ").replace('c','k').replace('g','ɡ').replace('y','j').replace('ʁ','ɹ')\n",
    "\n",
    "def strip_accents(string, accents=('COMBINING ACUTE ACCENT', \n",
    "    'COMBINING GRAVE ACCENT', 'COMBINING TILDE', 'COMBINING VERTICAL LINE BELOW',\n",
    "    'COMBINING SHORT STROKE OVERLAY')):\n",
    "    accents = set(map(unicodedata.lookup, accents))\n",
    "    chars = [c for c in unicodedata.normalize('NFD', string) if c not in accents]\n",
    "    return unicodedata.normalize('NFC', ''.join(chars))\n",
    "\n",
    "cv_map = dict(zip(phone_map_df['ipa'], phone_map_df['c_or_v']))\n",
    "cv_map['o'] = 'v' \n",
    "cv_map['ɜ'] = 'v'\n",
    "cv_map['e'] = 'v'\n",
    "cv_map['ʔ'] = 'c'\n",
    "cv_map['ɾ'] = 'c'\n",
    "cv_map['ɲ'] = 'c'\n",
    "cv_map['x'] = 'c'\n",
    "cv_map['ɱ'] = 'c'\n",
    "cv_map['ɣ'] = 'c'\n",
    "\n",
    "def cv_mapper(x, cv_map):\n",
    "    try:\n",
    "        return(cv_map[x])\n",
    "    except:\n",
    "        raise ValueError(x)\n",
    "\n",
    "if config.regenerate:    \n",
    "\n",
    "    # Do the same excludes as were used to identify appropriate utterances\n",
    "    excludes = ['*','(.)','(..)', '(...)','(....)','(.....)']\n",
    "    all_tokens_phono.loc[all_tokens_phono.actual_phonology.isin(excludes),'actual_phonology'] =''\n",
    "    all_tokens_phono.loc[all_tokens_phono.actual_phonology.str.contains('V'),'actual_phonology'] =''\n",
    "    \n",
    "    # remap phonology from narrow phonetic transcription to broad phonological transcription\n",
    "    all_tokens_phono['model_phonology_clean'] = [phone_remap(x) for x in all_tokens_phono['model_phonology']]\n",
    "    all_tokens_phono['actual_phonology_clean'] = [phone_remap(x) for x in all_tokens_phono['actual_phonology']]\n",
    "\n",
    "    # remove any non-combining diacritical marks\n",
    "    all_tokens_phono['model_phonology_no_dia'] = [strip_accents(x) for x in \\\n",
    "    all_tokens_phono['model_phonology_clean']]\n",
    "    all_tokens_phono['actual_phonology_no_dia'] = [strip_accents(x) for x in \\\n",
    "    all_tokens_phono['actual_phonology_clean']]\n",
    "    \n",
    "    # Compute the number of non-contiguous vowels.\n",
    "    # slightly different than the cmu vowel computation ---\n",
    "    # because here we are computing it directly from IPA\n",
    "    all_tokens_phono['cv_raw'] = [''.join([cv_mapper(x, cv_map) for x in list(y)]) if y != '' else '' for y in all_tokens_phono['actual_phonology_no_dia']]    \n",
    "    all_tokens_phono['cv_collapsed']  = [re.sub(r'(.)\\1+', r'\\1', str(x)) if x != '' else '' for x in all_tokens_phono['cv_raw']]\n",
    "    all_tokens_phono['num_vowels'] = [np.sum(np.array(list(x)) == 'v') if x !='' else np.nan for x in all_tokens_phono['cv_collapsed']]\n",
    "    all_tokens_phono.to_pickle('csv/pvd_utt_glosses_phono_cleaned_inflated.pkl')\n",
    "else:\n",
    "    all_tokens_phono = pd.read_pickle('csv/pvd_utt_glosses_phono_cleaned_inflated.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.verbose:\n",
    "    # Why no actual phonology?\n",
    "    print(all_tokens_phono.loc[all_tokens_phono.actual_phonology_no_dia != '']['actual_phonology_no_dia'])\n",
    "    print(all_tokens_phono.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the tokens that can be evaluated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_utt_ids = set(raw_success_utts['utterance_id']) \n",
    "initial_vocab_set = set(initial_vocab)\n",
    "yyy_utt_ids = set(raw_yyy_utts['utterance_id'])\n",
    "all_tokens_phono['in_vocab'] = all_tokens_phono['token'].isin(initial_vocab_set)\n",
    "all_tokens_phono['success_token'] = [x in successful_utt_ids for x in \n",
    "    all_tokens_phono['id']]\n",
    "all_tokens_phono['yyy_token'] = [x in yyy_utt_ids for x in \n",
    "    all_tokens_phono['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.verbose:\n",
    "    print(initial_vocab)\n",
    "    print(all_tokens_phono.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the subset of success and failure utterances that have transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_phono['partition'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_tokens = all_tokens_phono.loc[(all_tokens_phono['success_token']) & \n",
    "    (all_tokens_phono['num_vowels'] <= 2) ]\n",
    "all_tokens_phono.loc[(all_tokens_phono['success_token']) & \n",
    "    (all_tokens_phono['num_vowels'] <= 2), 'partition'] = 'success'     \n",
    "\n",
    "if config.verbose:\n",
    "    print(success_tokens.shape)\n",
    "    print(all_tokens_phono.loc[(all_tokens_phono['success_token']) & \n",
    "    (all_tokens_phono['num_vowels'] <= 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy_tokens = all_tokens_phono.loc[(all_tokens_phono['yyy_token']) & \n",
    "(all_tokens_phono['token'] == 'yyy') & (all_tokens_phono.num_vowels <= 2) ]\n",
    "all_tokens_phono.loc[(all_tokens_phono['yyy_token']) & \n",
    "(all_tokens_phono['token'] == 'yyy') & (all_tokens_phono.num_vowels <= 2),'partition'] = 'yyy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.verbose:\n",
    "    print(yyy_tokens.shape)\n",
    "    print(all_tokens_phono.partition.value_counts())\n",
    "    print(initial_tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_phono.loc[all_tokens_phono.token == 'xxx','token_id'] = initial_tokenizer.unk_token_id\n",
    "all_tokens_phono.loc[all_tokens_phono.token == 'yyy','token_id'] = initial_tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional section from 6 | Prevalence of Successes and Failures Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to augment successes/failures with information on age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwong/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/nwong/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing gloss df augmentation, 0.0% complete.\n",
      "Computing gloss df augmentation, 11.9218% complete.\n",
      "Computing gloss df augmentation, 23.8436% complete.\n",
      "Computing gloss df augmentation, 35.7654% complete.\n",
      "Computing gloss df augmentation, 47.6872% complete.\n",
      "Computing gloss df augmentation, 59.609% complete.\n",
      "Computing gloss df augmentation, 71.5308% complete.\n",
      "Computing gloss df augmentation, 83.4526% complete.\n",
      "Computing gloss df augmentation, 95.3743% complete.\n",
      "Computing gloss df augmentation, 0.0% complete.\n",
      "Computing gloss df augmentation, 31.7894% complete.\n",
      "Computing gloss df augmentation, 63.5789% complete.\n",
      "Computing gloss df augmentation, 95.3683% complete.\n"
     ]
    }
   ],
   "source": [
    "# get number of tokens per age\n",
    "# Warnings were present in the original code\n",
    "\n",
    "raw_success_utts['set'] = 'success'\n",
    "raw_yyy_utts['set'] = 'failure'\n",
    "\n",
    "utt_age = chi_phono.groupby('utterance_id').target_child_age.agg(np.unique).reset_index()\n",
    "\n",
    "# Additional attributes needed for the text split.\n",
    "utt_name = chi_phono.groupby('utterance_id').target_child_name.agg(np.unique).reset_index()\n",
    "utt_transcript = chi_phono.groupby('utterance_id').transcript_id.agg(np.unique).reset_index()\n",
    "# Manually asserted that speaker code is always CHI for all of chi_phono, so OK to set it directly to CHI later.\n",
    "\n",
    "inter_success_utts = raw_success_utts.copy()\n",
    "inter_yyy_utts = raw_yyy_utts.copy()\n",
    "\n",
    "for add_attr in [utt_age, utt_name, utt_transcript]:\n",
    "    inter_success_utts = inter_success_utts.merge(add_attr, on = 'utterance_id')\n",
    "    inter_yyy_utts = inter_yyy_utts.merge(add_attr, on = 'utterance_id')\n",
    "\n",
    "# Merge the glosses separately because they aren't the same for both the successes and the yyy.\n",
    "#Generate the glosses per utterance id\n",
    "utt_gloss_save_success = data_cleaning.gloss_df_augmentation(chi_phono, raw_success_utts.utterance_id)\n",
    "utt_gloss_save_yyy = data_cleaning.gloss_df_augmentation(chi_phono, raw_yyy_utts.utterance_id)\n",
    "    \n",
    "success_utts = inter_success_utts.merge(utt_gloss_save_success, on = 'utterance_id')\n",
    "yyy_utts = inter_yyy_utts.merge(utt_gloss_save_yyy, on = 'utterance_id')\n",
    "\n",
    "utts_with_ages = pd.concat([success_utts, yyy_utts])\n",
    "\n",
    "assert len(set(utts_with_ages['utterance_id'])) == utts_with_ages.shape[0],\\\n",
    "\"Make sure that the utterance id is a unique identifier for the observations in the yyy and success dataframes\"\n",
    "assert len(set(utt_age['utterance_id'])) == utt_age.shape[0],\\\n",
    "\"Make sure that the utterance id is a unique identifier for the observations in the utt_age dataframe\"\n",
    "\n",
    "# Changed from the original: the merged dfs don't have the same order of utterance id immediately,\n",
    "# so am now merging on utterance id\n",
    "\n",
    "utts_with_ages['year'] = .5*np.floor(utts_with_ages['target_child_age'] / (365. /2) ) \n",
    "\n",
    "if config.verbose:\n",
    "    print(utts_with_ages.loc[utts_with_ages.set == 'failure'].year.value_counts())\n",
    "    print(utts_with_ages.loc[utts_with_ages.set == 'success'].year.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% complete\n",
      "8.670244587599816% complete\n",
      "17.340489175199632% complete\n",
      "26.010733762799447% complete\n",
      "34.680978350399265% complete\n",
      "43.351222937999076% complete\n",
      "52.021467525598894% complete\n",
      "60.69171211319871% complete\n",
      "69.36195670079853% complete\n",
      "78.03220128839835% complete\n",
      "86.70244587599815% complete\n",
      "95.37269046359798% complete\n",
      "Asserts passed.\n"
     ]
    }
   ],
   "source": [
    "# A cell that acts as a check for the utt_age, utt_name augumentations.\n",
    "\n",
    "# Check that every utterance_id is matched to its right age in yyy/success dataframe\n",
    "# This is a valid method because all of the utterance IDs are unique per dataframe.\n",
    "\n",
    "for i in range(utts_with_ages.shape[0]):\n",
    "    \n",
    "    if i % 10000 == 0: print(f'{(i / utts_with_ages.shape[0]) * 100.0}% complete')\n",
    "    this_entry = utts_with_ages.iloc[i]\n",
    "    this_id = this_entry['utterance_id']\n",
    "    \n",
    "    keys_to_check = ['target_child_age', 'target_child_name', 'transcript_id', 'gloss']\n",
    "    \n",
    "    cross_entry = chi_phono[chi_phono['utterance_id'] == this_id]\n",
    "    # Why is the cross value actually still a string?\n",
    "    # Where is it converted to non-string -- is there a way to convert it to non string?\n",
    "\n",
    "    for key in keys_to_check:\n",
    "        \n",
    "        this_value = this_entry[key]\n",
    "    \n",
    "        \n",
    "        if key == 'gloss' and ' ' in this_entry['gloss']:\n",
    "            # If this utterance is multiple tokens,\n",
    "            # you will have to match across multiple entries in chi_phono and join them to make the gloss.\n",
    "            # For example, idx = 5 using utts_with_ages indexing\n",
    "            \n",
    "            # what this checks for\n",
    "            # 1) you got the right pieces of the gloss\n",
    "            # 2) they are in the right token order\n",
    "            \n",
    "            formatted_cross = list(cross_entry[key])\n",
    "            \n",
    "            assert list(cross_entry['token_order']) == list(range(1, 1 + cross_entry.shape[0])),\\\n",
    "            \"Cross entry was not sliced in ascending token order, so gloss order of words is wrong.\"\n",
    "            \n",
    "            \n",
    "            assert this_value == ' '.join(formatted_cross), f'if, at index: {i}, key: {key}, real: {this_value}, cross: {formatted_cross}'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # The item to be matched is a single value or string.\n",
    "            # Applies to everything but the multiple token gloss.\n",
    "            # If the gloss of the utt_with_ages is multiple tokens,\n",
    "            # then it will still match to multiple locations in chi_phono tokens.\n",
    "            # However, because it's not the gloss attribute itself, the child attribute\n",
    "            # should be repeated across all of those entries.\n",
    "            \n",
    "            if cross_entry.shape[0] == 1:\n",
    "                cross_single_val = cross_entry[key].item()\n",
    "            else:\n",
    "                cross_set = list(set(cross_entry[key]))\n",
    "                assert len(cross_set) == 1\n",
    "                cross_single_val = cross_set[0]\n",
    "                \n",
    "            assert this_value == cross_single_val, f'else, at index: {i}, key: {key}, real: {this_value}, cross: {cross_entry[key]}'\n",
    "\n",
    "print('Asserts passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This marks the end of the section before the model evaluations/queries beginning.\n",
    "## It also marks the end of chi_phono generation -- do NOT re-run above or you will double-merge and lose the target_child_age attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_eval_data(data, filename, split_name, dataset_name):\n",
    "    \n",
    "    assert split_name in ['all', 'age', 'child'], \"Invalid split name. Must be one of {all, age, child}.\"\n",
    "    \n",
    "    # Saving based on a mask of a copy of a? Will this be a problem?\n",
    "    \n",
    "    save_path = split_gen.get_split_folder(split_name, dataset_name, config.eval_dir)\n",
    "    save_location = join(save_path, filename)\n",
    "    if filename.endswith('.pkl'):\n",
    "        data.to_pickle(save_location)\n",
    "    elif filename.endswith('.csv'):\n",
    "        data.to_csv(save_location)\n",
    "    else:\n",
    "        assert False, \"Tried to save something that was neither a pkl nor a csv.\"\n",
    "    \n",
    "    print(f'Saved all/all evaluation data to {save_location}')\n",
    "    \n",
    "    return save_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "phono_filename = 'pvd_utt_glosses_phono_cleaned_inflated.pkl'\n",
    "success_utts_filename = 'success_utts.csv'\n",
    "yyy_utts_filename = 'yyy_utts.csv'\n",
    "\n",
    "data_filenames = [phono_filename, success_utts_filename, yyy_utts_filename]\n",
    "\n",
    "# Use this line from the original code and load the above two CSVs for model inputs later:\n",
    "# utts_with_ages = pd.concat([success_utts, yyy_utts]).merge(utt_age)\n",
    "\n",
    "# for the input into the actual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save evaluation data for all split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all/all evaluation data to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/all/all/pvd_utt_glosses_phono_cleaned_inflated.pkl\n",
      "Saved all/all evaluation data to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/all/all/success_utts.csv\n",
      "Saved all/all evaluation data to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/all/all/yyy_utts.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for this_data, filename in zip([all_tokens_phono, success_utts, yyy_utts], data_filenames):\n",
    "    save_eval_data(this_data, filename, 'all', 'all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save evaluation data for age split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all/all evaluation data to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/age/old/pvd_utt_glosses_phono_cleaned_inflated.pkl\n",
      "Saved all/all evaluation data to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/age/old/success_utts.csv\n",
      "Saved all/all evaluation data to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/age/old/yyy_utts.csv\n",
      "Saved all/all evaluation data to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/age/young/pvd_utt_glosses_phono_cleaned_inflated.pkl\n",
      "Saved all/all evaluation data to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/age/young/success_utts.csv\n",
      "Saved all/all evaluation data to /home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/eval/new_splits/age/young/yyy_utts.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "young_tokens_phono, old_tokens_phono = split_gen.get_age_split_data(all_tokens_phono)\n",
    "young_success_utts, old_success_utts = split_gen.get_age_split_data(success_utts)\n",
    "young_yyy_utts, old_yyy_utts = split_gen.get_age_split_data(yyy_utts)\n",
    "\n",
    "for this_data, filename in zip([old_tokens_phono, old_success_utts, old_yyy_utts], data_filenames):\n",
    "    save_eval_data(this_data, filename, 'age', 'old')\n",
    "    \n",
    "for this_data, filename in zip([young_tokens_phono, young_success_utts, young_yyy_utts], data_filenames):\n",
    "    save_eval_data(this_data, filename, 'age', 'young') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get successes and yyy samples for use in beta fitting and models across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling for: yyy, beta, all_debug, all_debug\n",
      "Resampling for: success, beta, all_debug, all_debug\n",
      "Resampling for: yyy, beta, all, all\n",
      "Resampling for: success, beta, all, all\n",
      "Resampling for: yyy, beta, age, young\n",
      "Resampling for: success, beta, age, young\n",
      "Resampling for: yyy, beta, age, old\n",
      "Resampling for: success, beta, age, old\n",
      "Resampling for: yyy, models_across_time, all_debug, all_debug\n",
      "Resampling for: success, models_across_time, all_debug, all_debug\n",
      "Resampling for: yyy, models_across_time, all, all\n",
      "Resampling for: success, models_across_time, all, all\n",
      "Resampling for: yyy, models_across_time, age, young\n",
      "Resampling for: success, models_across_time, age, young\n",
      "Resampling for: yyy, models_across_time, age, old\n",
      "Resampling for: success, models_across_time, age, old\n"
     ]
    }
   ],
   "source": [
    "# What to do here?\n",
    "\n",
    "# Temp comment out because I don't want to regen this and lose problematic cases\n",
    "\n",
    "task_types = ['beta', 'models_across_time']\n",
    "\n",
    "model_args = [('all_debug', 'all_debug'), ('all', 'all'), ('age', 'young'), ('age', 'old')]\n",
    "for task in task_types:\n",
    "    for split_name, dataset_name in model_args:\n",
    "        for pool in ['yyy', 'success']:      \n",
    "            eval_data = load_splits.load_eval_data_all(split_name, dataset_name)\n",
    "            load_splits.sample_successes_yyy(pool, task, split_name, dataset_name, eval_data[f'{pool}_utts'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results \n",
    "\n",
    "model_args = [('all_debug', 'all_debug'), ('all', 'all'), ('age', 'young'), ('age', 'old')]\n",
    "\n",
    "\n",
    "# Was working before with small sample sizes,\n",
    "# just need to integrate the condition for really big sample sizes check later\n",
    "\n",
    "\n",
    "# def expected_shape(sample, pool, which_ref_num):\n",
    "    \n",
    "#     # Sampled everything available\n",
    "#     pool_small = (pool.shape[0] < which_ref_num) and (sample.shape[0] == pool.shape[0])\n",
    "#     # Typical sampling\n",
    "#     pool_big = (pool.shape[0] >= which_ref_num) and (sample.shape[0] == which_ref_num)\n",
    "    \n",
    "#     return (pool_small or pool_big)\n",
    "    \n",
    "# expected_beta_shape = lambda df : (df.shape[0] >= config.n_beta and df.shape[0] == config.n_beta) or (df.shape[0] == )\n",
    "# expected_time_shape = and success_time.shape[0]\n",
    "# for task in task_types:\n",
    "#     for split_name, dataset_name in model_args:\n",
    "        \n",
    "#         success_beta = load_splits.load_sample_successes('beta', split_name, dataset_name)\n",
    "#         yyy_beta = load_splits.load_sample_yyy('beta', split_name, dataset_name)\n",
    "        \n",
    "#         success_time = load_splits.load_sample_successes('models_across_time', split_name, dataset_name)\n",
    "#         yyy_time = load_splits.load_sample_yyy('models_across_time', split_name, dataset_name)\n",
    "        \n",
    "        \n",
    "#         assert expected_shape(success_beta, success_beta, config.n_beta) and expected_shape(success_time, config.n_across_time)\n",
    "#         assert yyy_beta.shape[0] == config.n_beta and yyy_time.shape[0] == config.n_across_time\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On hold: Process finetuning and evaluation data for child split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Below are checks needed to ensure that disjoint splitting scheme is truly disjoint across train/val,\n",
    "# even with separate yyy/successes assignments.\n",
    "\n",
    "unique_success_ids = set(success_utts['utterance_id'])\n",
    "unique_yyy_ids = set(yyy_utts['utterance_id'])\n",
    "\n",
    "assert len(unique_success_ids & unique_yyy_ids) == 0,\\\n",
    "\"Overlap in utterance id exists between successes and yyy utterances.\"\n",
    "assert len(unique_success_ids) == success_utts.shape[0], \"Utterance ids are not unique in success_utts dataframe.\"\n",
    "assert len(unique_yyy_ids) == yyy_utts.shape[0], \"Utterance ids are not unique in yyy_utts dataframe.\"\n",
    "\n",
    "print('Disjoint assert assumptions passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note you'll have to restart runtime/re-gen all of this on random seed\n",
    "\n",
    "# For now, define a success as any utterance without any yyy and xxx -- any such utterance can be used for training.\n",
    "# This is how it's defined in the implementation elsewhere in the other splits\n",
    "# -- although technically the token itself should also be monosyllabic?\n",
    "\n",
    "# Because type is not available in the first query,\n",
    "# just make all of the sentences end with a period for now.\n",
    "\n",
    "\n",
    "# Save the successes and yyy and write them to files for finetuning.\n",
    "success_child_pool = success_utts; yyy_child_pool = yyy_utts\n",
    "\n",
    "# You need the speaker code to use the prep_utt_glosses function\n",
    "assert all(chi_phono.speaker_code == 'CHI') # The source of all of the utts_with_ages, success_utts\n",
    "\n",
    "# Augment with defaults. -> need to change this to preserve the punctuation somehow?\n",
    "# Where to actually find the punctuation?\n",
    "\n",
    "for df in [success_child_pool, yyy_child_pool]:\n",
    "    df['type'] = [ 'declarative' for _ in range(df.shape[0])]\n",
    "    df['speaker_code'] = ['CHI' for _ in range(df.shape[0])]\n",
    "\n",
    "child_names = set(success_child_pool['target_child_name'])\n",
    "\n",
    "split_attr = 'transcript_id'\n",
    "\n",
    "for name in child_names:\n",
    "    \n",
    "    # First, isolate the relevant data.\n",
    "    child_success_utts = success_child_pool[success_child_pool['target_child_name'] == name]\n",
    "    child_yyy_utts = yyy_child_pool[yyy_child_pool['target_child_name'] == name]\n",
    "    child_tokens_phono = all_tokens_phono[all_tokens_phono['target_child_name'] == name]\n",
    "    \n",
    "    # Prep finetuning data for text file writing and use None to match the processing of the other splits.\n",
    "    child_success_utts = data_cleaning.prep_utt_glosses(child_success_utts, None)\n",
    "    \n",
    "\n",
    "    # Split the successes and prepare them for file writing -- doesn't write to the state of the parent csv\n",
    "    this_partition_folder = split_gen.get_split_folder('child', name, config.data_dir)\n",
    "    train_success_idxs, val_eval_success_idxs = split_gen.determine_split_idxs(child_success_utts, split_on = split_attr, val_num = config.child_val_num)\n",
    "    \n",
    "    \n",
    "    # Find the eval data from the val data\n",
    "    val_eval_child_success_utts = split_gen.find_in_phase_idxs(child_success_utts, val_eval_success_idxs, split_attr)\n",
    "    val_idxs, eval_idxs = split_gen.determine_split_idxs(val_eval_child_success_utts, split_on = split_attr, val_num = config.child_eval_num)\n",
    "    \n",
    "    \n",
    "    success_dict = {}\n",
    "    # Mark phases in the data pool and write the finetuning data\n",
    "    for phase, phase_idxs in zip(['train', 'val', 'eval'], [train_success_idxs, val_success_idxs, eval_success_idxs]):\n",
    "        # eval.txt is not necessary, but is convenient for identifying and marking eval phase data.\n",
    "        child_success_utts, this_phase_successes = split_gen.write_data_partitions_text(child_success_utts, this_partition_folder, phase, phase_idxs, split_attr)\n",
    "        success_dict[phase] = this_phase_successes\n",
    "\n",
    "    # Identify yyy utts for use in validation. Only identify the validation data -- failures aren't used in training.\n",
    "    _, yyy_val_eval_idx = split_gen.determine_split_idxs(child_yyy_utts, split_on = split_attr, val_num = config.child_eval_num)\n",
    "    \n",
    "    val_eval_child_yyy_utts = split_gen.find_in_phase_idxs(child_yyy_utts, yyy_val_eval_idx, split_attr)\n",
    "    yyy_val_idx, yyy_eval_idx = split_gen.determine_split_idxs(val_eval_child_yyy_utts, split_on = split_attr, val_num = config.child_eval_num)\n",
    "    \n",
    "    yyy_dict = {}\n",
    "    for phase, phase_idxs in zip(['val', 'eval'], [yyy_val_idx, yyy_eval_idx]):\n",
    "        val_yyy_utts, this_phase_yyy = split_gen.assign_and_find_phase_data(phase, split_attr, phase_idxs, child_yyy_utts)\n",
    "        yyy_dict[phase] = this_phase_successes\n",
    "        \n",
    "    eval_utts = pd.concat([success_dict['eval'], yyy_dict['eval']])\n",
    "    this_eval_data_path = split_gen.get_split_folder('child', name, config.eval_dir)\n",
    "    eval_utts.to_csv(join(this_eval_data_path, 'eval_utts.csv'))\n",
    "    \n",
    "    # Save this for use in the notebook analyses. \n",
    "    \n",
    "    \n",
    "    # Disjoint check at data generation \n",
    "    # Check that utterance_id (per entry), used for splitting phases, is disjoint.\n",
    "    \n",
    "#     assert len(set(train_utts.utterance_id) & set(pd.concat([val_success_utts, val_yyy_utts]).utterance_id)) == 0, \"Train and validation data written was not disjoint.\"\n",
    "    # Need to add checks for val and eval.\n",
    "    \n",
    "#     pd.concat([child_success_utts, child_yyy_utts]).to_csv(join(this_partition_folder, 'utts_pooled_data_with_phases.csv'))\n",
    "#     # Note that not all of this data is actually used.\n",
    "#     # expected behavior is {train, val, none} where yyy not assigned to val split should read \"none\" in phase.\n",
    "    \n",
    "#     print(name, child_success_utts.shape[0], 'number of examples')\n",
    "    \n",
    "#     # Save the evaluation-related data.\n",
    "#     # Note there are no phase marks on the child_tokens_phono right now -- I just save the entire thing.\n",
    "#     # So technically, train and val are saved together -- but they should be retrieved via utterance_id\n",
    "#     # prompted by the splits of the utts_with_ages df.\n",
    "#     # This can be changed in the future if needed. \n",
    "\n",
    "    # Note not all of this is eval data, but it's being stored there for consistency\n",
    "    # with the other splits.\n",
    "    for this_data, filename in zip([child_tokens_phono, child_success_utts, child_yyy_utts], data_filenames):\n",
    "        save_eval_data(this_data, filename, 'child', name)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "child-listening-env",
   "language": "python",
   "name": "child-listening-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
