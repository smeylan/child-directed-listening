{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note that this should be run with the \"openfst\" kernel to allow us to open the correct python bindings for openfst,\n",
    "# pywrapfst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import Levenshtein\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return(pickle.load(file))\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_phono = load_pickle('likelihood_inputs/all_tokens_phono.obj')\n",
    "cmu_in_initial_vocab = load_pickle('likelihood_inputs/cmu_in_initial_vocab.obj')\n",
    "initial_vocab = load_pickle('likelihood_inputs/initial_vocab.obj')\n",
    "priors_for_age_interval = load_pickle('likelihood_inputs/priors_for_age_interval.obj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edit_distance_matrix(all_tokens_phono, prior_data, initial_vocab,  cmu_2syl_inchildes):    \n",
    "    '''\n",
    "    Get an edit distance matrix for matrix-based computation of the posterior\n",
    "\n",
    "    all_tokens_phono: corpus in tokenized from, with phonological transcriptions\n",
    "    prior_data: priors of the form output by `compare_successes_failures_*`\n",
    "    initial_vocab: word types corresponding to the softmask mask\n",
    "    cmu_2syl_inchildes: cmu pronunctiations, must have 'word' and 'ipa_short' columns \n",
    "    '''\n",
    "\n",
    "    bert_token_ids = prior_data['scores']['bert_token_id']\n",
    "    ipa = pd.DataFrame({'bert_token_id':bert_token_ids}).merge(all_tokens_phono[['bert_token_id',\n",
    "        'actual_phonology_no_dia']])\n",
    "\n",
    "    iv = pd.DataFrame({'word':initial_vocab})\n",
    "    iv = iv.merge(cmu_2syl_inchildes, how='left')\n",
    "\n",
    "    levdists = np.vstack([np.array([Levenshtein.distance(target,x) for x in iv.ipa_short\n",
    "    ]) for target in ipa.actual_phonology_no_dia])    \n",
    "    return(levdists)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "levdists = get_edit_distance_matrix(all_tokens_phono, priors_for_age_interval, initial_vocab,  cmu_in_initial_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11718, 7904)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levdists.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WFST Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.6/site-packages')\n",
    "import pywrapfst\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_compute_all_likelihoods_for_w_over_paths(d_fsa, w_fsas, ws):    \n",
    "    '''return a vector with entries corresponding to the total path weights from this d_fsa to each word in ws'''\n",
    "    return([get_likelihood_for_fsas_over_paths(d_fsa, w_fsas, w) for w in ws])\n",
    "\n",
    "def compute_all_likelihoods_for_w_over_paths_one(list_of_tuples):\n",
    "    '''wrapper to compute likelihoods for a list of n (d_fsa, w_fsa, w) tuples'''\n",
    "    test = [vectorized_compute_all_likelihoods_for_w_over_paths(x[0], x[1], x[2]) for x in list_of_tuples]\n",
    "    return(np.vstack(test))\n",
    "\n",
    "def get_weight_for_path(arc, shortest_paths): \n",
    "    '''get the weight of a single arc by iterating in Python'''\n",
    "    #print(arc)\n",
    "    path_weight = float(arc.weight)\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        #print(arc.nextstate)\n",
    "        outgoing_arcs = [x for x in shortest_paths.arcs(arc.nextstate)]\n",
    "        if len(outgoing_arcs) == 1: \n",
    "            arc = outgoing_arcs[0]\n",
    "            path_weight += float(arc.weight)\n",
    "        else:\n",
    "            finished = True\n",
    "\n",
    "    return(path_weight)\n",
    "\n",
    "def get_weights_for_paths(shortest_paths):\n",
    "    '''get the weights for a selection of paths'''\n",
    "    initial_arcs = [x for x in shortest_paths.arcs(0)]\n",
    "    return([get_weight_for_path(arc, shortest_paths) for arc in initial_arcs])\n",
    "\n",
    "def get_likelihood_for_fsas_over_paths(d_fsa, w_fsas, w, num_paths=10, return_type = \"probability\"):\n",
    "    '''get the weight of a single arc by iterating in Python'''\n",
    "    if num_paths <= 0:\n",
    "        raise ValueError('num_paths must be a positive integer')\n",
    "        \n",
    "    w_fsa = w_fsas[w]    \n",
    "    dw_composed = pywrapfst.compose(w_fsa, d_fsa)\n",
    "    dw_composed.arcsort(sort_type=\"ilabel\")\n",
    "       \n",
    "    if num_paths > 1:\n",
    "        shortest_paths = pywrapfst.epsnormalize(pywrapfst.shortestpath(dw_composed, nshortest=num_paths))\n",
    "        if return_type == \"shortest_paths\": \n",
    "            return(shortest_paths)\n",
    "        if shortest_paths.num_states() > 0:\n",
    "\n",
    "            # take the reverse distance because with multiple shortest paths, 0 is the start state, 1 is the final state\n",
    "            shortest_distance = pywrapfst.shortestdistance(shortest_paths, reverse=True)\n",
    "\n",
    "            # iterate over all outgoing arcs from the start state  \n",
    "            path_weights = get_weights_for_paths(shortest_paths)                                \n",
    "            if return_type == \"path_weights\":\n",
    "                return(path_weights)\n",
    "            shortest_paths_sum = np.sum(np.exp(-1. * np.array(path_weights)))                    \n",
    "            if return_type == \"probability\":\n",
    "                return(shortest_paths_sum)\n",
    "        else:\n",
    "            # this is the case where there is no way to compose the d_fsa and the w_fsa\n",
    "            return(0)\n",
    "\n",
    "    else:\n",
    "        shortest_path = pywrapfst.shortestpath(dw_composed)\n",
    "        if shortest_path.num_states() > 0:\n",
    "            shortest_distance = pywrapfst.shortestdistance(shortest_path)\n",
    "            return(np.exp(-1 *float(shortest_distance[0])))\n",
    "        else:\n",
    "            return(0)\n",
    "        \n",
    "def string_to_fsa(input_string, sym):\n",
    "    '''build an FSA for a given input string using the symbol table, sym'''\n",
    "    \n",
    "    # first make sure all chars can be converted\n",
    "    input_list = list(input_string)\n",
    "    for i in input_list:\n",
    "        if sym.find(i) == -1:\n",
    "            raise ValueError('Input character not found')\n",
    "    \n",
    "    # build the FSA\n",
    "    \n",
    "    f = pywrapfst.VectorFst()\n",
    "    one = pywrapfst.Weight.one(f.weight_type())\n",
    "    f.set_input_symbols(sym)\n",
    "    f.set_output_symbols(sym)\n",
    "    s = f.add_state()\n",
    "    f.set_start(s)\n",
    "    for i in input_list:    \n",
    "        n = f.add_state()\n",
    "        f.add_arc(s, pywrapfst.Arc(sym.find(i),\n",
    "            sym.find(i),  one, n))\n",
    "        s = n \n",
    "    f.set_final(n, 1)\n",
    "        \n",
    "    # verify\n",
    "    if not f.verify():\n",
    "        raise ValueError('FSA failed to verify')\n",
    "    return(f)\n",
    "\n",
    "def write_out_edited_fst(edited_fst, output_path):\n",
    "    '''writes out a pandas data frame to an FST formatted text file that can them be compiled with OpenFST'''\n",
    "\n",
    "    # needs to write each state terminal separately\n",
    "    \n",
    "    # get the indices of the terminals\n",
    "    state_weight = np.hstack([np.array([-1]), np.where(edited_fst[[3]] == '')[0]])\n",
    "    \n",
    "    first = True\n",
    "    for i in range(len(state_weight) -1):\n",
    "        section_start = state_weight[i] + 1\n",
    "        section_end = state_weight[i+1]         \n",
    "        #print('Main section: '+str(section_start)+ ' - ', str(section_end))\n",
    "        \n",
    "        terminal_start =  state_weight[i+1]\n",
    "        terminal_end = state_weight[i+1] + 1\n",
    "        #print('Terminal section: '+str(terminal_start)+ ' - ', str(terminal_end))\n",
    "        \n",
    "        ats_section = edited_fst[section_start:section_end]        \n",
    "        for j in range(3):\n",
    "            ats_section[[j]] = ats_section[[j]].astype('int')\n",
    "        #print(ats_section)\n",
    "            \n",
    "        if first: \n",
    "            ats_section.to_csv(output_path, index=False, header=None, sep='\\t')\n",
    "            first = False\n",
    "        else:\n",
    "            ats_section.to_csv(output_path, mode='a', index=False, header=None, sep='\\t')\n",
    "\n",
    "        ats_end = edited_fst.iloc[terminal_start : terminal_end]\n",
    "\n",
    "        ats_end[0,2] = ''\n",
    "        ats_end[0,3] = ''\n",
    "        ats_end[0,4] = ''\n",
    "        \n",
    "        ats_end.to_csv(output_path, mode='a',index=False, header=None, sep='\\t')\n",
    "\n",
    "    # catch any remaining arcs\n",
    "    ats_section = edited_fst[terminal_end:edited_fst.shape[0]]\n",
    "    for j in range(3):\n",
    "        ats_section[[j]] = ats_section[[j]].astype('int')\n",
    "    \n",
    "    ats_section.to_csv(output_path, mode='a',index=False, header=None, sep='\\t')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcile_symbols(fit_model, path_to_chi_phones_sym):\n",
    "    '''generate a transducer and symbol set in the same symbol set which includes all inputs and outputs'''\n",
    "    ints = [int(x) for x in np.unique(fit_model[[2]]) if not np.isnan(x)]        \n",
    "    input_symbol_table = pd.DataFrame({'symbol':[chr(x) for x in ints], 'int':ints})\n",
    "    input_symbol_table.at[input_symbol_table.int ==0, 'symbol'] = '<epsilon>'\n",
    "    #input_symbol_table.to_csv('test_input_phones.sym', sep='\\t', header=None, index=False)\n",
    "    input_cypher = dict(zip(input_symbol_table.int, input_symbol_table.symbol))\n",
    "    \n",
    "    \n",
    "    output_symbol_table = pd.read_csv(path_to_chi_phones_sym, sep='\\t', header=None)\n",
    "    output_symbol_table.columns = ['symbol','int']\n",
    "    output_cypher = dict(zip(output_symbol_table.int, output_symbol_table.symbol))\n",
    "    output_cypher\n",
    "    \n",
    "    symbols_not_in_output = set(input_symbol_table.symbol).difference(set(output_symbol_table.symbol))\n",
    "    \n",
    "    superset_cypher = copy.copy(output_cypher)\n",
    "    i = len(output_cypher.keys())\n",
    "    for symbol in symbols_not_in_output:\n",
    "        superset_cypher[i] = symbol\n",
    "        i += 1    \n",
    "    reverse_superset_cypher = dict(zip(superset_cypher.values(),superset_cypher.keys()))\n",
    "\n",
    "    fit_model_superset = copy.copy(fit_model)\n",
    "\n",
    "    # recode the input symbols\n",
    "    fit_model_superset[[2]] = [reverse_superset_cypher[input_cypher[int(x)]] if not np.isnan(x)\n",
    "            else '' for x in fit_model[[2]].values[:,0]]\n",
    "\n",
    "    # recode the output symbols\n",
    "    fit_model_superset[[3]] = [reverse_superset_cypher[output_cypher[int(x)]] if not np.isnan(x)\n",
    "            else '' for x in fit_model[[3]].values[:,0]]\n",
    "    \n",
    "    fit_model_labeled = copy.copy(fit_model)\n",
    "\n",
    "    write_out_edited_fst(fit_model_superset, 'fst/chi_edited_fst.csv')\n",
    "\n",
    "    superset_chi = pd.DataFrame({'sym': reverse_superset_cypher.keys(),\n",
    "        'utf8':reverse_superset_cypher.values()})\n",
    "    superset_chi.to_csv('fst/superset_chi.sym', header = None, index=False, sep='\\t')\n",
    "    return(fit_model_superset, superset_chi)\n",
    "\n",
    "def normalize_log_probs(vec):\n",
    "    vec = vec.values.flatten()\n",
    "    ps = np.exp(-1 * vec)\n",
    "    total = np.sum(ps)\n",
    "    return(ps / total)\n",
    "\n",
    "def normalize_partition(x): \n",
    "    '''for a given selection of FST arcs, for example all where input is a particular symbol, normalize the log probs'''\n",
    "    df = x[1]\n",
    "    df[[4]] = -1 * np.log(normalize_log_probs(df[[4]]))\n",
    "    return(df)\n",
    "\n",
    "def split(a, n):\n",
    "    '''split a list into n approximately equal length sublists, appropriate for parallelization'''\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wfst_distance_matrix(all_tokens_phono, prior_data, initial_vocab,  cmu_2syl_inchildes, \n",
    "    path_to_baum_welch_transducer, path_to_chi_phones_sym, num_cores=24):    \n",
    "    '''\n",
    "    Get wfst distance matrix for matrix-based computation of the posterior\n",
    "\n",
    "    all_tokens_phono: corpus in tokenized from, with phonological transcriptions\n",
    "    prior_data: priors of the form output by `compare_successes_failures_*`\n",
    "    initial_vocab: word types corresponding to the softmask mask\n",
    "    cmu_2syl_inchildes: cmu pronunctiations, must have 'word' and 'ipa_short' columns \n",
    "    path_to_baum_welch_transducer: path to the OpenFST transducer yielded by the BaumWelch package\n",
    "    '''\n",
    "    \n",
    "    bert_token_ids = prior_data['scores']['bert_token_id']\n",
    "    ipa = pd.DataFrame({'bert_token_id':bert_token_ids}).merge(all_tokens_phono[['bert_token_id',\n",
    "        'actual_phonology_no_dia']])\n",
    "\n",
    "    iv = pd.DataFrame({'word':initial_vocab})\n",
    "    iv = iv.merge(cmu_2syl_inchildes, how='left')\n",
    "    \n",
    "    \n",
    "    # [X] Load the transducer, create a covering symbol set, and change the transducer to the data symbol set\n",
    "    fit_model = pd.read_csv(path_to_baum_welch_transducer, sep='\\t', header=None)\n",
    "    \n",
    "    fit_model_superset, superset_chi = reconcile_symbols(fit_model, path_to_chi_phones_sym)\n",
    "    superset_chi_sym = pywrapfst.SymbolTable.read_text('fst/superset_chi.sym')\n",
    "\n",
    "    # [X] Change from a joint model to a conditional model.\n",
    "    # as of 11/10/21, only works for the unigram case\n",
    "    grouped = list(fit_model_superset.iloc[0:fit_model_superset.shape[0] - 1].groupby(2))\n",
    "    conditioned = pd.concat([normalize_partition(x) for x in grouped ])\n",
    "    tail = fit_model_superset.tail(1)\n",
    "    tail[[1]] = -1 * np.log(1)\n",
    "    conditioned = pd.concat([conditioned, tail])\n",
    "    write_out_edited_fst(conditioned, 'fst/chi_conditioned_fst.csv')\n",
    "    os.system('fstcompile --arc_type=standard fst/chi_conditioned_fst.csv fst/chi_conditioned.fst')    \n",
    "    transducer = pywrapfst.Fst.read(\"fst/chi_conditioned.fst\")\n",
    "            \n",
    "    #[X] translate all words in the vocab into FSAs (w_fsas)and compose with the n-gram transducer\n",
    "    \n",
    "    w_fsas = {}\n",
    "    ws = []\n",
    "    for w in iv.to_dict('records'):    \n",
    "        w_fsa = string_to_fsa(w['ipa_short'], superset_chi_sym)    \n",
    "        w_in = pywrapfst.compose(w_fsa.arcsort(sort_type=\"ilabel\"), transducer.arcsort(sort_type=\"ilabel\"))\n",
    "        w_fsas[w['ipa_short']] = w_in.arcsort(sort_type=\"ilabel\")\n",
    "        ws.append(w['ipa_short'])\n",
    "\n",
    "    #[ ]  check if there are repeats in d_fsas -- the pairwise computation is expensive\n",
    "    \n",
    "    #[X] translate all observed words (data) into FSAs (d_fsas)\n",
    "    d_fsas = [string_to_fsa(d, superset_chi_sym).arcsort(sort_type=\"olabel\") for d in ipa.actual_phonology_no_dia]\n",
    "    \n",
    "    \n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "    \n",
    "    data_test = string_to_fsa('hævəŋ', superset_chi_sym)\n",
    "    get_likelihood_for_fsas_over_paths(data_test, w_fsas, 'nɑəts', num_paths=10, return_type = \"probability\")\n",
    "    \n",
    "    # make the splits on the dfsas\n",
    "    serial_inputs = [(x, w_fsas, ws) for x in d_fsas[0:1000]]\n",
    "    d_fsa_inputs = split(serial_inputs, num_cores)    \n",
    "    distances = Parallel(n_jobs=num_cores)(delayed(compute_all_likelihoods_for_w_over_paths_one)(d_fsa_input) for d_fsa_input in d_fsa_inputs)\n",
    "    #distances = [vectorized_compute_all_likelihoods_for_w_over_paths(d_fsa, w_fsas, ws) for d_fsa in d_fsas]\n",
    "    \n",
    "    # yield the matrix of distances\n",
    "    \n",
    "    # make sure that the ordering of the results is not permuted \n",
    "    \n",
    "    return(np.vstack(distances))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/utils/openfst-1.8.1/openfst/lib/python3.6/site-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "/home/stephan/utils/openfst-1.8.1/openfst/lib/python3.6/site-packages/ipykernel_launcher.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/stephan/utils/openfst-1.8.1/openfst/lib/python3.6/site-packages/ipykernel_launcher.py:129: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/stephan/utils/openfst-1.8.1/openfst/lib/python3.6/site-packages/ipykernel_launcher.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/stephan/utils/openfst-1.8.1/openfst/lib/python3.6/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/stephan/utils/openfst-1.8.1/openfst/lib/python3.6/site-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-129-35fd31293ea2>\u001b[0m(57)\u001b[0;36mget_wfst_distance_matrix\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     55 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     56 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 57 \u001b[0;31m    \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring_to_fsa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hævəŋ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuperset_chi_sym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     58 \u001b[0;31m    \u001b[0mget_likelihood_for_fsas_over_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_fsas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nɑəts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"probability\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     59 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> data_test = string_to_fsa('zibwə', superset_chi_sym)\n",
      "ipdb> get_likelihood_for_fsas_over_paths(data_test, w_fsas, 'pɑz', num_paths=10, return_type = \"probability\")\n",
      "1.0260383567671263e-09\n"
     ]
    }
   ],
   "source": [
    "wfst_dists = get_wfst_distance_matrix(all_tokens_phono, priors_for_age_interval, initial_vocab,  cmu_in_initial_vocab,\n",
    "    'fst/chi-1.txt', 'fst/chi_phones.sym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.52810475e-05, 1.07906226e-02, 3.85958957e-04, ...,\n",
       "        3.55403403e-07, 7.24668517e-03, 5.87386341e-08],\n",
       "       [1.54677852e-05, 1.29174011e-07, 1.94688503e-06, ...,\n",
       "        5.18680997e-05, 2.73407839e-06, 6.45766567e-06],\n",
       "       [4.01254220e-05, 1.01620441e-05, 1.66337201e-04, ...,\n",
       "        9.13624215e-07, 8.65060033e-06, 2.61470627e-06],\n",
       "       ...,\n",
       "       [5.46743849e-05, 7.70809733e-05, 7.70398642e-01, ...,\n",
       "        8.90087130e-07, 7.22927414e-06, 4.32638519e-05],\n",
       "       [5.82977784e-04, 1.27004903e-05, 9.61850316e-04, ...,\n",
       "        2.62190922e-06, 1.10474269e-05, 2.42942632e-05],\n",
       "       [2.26606102e-04, 4.01385454e-05, 1.40714475e-04, ...,\n",
       "        1.42982341e-06, 5.33623727e-05, 3.82358853e-06]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wfst_dists\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openfst",
   "language": "python",
   "name": "openfst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
