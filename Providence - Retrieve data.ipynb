{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note for the repository: Not re-running reproducibility checks on\n",
    "# corrected version with this notebook\n",
    "# because no changes to the sampling/splitting logic itself\n",
    "# and this notebook/data preprocessing is deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "import rpy2.robjects.lib.ggplot2 as ggplot2\n",
    "import childespy\n",
    "import numpy as np\n",
    "import os\n",
    "import imp\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import re\n",
    "import unicodedata\n",
    "import scipy.stats\n",
    "import copy\n",
    "from string import punctuation\n",
    "\n",
    "from os.path import join, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_models, transformers_bert_completions, data_cleaning\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Communicative Successes and Failures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communicative success: how many no-xxx, no-yyy child  utterances are in Providence? \n",
    "# Communicative failures: how many one-yyy, no-xxx child utterances are in Providence?\n",
    "# Subset to instances that are monosyllabic later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using current database version: '2020.1'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pvd_idx = childespy.get_sql_query('select * from corpus where name = \"Providence\"').iloc[0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using supported database version: '2020.1'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phono_glosses = childespy.get_sql_query('select gloss, target_child_name, target_child_age, \\\n",
    "    speaker_code, actual_phonology, model_phonology, transcript_id, utterance_id, \\\n",
    "    token_order, corpus_name, collection_name, language from token where \\\n",
    "    actual_phonology != \"\" and model_phonology != \"\" and speaker_code in (\"MOT\", \"FAT\",\"CHI\") and collection_name = \"Eng-NA\" \\\n",
    "    and corpus_id = '+str(pvd_idx) ,\n",
    "        db_version = \"2020.1\")\n",
    "\n",
    "# 8/1/21: Added constraint for speaker_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Providence    396621\n",
       "Name: corpus_name, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phono_glosses.corpus_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*        26736\n",
       "ə           10\n",
       "(.)          7\n",
       "aɪ           4\n",
       "ən           2\n",
       "         ...  \n",
       "tæ           1\n",
       "ɛs           1\n",
       "roːts        1\n",
       "eɪ           1\n",
       "u            1\n",
       "Name: actual_phonology, Length: 76, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phono_glosses.loc[phono_glosses.gloss == 'xxx'].actual_phonology.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual phonology is almost always * for xxx items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ɛ                   3206\n",
       "ʌ                   2132\n",
       "ɪ                   1881\n",
       "ə                    512\n",
       "o                    507\n",
       "                    ... \n",
       "ɑdzəɑdɪlɑzəzduɪt       1\n",
       "ɡɪɪlɛ                  1\n",
       "ʌm̩di                  1\n",
       "ɑwɪs                   1\n",
       "əlɛdɪdɪ                1\n",
       "Name: actual_phonology, Length: 30293, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phono_glosses.loc[phono_glosses.gloss == 'yyy'].actual_phonology.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual phonology is populated for yyy items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_phono = phono_glosses.loc[(phono_glosses.speaker_code == 'CHI') & \n",
    "    (phono_glosses.target_child_age < (365*5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_transmission_errors(utt_vector, error_codes):\n",
    "    return(np.sum([x in error_codes for x in  utt_vector]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxs_per_utt = chi_phono.groupby('utterance_id').gloss.agg(\n",
    "    lambda x: count_transmission_errors(x, ['xxx'])).reset_index()\n",
    "xxxs_per_utt.columns = ['utterance_id', 'num_xxx']\n",
    "yyys_per_utt = chi_phono.groupby('utterance_id').gloss.agg(\n",
    "    lambda x: count_transmission_errors(x, ['yyy'])).reset_index()\n",
    "yyys_per_utt.columns = ['utterance_id', 'num_yyy']\n",
    "failures_per_utt = xxxs_per_utt.merge(yyys_per_utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy_utts = failures_per_utt.loc[(failures_per_utt.num_xxx == 0) &  (failures_per_utt.num_yyy == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31457, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yyy_utts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_utts = failures_per_utt.loc[(failures_per_utt.num_xxx == 0) &  \n",
    "    (failures_per_utt.num_yyy == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83880, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success_utts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_from_errorless_utts = chi_phono.loc[chi_phono.utterance_id.isin(success_utts.utterance_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude un-transcribed tokens and syllabically transcribed tokens\n",
    "excludes = ['*','(.)','(..)', '(...)','(....)','(.....)']\n",
    "tokens_from_errorless_utts = tokens_from_errorless_utts.loc[~(tokens_from_errorless_utts.actual_phonology.isin(excludes) |\n",
    "    tokens_from_errorless_utts.model_phonology.isin(excludes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214239, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_from_errorless_utts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1          ɑmɪ\n",
       "3          wiː\n",
       "4          wiː\n",
       "5           uː\n",
       "52           ɛ\n",
       "          ... \n",
       "396606       o\n",
       "396607     waɪ\n",
       "396608     liʔ\n",
       "396609       ɪ\n",
       "396610    hɪpo\n",
       "Name: actual_phonology, Length: 214239, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example phonology\n",
    "tokens_from_errorless_utts.actual_phonology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31,457 transmission errors (from 31,457 utterances)\n",
    "# 214,239 transmission successes (from 83,880 utterances)\n",
    "# this will be further decreased later by the need to test monosyllabic forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load BERT Models + CMU Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the CMU Pronunciation Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_in_childes = pd.read_pickle(config.cmu_path)\n",
    "cmu_2syl_inchildes = cmu_in_childes.loc[cmu_in_childes.num_vowels <=2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Utterances / Tokens for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using current database version: '2020.1'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the index of the Providence corpus\n",
    "pvd_idx = childespy.get_sql_query('select * from corpus where name = \"Providence\"').iloc[0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using supported database version: '2020.1'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load utterances from the Providence corpus from childs-db\n",
    "\n",
    "this_path = join(config.prov_csv_dir, 'pvd_utt_glosses.csv')\n",
    "if config.regenerate:\n",
    "    utt_glosses = childespy.get_sql_query('select gloss, transcript_id, id, \\\n",
    "    utterance_order, target_child_name, speaker_code, type from utterance where corpus_id = '+str(pvd_idx) ,\n",
    "        db_version = \"2020.1\")\n",
    "    utt_glosses.to_csv(this_path, index=False)\n",
    "else: \n",
    "    utt_glosses = pd.read_csv(this_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_glosses = utt_glosses.rename(columns = {'id' : 'utterance_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "declarative                   335678\n",
      "question                       84707\n",
      "imperative_emphatic            15954\n",
      "trail off                      12351\n",
      "self interruption               6658\n",
      "interruption                    2928\n",
      "self interruption question       825\n",
      "trail off question               650\n",
      "interruption question            304\n",
      "quotation precedes                 3\n",
      "question exclamation               2\n",
      "broken for coding                  1\n",
      "Name: type, dtype: int64\n",
      "Cell 238 gloss                where do you want me to go\n",
      "transcript_id                             42204\n",
      "utterance_id                           16759250\n",
      "utterance_order                               1\n",
      "target_child_name                          Alex\n",
      "speaker_code                                OPE\n",
      "type                                   question\n",
      "punct                                         ?\n",
      "Name: 1, dtype: object\n",
      "(460061, 10)\n"
     ]
    }
   ],
   "source": [
    "# Prep the utterances for tokenization\n",
    "\n",
    "\n",
    "import importlib\n",
    "importlib.reload(data_cleaning)\n",
    "\n",
    "utt_glosses = data_cleaning.clean_glosses(utt_glosses)\n",
    "print(utt_glosses.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tokenizer = load_models.get_primary_tokenizer()\n",
    "initial_vocab, cmu_in_initial_vocab = load_models.get_initial_vocab_info(initial_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'yyy', '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm yyy treated as a separate character\n",
    "initial_tokenizer.tokenize('this is a yyy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7904, 8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmu_in_initial_vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataframe of tokens \n",
    "# this is slow, because tokenization is slow\n",
    "def inflate(row):\n",
    "    tokens = initial_tokenizer.tokenize(row['gloss_with_punct'])\n",
    "    return(pd.DataFrame({'token':tokens, 'utterance_id':row['utterance_id']}) )\n",
    "\n",
    "inflate_path = join(config.prov_csv_dir, 'pvd_utt_glosses_inflated.csv')\n",
    "if config.regenerate:\n",
    "    all_tokens = pd.concat([inflate(x) for x in utt_glosses.to_dict('records')])\n",
    "    all_tokens = all_tokens.merge(utt_glosses)\n",
    "    all_tokens.to_csv(inflate_path)\n",
    "\n",
    "else:\n",
    "    all_tokens = pd.read_csv(inflate_path, na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>gloss</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>utterance_order</th>\n",
       "      <th>target_child_name</th>\n",
       "      <th>speaker_code</th>\n",
       "      <th>type</th>\n",
       "      <th>punct</th>\n",
       "      <th>speaker_code_simple</th>\n",
       "      <th>gloss_with_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[cgv]</td>\n",
       "      <td>16759250</td>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>42204</td>\n",
       "      <td>1</td>\n",
       "      <td>Alex</td>\n",
       "      <td>OPE</td>\n",
       "      <td>question</td>\n",
       "      <td>?</td>\n",
       "      <td>[CGV]</td>\n",
       "      <td>[CGV] where do you want me to go?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>where</td>\n",
       "      <td>16759250</td>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>42204</td>\n",
       "      <td>1</td>\n",
       "      <td>Alex</td>\n",
       "      <td>OPE</td>\n",
       "      <td>question</td>\n",
       "      <td>?</td>\n",
       "      <td>[CGV]</td>\n",
       "      <td>[CGV] where do you want me to go?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>do</td>\n",
       "      <td>16759250</td>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>42204</td>\n",
       "      <td>1</td>\n",
       "      <td>Alex</td>\n",
       "      <td>OPE</td>\n",
       "      <td>question</td>\n",
       "      <td>?</td>\n",
       "      <td>[CGV]</td>\n",
       "      <td>[CGV] where do you want me to go?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you</td>\n",
       "      <td>16759250</td>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>42204</td>\n",
       "      <td>1</td>\n",
       "      <td>Alex</td>\n",
       "      <td>OPE</td>\n",
       "      <td>question</td>\n",
       "      <td>?</td>\n",
       "      <td>[CGV]</td>\n",
       "      <td>[CGV] where do you want me to go?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>want</td>\n",
       "      <td>16759250</td>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>42204</td>\n",
       "      <td>1</td>\n",
       "      <td>Alex</td>\n",
       "      <td>OPE</td>\n",
       "      <td>question</td>\n",
       "      <td>?</td>\n",
       "      <td>[CGV]</td>\n",
       "      <td>[CGV] where do you want me to go?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>me</td>\n",
       "      <td>16759250</td>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>42204</td>\n",
       "      <td>1</td>\n",
       "      <td>Alex</td>\n",
       "      <td>OPE</td>\n",
       "      <td>question</td>\n",
       "      <td>?</td>\n",
       "      <td>[CGV]</td>\n",
       "      <td>[CGV] where do you want me to go?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>to</td>\n",
       "      <td>16759250</td>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>42204</td>\n",
       "      <td>1</td>\n",
       "      <td>Alex</td>\n",
       "      <td>OPE</td>\n",
       "      <td>question</td>\n",
       "      <td>?</td>\n",
       "      <td>[CGV]</td>\n",
       "      <td>[CGV] where do you want me to go?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>go</td>\n",
       "      <td>16759250</td>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>42204</td>\n",
       "      <td>1</td>\n",
       "      <td>Alex</td>\n",
       "      <td>OPE</td>\n",
       "      <td>question</td>\n",
       "      <td>?</td>\n",
       "      <td>[CGV]</td>\n",
       "      <td>[CGV] where do you want me to go?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>?</td>\n",
       "      <td>16759250</td>\n",
       "      <td>where do you want me to go</td>\n",
       "      <td>42204</td>\n",
       "      <td>1</td>\n",
       "      <td>Alex</td>\n",
       "      <td>OPE</td>\n",
       "      <td>question</td>\n",
       "      <td>?</td>\n",
       "      <td>[CGV]</td>\n",
       "      <td>[CGV] where do you want me to go?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[cgv]</td>\n",
       "      <td>16759261</td>\n",
       "      <td>anywhere you'll feel comfortable um anywhere</td>\n",
       "      <td>42204</td>\n",
       "      <td>2</td>\n",
       "      <td>Alex</td>\n",
       "      <td>MOT</td>\n",
       "      <td>declarative</td>\n",
       "      <td>.</td>\n",
       "      <td>[CGV]</td>\n",
       "      <td>[CGV] anywhere you'll feel comfortable um anyw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token  utterance_id                                         gloss  \\\n",
       "0  [cgv]      16759250                    where do you want me to go   \n",
       "1  where      16759250                    where do you want me to go   \n",
       "2     do      16759250                    where do you want me to go   \n",
       "3    you      16759250                    where do you want me to go   \n",
       "4   want      16759250                    where do you want me to go   \n",
       "5     me      16759250                    where do you want me to go   \n",
       "6     to      16759250                    where do you want me to go   \n",
       "7     go      16759250                    where do you want me to go   \n",
       "8      ?      16759250                    where do you want me to go   \n",
       "9  [cgv]      16759261  anywhere you'll feel comfortable um anywhere   \n",
       "\n",
       "   transcript_id  utterance_order target_child_name speaker_code         type  \\\n",
       "0          42204                1              Alex          OPE     question   \n",
       "1          42204                1              Alex          OPE     question   \n",
       "2          42204                1              Alex          OPE     question   \n",
       "3          42204                1              Alex          OPE     question   \n",
       "4          42204                1              Alex          OPE     question   \n",
       "5          42204                1              Alex          OPE     question   \n",
       "6          42204                1              Alex          OPE     question   \n",
       "7          42204                1              Alex          OPE     question   \n",
       "8          42204                1              Alex          OPE     question   \n",
       "9          42204                2              Alex          MOT  declarative   \n",
       "\n",
       "  punct speaker_code_simple                                   gloss_with_punct  \n",
       "0     ?               [CGV]                  [CGV] where do you want me to go?  \n",
       "1     ?               [CGV]                  [CGV] where do you want me to go?  \n",
       "2     ?               [CGV]                  [CGV] where do you want me to go?  \n",
       "3     ?               [CGV]                  [CGV] where do you want me to go?  \n",
       "4     ?               [CGV]                  [CGV] where do you want me to go?  \n",
       "5     ?               [CGV]                  [CGV] where do you want me to go?  \n",
       "6     ?               [CGV]                  [CGV] where do you want me to go?  \n",
       "7     ?               [CGV]                  [CGV] where do you want me to go?  \n",
       "8     ?               [CGV]                  [CGV] where do you want me to go?  \n",
       "9     .               [CGV]  [CGV] anywhere you'll feel comfortable um anyw...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a token_id (integer in the BERT vocabulary). \n",
    "# Because these are from the tokenized utterances, there is no correpsondence \n",
    "# with childes-db token ids\n",
    "all_tokens['token_id'] = initial_tokenizer.convert_tokens_to_ids(all_tokens['token'])\n",
    "# assigns utterances a 0-indexed index column\n",
    "all_tokens['seq_utt_id'] = all_tokens['utterance_id'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add back IPA, syllable structure, and child ages for child productions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using supported database version: '2020.1'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% complete...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwong/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/pandas/core/frame.py:1554: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0% complete...\n",
      "2.0% complete...\n",
      "3.0% complete...\n",
      "4.0% complete...\n",
      "5.0% complete...\n",
      "6.0% complete...\n",
      "7.0% complete...\n",
      "8.0% complete...\n",
      "9.0% complete...\n",
      "10.0% complete...\n",
      "11.0% complete...\n",
      "12.0% complete...\n",
      "13.0% complete...\n",
      "14.0% complete...\n",
      "15.0% complete...\n",
      "16.0% complete...\n",
      "17.0% complete...\n",
      "18.0% complete...\n",
      "19.0% complete...\n",
      "20.0% complete...\n",
      "21.0% complete...\n",
      "22.0% complete...\n",
      "23.0% complete...\n",
      "24.0% complete...\n",
      "25.0% complete...\n",
      "26.0% complete...\n",
      "27.0% complete...\n",
      "28.0% complete...\n",
      "29.0% complete...\n",
      "30.0% complete...\n",
      "31.0% complete...\n",
      "32.0% complete...\n",
      "33.0% complete...\n",
      "34.0% complete...\n",
      "35.0% complete...\n",
      "36.0% complete...\n",
      "37.0% complete...\n",
      "37.99% complete...\n",
      "38.99% complete...\n",
      "39.99% complete...\n",
      "40.99% complete...\n",
      "41.99% complete...\n",
      "42.99% complete...\n",
      "43.99% complete...\n",
      "44.99% complete...\n",
      "45.99% complete...\n",
      "46.99% complete...\n",
      "47.99% complete...\n",
      "48.99% complete...\n",
      "49.99% complete...\n",
      "50.99% complete...\n",
      "51.99% complete...\n",
      "52.99% complete...\n",
      "53.99% complete...\n",
      "54.99% complete...\n",
      "55.99% complete...\n",
      "56.99% complete...\n",
      "57.99% complete...\n",
      "58.99% complete...\n",
      "59.99% complete...\n",
      "60.99% complete...\n",
      "61.99% complete...\n",
      "62.99% complete...\n",
      "63.99% complete...\n",
      "64.99% complete...\n",
      "65.99% complete...\n",
      "66.99% complete...\n",
      "67.99% complete...\n",
      "68.99% complete...\n",
      "69.99% complete...\n",
      "70.99% complete...\n",
      "71.99% complete...\n",
      "72.99% complete...\n",
      "73.99% complete...\n",
      "74.99% complete...\n",
      "75.99% complete...\n",
      "76.99% complete...\n",
      "77.99% complete...\n",
      "78.99% complete...\n",
      "79.99% complete...\n",
      "80.99% complete...\n",
      "81.99% complete...\n",
      "82.99% complete...\n",
      "83.99% complete...\n",
      "84.99% complete...\n",
      "85.99% complete...\n",
      "86.99% complete...\n",
      "87.99% complete...\n",
      "88.99% complete...\n",
      "89.99% complete...\n",
      "90.99% complete...\n",
      "91.99% complete...\n",
      "92.99% complete...\n",
      "93.99% complete...\n",
      "94.99% complete...\n",
      "95.99% complete...\n",
      "96.99% complete...\n",
      "97.99% complete...\n",
      "98.99% complete...\n",
      "99.99% complete...\n"
     ]
    }
   ],
   "source": [
    "# get the token-level data, esp phonology\n",
    "\n",
    "save_phono_inflated_path = join(config.prov_csv_dir, 'pvd_utt_glosses_phono_inflated.pkl')\n",
    "\n",
    "if config.regenerate:\n",
    "\n",
    "    # get token-level information for Providence\n",
    "    pvd_chi_tokens = childespy.get_sql_query('select gloss, target_child_name, target_child_age, \\\n",
    "    speaker_code, actual_phonology, model_phonology, transcript_id, utterance_id, \\\n",
    "    token_order from token where speaker_code = \"CHI\" and corpus_id = '+str(pvd_idx),\n",
    "        db_version = \"2020.1\")\n",
    "    pvd_chi_tokens['gloss'] = [data_cleaning.fix_gloss(x) for x in pvd_chi_tokens.gloss]\n",
    "    \n",
    "    # prep the tokens generated from segmenting the utterances\n",
    "    all_tokens_test = copy.deepcopy(all_tokens) \n",
    "\n",
    "    # initialize the fields that need to be populated\n",
    "    all_tokens_test['actual_phonology'] = ''\n",
    "    all_tokens_test['model_phonology'] = ''\n",
    "    all_tokens_test['target_child_age'] = np.nan\n",
    "    \n",
    "    # get a set of unique utterances\n",
    "    _, idx = np.unique(all_tokens_test.utterance_id, return_index=True)\n",
    "    all_utt_indices = all_tokens_test.utterance_id[np.sort(idx)]\n",
    "    \n",
    "    # For fast retrieval of IPA, split pvd_chi_tokens into a dictionary\n",
    "    pvd_chi_tokens_list = pvd_chi_tokens.groupby(['utterance_id'])\n",
    "    pvd_chi_tokens_dict = dict(zip(\n",
    "        [x[0] for x in pvd_chi_tokens_list], \n",
    "        [x[1] for x in pvd_chi_tokens_list], \n",
    "    ))\n",
    "    \n",
    "    # For fast retrival of BERT tokenization\n",
    "    all_tokens_test_list = all_tokens_test.groupby(['utterance_id'])\n",
    "    all_tokens_test_dict = dict(zip(\n",
    "        [x[0] for x in all_tokens_test_list], \n",
    "        [x[1] for x in all_tokens_test_list], \n",
    "    ))\n",
    "        \n",
    "    # Augment the tokens from all_tokens with the IPA from pvd_chi_tokens \n",
    "    rvs = [] \n",
    "    utts_to_retrieve = yyy_utts.utterance_id.to_list() + success_utts.utterance_id.to_list()\n",
    "    i=-1\n",
    "    for utt_index in all_utt_indices: #utts_to_retrieve: #[16760331]:       \n",
    "        i+=1\n",
    "        if i % int(len(all_utt_indices) / 100) == 0:\n",
    "            print(str(np.round((i / (len(all_utt_indices)) * 100),2))+'% complete...')    \n",
    "            # should learn to use tqdm instead\n",
    "        if utt_index in utts_to_retrieve:        \n",
    "            utt_df = copy.deepcopy(all_tokens_test_dict[utt_index])\n",
    "            utt_df['model_phonology'] = transformers_bert_completions.augment_with_ipa(\n",
    "              utt_df, pvd_chi_tokens_dict[utt_index],initial_tokenizer, 'model_phonology')\n",
    "            utt_df['actual_phonology'] = transformers_bert_completions.augment_with_ipa(\n",
    "              utt_df, pvd_chi_tokens_dict[utt_index],initial_tokenizer, 'actual_phonology')\n",
    "            utt_df['target_child_age'] = pvd_chi_tokens_dict[utt_index].iloc[0].target_child_age    \n",
    "            rvs.append(utt_df)  \n",
    "        else:\n",
    "            rvs.append(all_tokens_test_dict[utt_index])  \n",
    "            \n",
    "    # get the resulting augmented forms back into a dataframe\n",
    "    all_tokens_phono = pd.concat(rvs)\n",
    "    \n",
    "    # add a unique identifier to the BERT tokens\n",
    "    all_tokens_phono['bert_token_id'] = range(all_tokens_phono.shape[0])\n",
    "    \n",
    "    #save the results\n",
    "    all_tokens_phono.to_pickle(save_phono_inflated_path)\n",
    "else:\n",
    "    all_tokens_phono = pd.read_pickle(save_phono_inflated_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>actual_phonology</th>\n",
       "      <th>model_phonology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>mommy</td>\n",
       "      <td>ɑmɪ</td>\n",
       "      <td>mɑmiː</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>yyy</td>\n",
       "      <td>ʌ</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>wee</td>\n",
       "      <td>wiː</td>\n",
       "      <td>wiː</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>yyy</td>\n",
       "      <td>aʊ</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>wee</td>\n",
       "      <td>wiː</td>\n",
       "      <td>wiː</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083588</th>\n",
       "      <td>nobody</td>\n",
       "      <td>nobɑɾi</td>\n",
       "      <td>noʊbɑdiː</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083589</th>\n",
       "      <td>hates</td>\n",
       "      <td>heɪs</td>\n",
       "      <td>heɪts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083594</th>\n",
       "      <td>oh</td>\n",
       "      <td>o</td>\n",
       "      <td>oʊ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083595</th>\n",
       "      <td>why</td>\n",
       "      <td>waɪ</td>\n",
       "      <td>waɪ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083596</th>\n",
       "      <td>lick</td>\n",
       "      <td>liʔ</td>\n",
       "      <td>lɪk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254517 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token actual_phonology model_phonology\n",
       "42        mommy              ɑmɪ           mɑmiː\n",
       "81          yyy                ʌ               *\n",
       "170         wee              wiː             wiː\n",
       "173         yyy               aʊ               *\n",
       "201         wee              wiː             wiː\n",
       "...         ...              ...             ...\n",
       "3083588  nobody           nobɑɾi        noʊbɑdiː\n",
       "3083589   hates             heɪs           heɪts\n",
       "3083594      oh                o              oʊ\n",
       "3083595     why              waɪ             waɪ\n",
       "3083596    lick              liʔ             lɪk\n",
       "\n",
       "[254517 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the IPA\n",
    "all_tokens_phono.loc[all_tokens_phono.actual_phonology != ''][['token','actual_phonology','model_phonology']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arpa</th>\n",
       "      <th>ipa</th>\n",
       "      <th>c_or_v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA</td>\n",
       "      <td>ɑ</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE</td>\n",
       "      <td>æ</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AH</td>\n",
       "      <td>ə</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AO</td>\n",
       "      <td>ɔ</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AW</td>\n",
       "      <td>aʊ</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  arpa ipa c_or_v\n",
       "0   AA   ɑ      v\n",
       "1   AE   æ      v\n",
       "2   AH   ə      v\n",
       "3   AO   ɔ      v\n",
       "4   AW  aʊ      v"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the IPA map\n",
    "phone_map_df = pd.read_csv('phon/phon_map_populated.csv')\n",
    "phone_map_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phone_remap(x):\n",
    "    return(x.replace(\"ː\",\"\").replace('ʌ','ə')\n",
    ".replace('ɪ','ə').replace('ɔ','ɑ').replace('a','ɑ').replace('o','oʊ').replace('˞','').replace('ʰ',\n",
    "    ''). replace('r','ɹ')).replace('\\\\^','').replace('\\\\ ̃','').replace(' ̩','').replace('^',''\n",
    ").replace('ʙ','b').replace('(','').replace(')','').replace('.','').replace('ch','ʧ'\n",
    ").replace('c','k').replace('g','ɡ').replace('y','j').replace('ʁ','ɹ')\n",
    "\n",
    "def strip_accents(string, accents=('COMBINING ACUTE ACCENT', \n",
    "    'COMBINING GRAVE ACCENT', 'COMBINING TILDE', 'COMBINING VERTICAL LINE BELOW',\n",
    "    'COMBINING SHORT STROKE OVERLAY')):\n",
    "    accents = set(map(unicodedata.lookup, accents))\n",
    "    chars = [c for c in unicodedata.normalize('NFD', string) if c not in accents]\n",
    "    return unicodedata.normalize('NFC', ''.join(chars))\n",
    "\n",
    "cv_map = dict(zip(phone_map_df['ipa'], phone_map_df['c_or_v']))\n",
    "cv_map['o'] = 'v' \n",
    "cv_map['ɜ'] = 'v'\n",
    "cv_map['e'] = 'v'\n",
    "cv_map['ʔ'] = 'c'\n",
    "cv_map['ɾ'] = 'c'\n",
    "cv_map['ɲ'] = 'c'\n",
    "cv_map['x'] = 'c'\n",
    "cv_map['ɱ'] = 'c'\n",
    "cv_map['ɣ'] = 'c'\n",
    "\n",
    "def cv_mapper(x, cv_map):\n",
    "    try:\n",
    "        return(cv_map[x])\n",
    "    except:\n",
    "        raise ValueError(x)\n",
    "\n",
    "cleaned_inflated_save = join(config.prov_csv_dir, 'pvd_utt_glosses_phono_cleaned_inflated.pkl')\n",
    "\n",
    "if config.regenerate:    \n",
    "\n",
    "    # Do the same excludes as were used to identify appropriate utterances\n",
    "    excludes = ['*','(.)','(..)', '(...)','(....)','(.....)']\n",
    "    all_tokens_phono.loc[all_tokens_phono.actual_phonology.isin(excludes),'actual_phonology'] =''\n",
    "    all_tokens_phono.loc[all_tokens_phono.actual_phonology.str.contains('V'),'actual_phonology'] =''\n",
    "    \n",
    "    # remap phonology from narrow phonetic transcription to broad phonological transcription\n",
    "    all_tokens_phono['model_phonology_clean'] = [phone_remap(x) for x in all_tokens_phono['model_phonology']]\n",
    "    all_tokens_phono['actual_phonology_clean'] = [phone_remap(x) for x in all_tokens_phono['actual_phonology']]\n",
    "\n",
    "    # remove any non-combining diacritical marks\n",
    "    all_tokens_phono['model_phonology_no_dia'] = [strip_accents(x) for x in \\\n",
    "    all_tokens_phono['model_phonology_clean']]\n",
    "    all_tokens_phono['actual_phonology_no_dia'] = [strip_accents(x) for x in \\\n",
    "    all_tokens_phono['actual_phonology_clean']]\n",
    "    \n",
    "    # Compute the number of non-contiguous vowels.\n",
    "    # slightly different than the cmu vowel computation ---\n",
    "    # because here we are computing it directly from IPA\n",
    "    all_tokens_phono['cv_raw'] = [''.join([cv_mapper(x, cv_map) for x in list(y)]) if y != '' else '' for y in all_tokens_phono['actual_phonology_no_dia']]    \n",
    "    all_tokens_phono['cv_collapsed']  = [re.sub(r'(.)\\1+', r'\\1', str(x)) if x != '' else '' for x in all_tokens_phono['cv_raw']]\n",
    "    all_tokens_phono['num_vowels'] = [np.sum(np.array(list(x)) == 'v') if x !='' else np.nan for x in all_tokens_phono['cv_collapsed']]\n",
    "    all_tokens_phono.to_pickle(cleaned_inflated_save)\n",
    "else:\n",
    "    all_tokens_phono = pd.read_pickle(cleaned_inflated_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42             ɑmə\n",
       "81               ə\n",
       "170             wi\n",
       "173             ɑʊ\n",
       "201             wi\n",
       "            ...   \n",
       "3083588    noʊbɑɾi\n",
       "3083589       heəs\n",
       "3083594         oʊ\n",
       "3083595        wɑə\n",
       "3083596        liʔ\n",
       "Name: actual_phonology_no_dia, Length: 254440, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens_phono.loc[all_tokens_phono.actual_phonology_no_dia != '']['actual_phonology_no_dia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3083625, 24)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens_phono.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the tokens that can be evaluated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the tokens in the resulting dataframe that belong to the utterances identified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'b', 'c', ..., 'hideout', 'pudding', 'stalks'], dtype='<U18')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "successful_utt_ids = set(success_utts['utterance_id'])\n",
    "\n",
    "initial_vocab_set = set(initial_vocab)\n",
    "\n",
    "yyy_utt_ids = set(yyy_utts['utterance_id'])\n",
    "\n",
    "all_tokens_phono['in_vocab'] = all_tokens_phono['token'].isin(initial_vocab_set)\n",
    "\n",
    "# 8/1/21: Changed this line to include the vocab constraint.\n",
    "all_tokens_phono['success_token'] = [(x in successful_utt_ids) and (y) for x, y in \n",
    "    zip(all_tokens_phono['utterance_id'], all_tokens_phono['in_vocab'])]\n",
    "# end changes\n",
    "\n",
    "all_tokens_phono['yyy_token'] = [x in yyy_utt_ids for x in \n",
    "    all_tokens_phono['utterance_id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3083625, 27)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens_phono.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert '' not in set(all_tokens_phono[all_tokens_phono['num_vowels'] <= 2].actual_phonology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential changes to counts begin here due to filtering on in_vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the subset of success and failure utterances that have transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_phono['partition'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185723, 28)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success_tokens = all_tokens_phono.loc[(all_tokens_phono['success_token']) & \n",
    "    (all_tokens_phono['num_vowels'] <= 2) ]\n",
    "all_tokens_phono.loc[(all_tokens_phono['success_token']) & \n",
    "    (all_tokens_phono['num_vowels'] <= 2), 'partition'] = 'success'     \n",
    "success_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Successes conditions\n",
    "\n",
    "success_tokens_check = all_tokens_phono[all_tokens_phono.partition == 'success']\n",
    "assert all(success_tokens_check['in_vocab'])\n",
    "assert all(success_tokens_check.utterance_id.isin(successful_utt_ids))\n",
    "assert all(success_tokens_check['num_vowels'] <= 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>gloss</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>utterance_order</th>\n",
       "      <th>target_child_name</th>\n",
       "      <th>speaker_code</th>\n",
       "      <th>type</th>\n",
       "      <th>punct</th>\n",
       "      <th>speaker_code_simple</th>\n",
       "      <th>...</th>\n",
       "      <th>actual_phonology_clean</th>\n",
       "      <th>model_phonology_no_dia</th>\n",
       "      <th>actual_phonology_no_dia</th>\n",
       "      <th>cv_raw</th>\n",
       "      <th>cv_collapsed</th>\n",
       "      <th>num_vowels</th>\n",
       "      <th>in_vocab</th>\n",
       "      <th>success_token</th>\n",
       "      <th>yyy_token</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>mommy</td>\n",
       "      <td>16759315</td>\n",
       "      <td>Mommy</td>\n",
       "      <td>42204</td>\n",
       "      <td>6</td>\n",
       "      <td>Alex</td>\n",
       "      <td>CHI</td>\n",
       "      <td>declarative</td>\n",
       "      <td>.</td>\n",
       "      <td>[CHI]</td>\n",
       "      <td>...</td>\n",
       "      <td>ɑmə</td>\n",
       "      <td>mɑmi</td>\n",
       "      <td>ɑmə</td>\n",
       "      <td>vcv</td>\n",
       "      <td>vcv</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>wee</td>\n",
       "      <td>16759467</td>\n",
       "      <td>wee</td>\n",
       "      <td>42204</td>\n",
       "      <td>24</td>\n",
       "      <td>Alex</td>\n",
       "      <td>CHI</td>\n",
       "      <td>declarative</td>\n",
       "      <td>.</td>\n",
       "      <td>[CHI]</td>\n",
       "      <td>...</td>\n",
       "      <td>wi</td>\n",
       "      <td>wi</td>\n",
       "      <td>wi</td>\n",
       "      <td>cv</td>\n",
       "      <td>cv</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>wee</td>\n",
       "      <td>16759501</td>\n",
       "      <td>wee</td>\n",
       "      <td>42204</td>\n",
       "      <td>28</td>\n",
       "      <td>Alex</td>\n",
       "      <td>CHI</td>\n",
       "      <td>declarative</td>\n",
       "      <td>.</td>\n",
       "      <td>[CHI]</td>\n",
       "      <td>...</td>\n",
       "      <td>wi</td>\n",
       "      <td>wi</td>\n",
       "      <td>wi</td>\n",
       "      <td>cv</td>\n",
       "      <td>cv</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>woo</td>\n",
       "      <td>16759549</td>\n",
       "      <td>woo</td>\n",
       "      <td>42204</td>\n",
       "      <td>33</td>\n",
       "      <td>Alex</td>\n",
       "      <td>CHI</td>\n",
       "      <td>declarative</td>\n",
       "      <td>.</td>\n",
       "      <td>[CHI]</td>\n",
       "      <td>...</td>\n",
       "      <td>u</td>\n",
       "      <td>wu</td>\n",
       "      <td>u</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>ernie</td>\n",
       "      <td>16759752</td>\n",
       "      <td>Ernie</td>\n",
       "      <td>42204</td>\n",
       "      <td>58</td>\n",
       "      <td>Alex</td>\n",
       "      <td>CHI</td>\n",
       "      <td>declarative</td>\n",
       "      <td>.</td>\n",
       "      <td>[CHI]</td>\n",
       "      <td>...</td>\n",
       "      <td>ɛ</td>\n",
       "      <td>əɹni</td>\n",
       "      <td>ɛ</td>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083575</th>\n",
       "      <td>help</td>\n",
       "      <td>17280891</td>\n",
       "      <td>help</td>\n",
       "      <td>42569</td>\n",
       "      <td>752</td>\n",
       "      <td>William</td>\n",
       "      <td>CHI</td>\n",
       "      <td>declarative</td>\n",
       "      <td>.</td>\n",
       "      <td>[CHI]</td>\n",
       "      <td>...</td>\n",
       "      <td>ɛlp</td>\n",
       "      <td>hɛlp</td>\n",
       "      <td>ɛlp</td>\n",
       "      <td>vcc</td>\n",
       "      <td>vc</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083589</th>\n",
       "      <td>hates</td>\n",
       "      <td>17280946</td>\n",
       "      <td>nobody hates Simba</td>\n",
       "      <td>42569</td>\n",
       "      <td>755</td>\n",
       "      <td>William</td>\n",
       "      <td>CHI</td>\n",
       "      <td>declarative</td>\n",
       "      <td>.</td>\n",
       "      <td>[CHI]</td>\n",
       "      <td>...</td>\n",
       "      <td>heəs</td>\n",
       "      <td>heəts</td>\n",
       "      <td>heəs</td>\n",
       "      <td>cvvc</td>\n",
       "      <td>cvc</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083594</th>\n",
       "      <td>oh</td>\n",
       "      <td>17280964</td>\n",
       "      <td>oh why lick hippo</td>\n",
       "      <td>42569</td>\n",
       "      <td>756</td>\n",
       "      <td>William</td>\n",
       "      <td>CHI</td>\n",
       "      <td>self interruption</td>\n",
       "      <td>.</td>\n",
       "      <td>[CHI]</td>\n",
       "      <td>...</td>\n",
       "      <td>oʊ</td>\n",
       "      <td>oʊʊ</td>\n",
       "      <td>oʊ</td>\n",
       "      <td>vv</td>\n",
       "      <td>v</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083595</th>\n",
       "      <td>why</td>\n",
       "      <td>17280964</td>\n",
       "      <td>oh why lick hippo</td>\n",
       "      <td>42569</td>\n",
       "      <td>756</td>\n",
       "      <td>William</td>\n",
       "      <td>CHI</td>\n",
       "      <td>self interruption</td>\n",
       "      <td>.</td>\n",
       "      <td>[CHI]</td>\n",
       "      <td>...</td>\n",
       "      <td>wɑə</td>\n",
       "      <td>wɑə</td>\n",
       "      <td>wɑə</td>\n",
       "      <td>cvv</td>\n",
       "      <td>cv</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083596</th>\n",
       "      <td>lick</td>\n",
       "      <td>17280964</td>\n",
       "      <td>oh why lick hippo</td>\n",
       "      <td>42569</td>\n",
       "      <td>756</td>\n",
       "      <td>William</td>\n",
       "      <td>CHI</td>\n",
       "      <td>self interruption</td>\n",
       "      <td>.</td>\n",
       "      <td>[CHI]</td>\n",
       "      <td>...</td>\n",
       "      <td>liʔ</td>\n",
       "      <td>lək</td>\n",
       "      <td>liʔ</td>\n",
       "      <td>cvc</td>\n",
       "      <td>cvc</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185723 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         token  utterance_id               gloss  transcript_id  \\\n",
       "42       mommy      16759315               Mommy          42204   \n",
       "170        wee      16759467                 wee          42204   \n",
       "201        wee      16759501                 wee          42204   \n",
       "239        woo      16759549                 woo          42204   \n",
       "743      ernie      16759752               Ernie          42204   \n",
       "...        ...           ...                 ...            ...   \n",
       "3083575   help      17280891                help          42569   \n",
       "3083589  hates      17280946  nobody hates Simba          42569   \n",
       "3083594     oh      17280964   oh why lick hippo          42569   \n",
       "3083595    why      17280964   oh why lick hippo          42569   \n",
       "3083596   lick      17280964   oh why lick hippo          42569   \n",
       "\n",
       "         utterance_order target_child_name speaker_code               type  \\\n",
       "42                     6              Alex          CHI        declarative   \n",
       "170                   24              Alex          CHI        declarative   \n",
       "201                   28              Alex          CHI        declarative   \n",
       "239                   33              Alex          CHI        declarative   \n",
       "743                   58              Alex          CHI        declarative   \n",
       "...                  ...               ...          ...                ...   \n",
       "3083575              752           William          CHI        declarative   \n",
       "3083589              755           William          CHI        declarative   \n",
       "3083594              756           William          CHI  self interruption   \n",
       "3083595              756           William          CHI  self interruption   \n",
       "3083596              756           William          CHI  self interruption   \n",
       "\n",
       "        punct speaker_code_simple  ... actual_phonology_clean  \\\n",
       "42          .               [CHI]  ...                    ɑmə   \n",
       "170         .               [CHI]  ...                     wi   \n",
       "201         .               [CHI]  ...                     wi   \n",
       "239         .               [CHI]  ...                      u   \n",
       "743         .               [CHI]  ...                      ɛ   \n",
       "...       ...                 ...  ...                    ...   \n",
       "3083575     .               [CHI]  ...                    ɛlp   \n",
       "3083589     .               [CHI]  ...                   heəs   \n",
       "3083594     .               [CHI]  ...                     oʊ   \n",
       "3083595     .               [CHI]  ...                    wɑə   \n",
       "3083596     .               [CHI]  ...                    liʔ   \n",
       "\n",
       "         model_phonology_no_dia  actual_phonology_no_dia cv_raw cv_collapsed  \\\n",
       "42                         mɑmi                      ɑmə    vcv          vcv   \n",
       "170                          wi                       wi     cv           cv   \n",
       "201                          wi                       wi     cv           cv   \n",
       "239                          wu                        u      v            v   \n",
       "743                        əɹni                        ɛ      v            v   \n",
       "...                         ...                      ...    ...          ...   \n",
       "3083575                    hɛlp                      ɛlp    vcc           vc   \n",
       "3083589                   heəts                     heəs   cvvc          cvc   \n",
       "3083594                     oʊʊ                       oʊ     vv            v   \n",
       "3083595                     wɑə                      wɑə    cvv           cv   \n",
       "3083596                     lək                      liʔ    cvc          cvc   \n",
       "\n",
       "         num_vowels  in_vocab success_token yyy_token partition  \n",
       "42              2.0      True          True     False   success  \n",
       "170             1.0      True          True     False   success  \n",
       "201             1.0      True          True     False   success  \n",
       "239             1.0      True          True     False   success  \n",
       "743             1.0      True          True     False   success  \n",
       "...             ...       ...           ...       ...       ...  \n",
       "3083575         1.0      True          True     False   success  \n",
       "3083589         1.0      True          True     False   success  \n",
       "3083594         1.0      True          True     False   success  \n",
       "3083595         1.0      True          True     False   success  \n",
       "3083596         1.0      True          True     False   success  \n",
       "\n",
       "[185723 rows x 28 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens_phono.loc[(all_tokens_phono['success_token']) & \n",
    "    (all_tokens_phono['num_vowels'] <= 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27693, 28)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yyy_tokens = all_tokens_phono.loc[(all_tokens_phono['yyy_token']) & \n",
    "(all_tokens_phono['token'] == 'yyy') & (all_tokens_phono.num_vowels <= 2) ]\n",
    "all_tokens_phono.loc[(all_tokens_phono['yyy_token']) & \n",
    "(all_tokens_phono['token'] == 'yyy') & (all_tokens_phono.num_vowels <= 2),'partition'] = 'yyy'\n",
    "yyy_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none       2870209\n",
       "success     185723\n",
       "yyy          27693\n",
       "Name: partition, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens_phono.partition.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_phono.loc[all_tokens_phono.token == 'xxx','token_id'] = initial_tokenizer.unk_token_id\n",
    "all_tokens_phono.loc[all_tokens_phono.token == 'yyy','token_id'] = initial_tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this adds the partition information\n",
    "\n",
    "final_save_path = join(config.prov_csv_dir, 'pvd_utt_glosses_phono_cleaned_inflated_to_next_notebook.pkl')\n",
    "all_tokens_phono.to_pickle(final_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prevalence of Successes and Failures Across Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwong/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/nwong/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# get number of tokens per age\n",
    "success_utts['set'] = 'success'\n",
    "yyy_utts['set'] = 'failure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get child age in days associated with each utterance id and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_age = chi_phono.groupby('utterance_id').target_child_age.agg(np.unique).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "utts_with_ages = pd.concat([success_utts, yyy_utts]).merge(utt_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5    9919\n",
      "2.0    7261\n",
      "1.0    6693\n",
      "2.5    4895\n",
      "3.0    2097\n",
      "3.5     414\n",
      "0.5     167\n",
      "4.0      11\n",
      "Name: year, dtype: int64\n",
      "2.0    22432\n",
      "2.5    21194\n",
      "1.5    16798\n",
      "3.0    12564\n",
      "1.0     6697\n",
      "3.5     3683\n",
      "4.0      379\n",
      "0.5      133\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "utts_with_ages['year'] = .5*np.floor(utts_with_ages['target_child_age'] / (365. /2) ) \n",
    "print(utts_with_ages.loc[utts_with_ages.set == 'failure'].year.value_counts())\n",
    "print(utts_with_ages.loc[utts_with_ages.set == 'success'].year.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_utts_save_path = join(config.prov_csv_dir, 'utts_with_ages.csv')\n",
    "utts_with_ages.to_csv(final_utts_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'yyy' not in set(success_tokens_check['token'])\n",
    "# This was the problem that was observed in my iteration of the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "child-listening-env",
   "language": "python",
   "name": "child-listening-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
