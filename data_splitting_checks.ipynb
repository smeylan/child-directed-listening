{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# 7/22/21: https://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# end cite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from os.path import join, exists\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from utils import load_splits, split_gen, data_cleaning\n",
    "from utils_child import child_models\n",
    "\n",
    "import config\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sampled_proper_attributes(phono, utt_id_set):\n",
    "    \n",
    "    \"\"\"\n",
    "    Ensures that a given df with tokens that are part of a sample follow the proper attributes\n",
    "    (actual_phonology and model_phonology are populated, and CHI utterance)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = phono[phono.utterance_id.isin(utt_id_set)]\n",
    "    \n",
    "    assert all(df.speaker_code_simple == '[CHI]')\n",
    "    assert has_phonology(df)\n",
    "    assert has_success_or_yyy(df) # Also checked for more stringent criteria later (has success and yyy)\n",
    "    \n",
    "def check_chi_cgv_present(this_df):\n",
    "    this_codes = set(this_df.speaker_code)\n",
    "    assert this_codes.issubset({'CHI', 'FAT', 'MOT'}) and this_codes != {'CHI'}\n",
    "        \n",
    "def load_marked_pooled_data(this_split_path):\n",
    "    return pd.read_pickle(join(this_split_path, 'data_pool_with_phases.pkl'))\n",
    "\n",
    "def check_disjoint_and_phase_written(split, name, base_dir):\n",
    "    \"\"\"\n",
    "    Checks that the phase data indicated per entry corresponds to the written text in the file.\n",
    "    \"\"\"\n",
    "    \n",
    "    this_split_loc = split_gen.get_split_folder(split, name, base_dir)\n",
    "    \n",
    "    if split in {'all', 'age'}:\n",
    "        \n",
    "        this_all_loc = split_gen.get_split_folder('all', 'all', base_dir)\n",
    "        this_pool_data = load_marked_pooled_data(this_all_loc)\n",
    "        if name == 'young':\n",
    "            this_pool_data, _ = split_gen.get_age_split_data(this_pool_data)\n",
    "        if name == 'old':\n",
    "            _, this_pool_data = split_gen.get_age_split_data(this_pool_data)\n",
    "            \n",
    "    if split == 'child':\n",
    "        \n",
    "        # Check that the train/val text files are as expected\n",
    "        #     they match the right phase as marked in the df\n",
    "        #     they match the right child\n",
    "        #     they don't contain errors\n",
    "\n",
    "        this_pool_data = data_cleaning.drop_errors(load_splits.load_phono())\n",
    "        this_pool_data = this_pool_data[(this_pool_data.target_child_name == name)]\n",
    "        this_pool_data = this_pool_data[['utterance_id', 'gloss_with_punct', 'phase_child_finetune']].drop_duplicates()\n",
    "        \n",
    "    for phase in ['train', 'val']:\n",
    "        phase_locs = this_pool_data[this_pool_data[which_phase] == phase]\n",
    "        with open(join(this_split_loc, f\"{phase}.txt\"), 'r') as f:\n",
    "            from_text_text = sorted([l.strip() for l in f.readlines()]) # Get rid of trailing \\n\n",
    "        \n",
    "        from_df_text = sorted(list(phase_locs['gloss_with_punct']))\n",
    "        \n",
    "        assert from_text_text == from_df_text, f'Failed to match phase data for: {split}, {name}, {phase}'\n",
    "    \n",
    "    print(f'Assert passed for {split}, {name}')\n",
    "    return True\n",
    "\n",
    "\n",
    "def has_phonology(df):\n",
    "    \n",
    "    def non_empty(entry, attribute):\n",
    "        return any(entry != '')\n",
    "    \n",
    "    actual = df.groupby('utterance_id').actual_phonology.agg(lambda x : non_empty(x, 'actual_phonology')).reset_index()\n",
    "    \n",
    "    return all(actual.actual_phonology)\n",
    "    \n",
    "def has_this(collect, token_type):\n",
    "    return (token_type in set(collect))\n",
    "\n",
    "def give_success(df):\n",
    "    return df.groupby('utterance_id').partition.agg(lambda x : has_this(x, 'success')).reset_index()\n",
    "\n",
    "def give_yyy(df):\n",
    "    return df.groupby('utterance_id').partition.agg(lambda x : has_this(x, 'yyy')).reset_index()\n",
    "    \n",
    "def has_success(df):\n",
    "    return all(give_success(df).partition)\n",
    "\n",
    "def has_yyy(df):\n",
    "    return all(give_yyy(df).partition)\n",
    "\n",
    "def has_success_or_yyy(df):\n",
    "    \n",
    "    success_df = give_success(df)\n",
    "    yyy_df = give_yyy(df)\n",
    "    \n",
    "    either = success_df.partition | yyy_df.partition\n",
    "    \n",
    "    return all(either)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_names = child_models.get_child_names()\n",
    "\n",
    "all_phono = pd.read_pickle(join(config.prov_dir, 'pvd_all_tokens_phono_for_eval.pkl'))\n",
    "\n",
    "phases = ['train', 'val', 'eval']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Providence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of config failed: Traceback (most recent call last):\n",
      "  File \"/home/nwong/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/nwong/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/nwong/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/nwong/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 860, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 791, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/config.py\", line 40\n",
      "    model_id = # Update this manually to update models used for beta, across time -- this will eventually correspond to a date auto-generated by training.\n",
      "                                                                                                                                                         ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert passed for all, all\n",
      "Assert passed for age, young\n",
      "Assert passed for age, old\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Are the training/text files disjoint for non-Providence?\n",
    "\n",
    "check_disjoint_and_phase_written('all', 'all', config.finetune_dir, which_phase = 'phase_finetune')\n",
    "check_disjoint_and_phase_written('age', 'young', config.finetune_dir, which_phase = 'phase_finetune')\n",
    "check_disjoint_and_phase_written('age', 'old', config.finetune_dir, which_phase = 'phase_finetune')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/finetune/all/all/data_pool_with_phases.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-e00000537b88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_split_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mall_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_marked_pooled_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0myoung_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_age_split_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-9617aaeeb984>\u001b[0m in \u001b[0;36mload_marked_pooled_data\u001b[0;34m(this_split_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_marked_pooled_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_split_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_split_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_pool_with_phases.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_disjoint_and_phase_written\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'phase'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     ) as handles:\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chompsky/childes/child_listening_continuation/child-listening-env/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/finetune/all/all/data_pool_with_phases.pkl'"
     ]
    }
   ],
   "source": [
    "# Make sure that [CHI], [CGV] are present in the model inputs\n",
    "    \n",
    "folder = split_gen.get_split_folder('all', 'all', config.finetune_dir)\n",
    "\n",
    "all_df = load_marked_pooled_data(folder)\n",
    "young_df, old_df = split_gen.get_age_split_data(all_df)\n",
    "\n",
    "for s, d in config.childes_model_args:\n",
    "    \n",
    "    if d == 'old':\n",
    "        this_df = old_df\n",
    "    if d == 'young':\n",
    "        this_df = young_df\n",
    "    if d == 'all':\n",
    "        this_df = all_df\n",
    "            \n",
    "    check_chi_cgv_present(this_df)\n",
    "\n",
    "print('Passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying: Alex\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/finetune/child/Alex/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-992eb5a1ed41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchild_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Verifying: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcheck_disjoint_and_phase_written\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'child'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'phase_child_finetune'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-9617aaeeb984>\u001b[0m in \u001b[0;36mcheck_disjoint_and_phase_written\u001b[0;34m(split, name, base_dir, which_phase)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mphase_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_pool_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthis_pool_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwhich_phase\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_split_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{phase}.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mfrom_text_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get rid of trailing \\n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/nwong/chompsky/childes/child_listening_continuation/child-directed-listening/finetune/child/Alex/train.txt'"
     ]
    }
   ],
   "source": [
    "# Check the text files for disjointedness (necessary because it splits on a different phase label)\n",
    "# Note this also checks the correctness of train and val partition.\n",
    "\n",
    "for name in child_names:\n",
    "    print(f\"Verifying: {name}\")\n",
    "    check_disjoint_and_phase_written('child', name, config.finetune_dir, which_phase = 'phase_child_finetune')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n",
      "Size of phase in transcripts 87\n",
      "\tfor success, number of unique transcripts: 87\n",
      "\tfor yyy, number of unique transcripts: 87\n",
      "Size of phase in transcripts 87\n",
      "\tfor success, number of unique transcripts: 87\n",
      "\tfor yyy, number of unique transcripts: 87\n",
      "Size of phase in transcripts 173\n",
      "\tfor success, number of unique transcripts: 173\n",
      "\tfor yyy, number of unique transcripts: 173\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Check that train, val, eval are disjoint for the age/all splits.\n",
    "\n",
    "phase_set = {}\n",
    "\n",
    "for phase in phases:\n",
    "    phase_set[phase] = set(all_phono[all_phono['phase_sample'] == phase].transcript_id)\n",
    "\n",
    "for p1 in phases:\n",
    "    for p2 in phases:\n",
    "        if p1 == p2 : continue\n",
    "        assert not (phase_set[p1] & phase_set[p2]), f'Overlap for {p1} and {p2}'\n",
    "\n",
    "print('Passed')\n",
    "\n",
    "# For phase sample, for each of success and yyy,\n",
    "# there should be a 25/50/50 split on transcript ids for each of these,\n",
    "# that is about equal for success and yyy\n",
    "\n",
    "for phase in phases:\n",
    "    \n",
    "    print('Size of phase in transcripts', len(set(all_phono[all_phono['phase_sample'] == phase].transcript_id)))\n",
    "    \n",
    "    for part_type in ['success', 'yyy']:\n",
    "        \n",
    "        success_ids_num = len(set(all_phono[all_phono.transcript_id.isin(phase_set[phase]) & (all_phono.partition == part_type)].transcript_id))\n",
    "        print(f'\\tfor {part_type}, number of unique transcripts:', success_ids_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed asserts\n"
     ]
    }
   ],
   "source": [
    "# Make sure that all of the eval and val data are separate for across_time_samples\n",
    "\n",
    "all_time_samples = glob.glob(join(config.prov_dir, 'across_time_samples/*'))\n",
    "\n",
    "val_ids = set(pd.concat([pd.read_csv(path) for path in all_time_samples if '_val' in path]).utterance_id)\n",
    "eval_ids = set(pd.concat([pd.read_csv(path) for path in all_time_samples if '_eval' in path]).utterance_id)\n",
    "\n",
    "val_phases = set(all_phono[all_phono.utterance_id.isin(val_ids)].phase_sample)\n",
    "eval_phases = set(all_phono[all_phono.utterance_id.isin(eval_ids)].phase_sample)\n",
    "\n",
    "assert val_phases == {'val'}\n",
    "assert eval_phases == {'eval'}\n",
    "\n",
    "sampled_proper_attributes(all_phono, val_ids | eval_ids)\n",
    "\n",
    "print('Passed asserts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Phase: val\n",
      "For success\n",
      "Sample shapes\n",
      "\t\tNumber of total transcripts in all pool: (1, 2)\n",
      "\t\tNumber of total transcripts in all pool: (2316, 2)\n",
      "\t\tNumber of total transcripts in all pool: (5000, 2)\n",
      "\t\tNumber of total transcripts in all pool: (4113, 2)\n",
      "\t\tNumber of total transcripts in all pool: (5000, 2)\n",
      "\t\tNumber of total transcripts in all pool: (5000, 2)\n",
      "\t\tNumber of total transcripts in all pool: (699, 2)\n",
      "\t\tNumber of total transcripts in all pool: (379, 2)\n",
      "For yyy\n",
      "Sample shapes\n",
      "\t\tNumber of total transcripts in all pool: (1, 2)\n",
      "\t\tNumber of total transcripts in all pool: (1343, 2)\n",
      "\t\tNumber of total transcripts in all pool: (1060, 2)\n",
      "\t\tNumber of total transcripts in all pool: (493, 2)\n",
      "\t\tNumber of total transcripts in all pool: (669, 2)\n",
      "\t\tNumber of total transcripts in all pool: (262, 2)\n",
      "\t\tNumber of total transcripts in all pool: (19, 2)\n",
      "\t\tNumber of total transcripts in all pool: (2, 2)\n",
      "******************** Phase: eval\n",
      "For success\n",
      "Sample shapes\n",
      "\t\tNumber of total transcripts in all pool: (97, 2)\n",
      "\t\tNumber of total transcripts in all pool: (2691, 2)\n",
      "\t\tNumber of total transcripts in all pool: (5000, 2)\n",
      "\t\tNumber of total transcripts in all pool: (5000, 2)\n",
      "\t\tNumber of total transcripts in all pool: (5000, 2)\n",
      "\t\tNumber of total transcripts in all pool: (5000, 2)\n",
      "\t\tNumber of total transcripts in all pool: (1971, 2)\n",
      "\t\tNumber of total transcripts in all pool: (0, 2)\n",
      "For yyy\n",
      "Sample shapes\n",
      "\t\tNumber of total transcripts in all pool: (101, 2)\n",
      "\t\tNumber of total transcripts in all pool: (1921, 2)\n",
      "\t\tNumber of total transcripts in all pool: (3253, 2)\n",
      "\t\tNumber of total transcripts in all pool: (1364, 2)\n",
      "\t\tNumber of total transcripts in all pool: (604, 2)\n",
      "\t\tNumber of total transcripts in all pool: (345, 2)\n",
      "\t\tNumber of total transcripts in all pool: (49, 2)\n",
      "\t\tNumber of total transcripts in all pool: (0, 2)\n"
     ]
    }
   ],
   "source": [
    "# For the used ages, check the following:\n",
    "\n",
    "# there are approx 5000 of them (by utterance id!)\n",
    "#     some may have fewer because of data sparsity.\n",
    "\n",
    "# per EACH of successes and yyy\n",
    "# And, they correspond to the right splits (the one in their name)\n",
    "\n",
    "for phase in ['val', 'eval']:\n",
    "    \n",
    "    print('*'*20, f\"Phase: {phase}\")\n",
    "    \n",
    "    for data_type, data_func in zip(['success', 'yyy'], [load_splits.get_age_success_sample_paths, load_splits.get_age_yyy_sample_paths]):\n",
    "\n",
    "        print(f'For {data_type}')\n",
    "        all_paths = data_func(phase = phase)\n",
    "        all_samples = [ pd.read_csv(p) for p in all_paths ]\n",
    "\n",
    "        sample_ids = set(pd.concat(all_samples).utterance_id)\n",
    "\n",
    "        print('Sample shapes')\n",
    "        for sample in all_samples:\n",
    "            print(f'\\t\\tNumber of total transcripts in all pool: {sample.shape}')\n",
    "\n",
    "        this_phono = all_phono[all_phono.utterance_id.isin(sample_ids)]\n",
    "        if data_type == 'success':\n",
    "            assert has_success(this_phono)\n",
    "        else:\n",
    "            assert has_yyy(this_phono)\n",
    "\n",
    "# Age 3.5 can be empty\n",
    "# but on the run where it was observed,\n",
    "# it seems that there are only 13 transcripts -- in a development run 10 were assigned to eval\n",
    "# and 3 were assigned to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Make sure:\n",
    "# The young data is actually young data\n",
    "# The old data is actually old data\n",
    "\n",
    "old_ids = set(pd.read_csv(join(split_gen.get_split_folder('age', 'old', config.prov_dir), 'success_utts_beta_5000_val.csv')).utterance_id)\n",
    "assert all(all_phono[all_phono.utterance_id.isin(old_ids)].target_child_age > config.age_split * 30.5)\n",
    "\n",
    "young_ids = set(pd.read_csv(join(split_gen.get_split_folder('age', 'young', config.prov_dir), 'success_utts_beta_5000_val.csv')).utterance_id)\n",
    "assert all(all_phono[all_phono.utterance_id.isin(young_ids)].target_child_age <= config.age_split * 30.5)\n",
    "\n",
    "print('Passed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for phase_sample\n",
      "Passed asserts\n",
      "Dataset all all\n",
      "\tNumber of utterances: 5000\n",
      "Dataset age young\n",
      "\tNumber of utterances: 5000\n",
      "Dataset age old\n",
      "\tNumber of utterances: 5000\n",
      "Checking for phase_child_sample\n",
      "Passed asserts\n",
      "Dataset child Alex\n",
      "\tNumber of utterances: 1808\n",
      "Dataset child Ethan\n",
      "\tNumber of utterances: 896\n",
      "Dataset child Lily\n",
      "\tNumber of utterances: 1815\n",
      "Dataset child Naima\n",
      "\tNumber of utterances: 2146\n",
      "Dataset child Violet\n",
      "\tNumber of utterances: 1358\n",
      "Dataset child William\n",
      "\tNumber of utterances: 1355\n"
     ]
    }
   ],
   "source": [
    "# For the beta samples,\n",
    "# they are all successes\n",
    "# there is approx 5000 of them (by utterance id!)\n",
    "# they all have phase == 'val'\n",
    "\n",
    "def check_beta_samples(arg_set, phase_label):\n",
    "    \n",
    "    print(f'Checking for {phase_label}')\n",
    "    \n",
    "    all_beta_samples = [\n",
    "        pd.read_csv(join(split_gen.get_split_folder(s, d, config.prov_dir), 'success_utts_beta_5000_val.csv'))\n",
    "        for s, d in arg_set\n",
    "    ]\n",
    "\n",
    "    beta_ids = set(pd.concat(all_beta_samples).utterance_id)\n",
    "    sel_phono = all_phono[all_phono.utterance_id.isin(beta_ids)]\n",
    "    beta_phases = set(sel_phono[phase_label])\n",
    "\n",
    "    assert beta_phases == {'val'}\n",
    "    assert has_success(sel_phono)\n",
    "    \n",
    "    # You should have at least one success per utterance, but not all tokens have to be marked as a success.\n",
    "\n",
    "    print('Passed asserts')\n",
    "\n",
    "    for (s, d), (beta_sample) in zip(arg_set, all_beta_samples):\n",
    "        print('Dataset', s, d)\n",
    "        print(f'\\tNumber of utterances: {len(set(beta_sample.utterance_id))}')\n",
    "        \n",
    "    sampled_proper_attributes(all_phono, beta_ids)\n",
    "    \n",
    "check_beta_samples(config.childes_model_args, 'phase_sample')\n",
    "check_beta_samples([('child', name) for name in child_names], 'phase_child_sample')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing across time for split: all, all\n",
      "     year\n",
      "4.0    61\n",
      "3.5   148\n",
      "1.0   420\n",
      "2.0   780\n",
      "1.5   952\n",
      "3.0  1107\n",
      "2.5  1532\n",
      "Analyzing across time for split: age, young\n",
      "     year\n",
      "0.5     1\n",
      "1.0   580\n",
      "2.0  1028\n",
      "1.5  1295\n",
      "2.5  2096\n",
      "Analyzing across time for split: age, old\n",
      "     year\n",
      "4.0   264\n",
      "3.5   487\n",
      "3.0  4249\n",
      "Analyzing across time for split: child, Alex\n",
      "     year\n",
      "1.0    16\n",
      "1.5   278\n",
      "2.0   363\n",
      "3.0   547\n",
      "2.5   604\n",
      "Analyzing across time for split: child, Ethan\n",
      "     year\n",
      "0.5     1\n",
      "1.5   143\n",
      "2.0   168\n",
      "2.5   222\n",
      "1.0   362\n",
      "Analyzing across time for split: child, Lily\n",
      "     year\n",
      "1.0    18\n",
      "2.5   178\n",
      "2.0   303\n",
      "1.5   398\n",
      "3.0   457\n",
      "3.5   461\n",
      "Analyzing across time for split: child, Naima\n",
      "     year\n",
      "1.0   319\n",
      "1.5   349\n",
      "2.5   361\n",
      "3.5   362\n",
      "3.0   376\n",
      "2.0   379\n",
      "Analyzing across time for split: child, Violet\n",
      "     year\n",
      "1.0     2\n",
      "1.5   141\n",
      "3.5   198\n",
      "3.0   203\n",
      "2.5   367\n",
      "2.0   447\n",
      "Analyzing across time for split: child, William\n",
      "     year\n",
      "1.0    67\n",
      "1.5   161\n",
      "2.5   320\n",
      "2.0   344\n",
      "3.0   463\n"
     ]
    }
   ],
   "source": [
    "# Check if my samples are sufficiently across time (beta)\n",
    "\n",
    "all_beta_args = config.childes_model_args + [('child', name) for name in child_names]\n",
    "\n",
    "for (s, d) in all_beta_args:\n",
    "    \n",
    "    print(f'Analyzing across time for split: {s}, {d}')\n",
    "    this_sample = set(pd.read_csv(join(split_gen.get_split_folder(s, d, config.prov_dir), 'success_utts_beta_5000_val.csv')).utterance_id)\n",
    "    \n",
    "    # Select utterances in the beta sample\n",
    "    sel = all_phono[all_phono.utterance_id.isin(this_sample)][['utterance_id', 'year']].drop_duplicates()\n",
    "    \n",
    "    # Year actually means counts of certain years, not the years themselves.\n",
    "    counts = sel.year.value_counts().to_frame().sort_values('year')\n",
    "    \n",
    "    print(counts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts passed.\n"
     ]
    }
   ],
   "source": [
    "# Make sure the child train/val/eval data is separate within child\n",
    "# Note this no longer requires/uses the constraint that child val/eval matches the overall val/eval.\n",
    "\n",
    "\n",
    "for attr, phase_set in zip(['phase_child_sample', 'phase_child_finetune'], [phases[:], ['train', 'val']]):\n",
    "                                                                            \n",
    "    for name in child_names:\n",
    "\n",
    "        child_pool = all_phono[all_phono.target_child_name == name]\n",
    "        ids = {}\n",
    "        \n",
    "        for phase in phase_set:\n",
    "            ids[phase] = set(child_pool[child_pool[attr] == phase].utterance_id)\n",
    "\n",
    "        for p1 in phase_set:\n",
    "            for p2 in phase_set:\n",
    "                if p1 == p2: continue\n",
    "                assert len(ids[p1] & ids[p2]) == 0, f'{attr}, {p1}, {p2}'\n",
    "    \n",
    "print('Asserts passed.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of phase: train 279\n",
      "Size of phase: val 33\n",
      "Size of phase: eval 35\n",
      "Size of phase: train 291\n",
      "Size of phase: val 38\n",
      "Passed.\n"
     ]
    }
   ],
   "source": [
    "# Other quick checks\n",
    "\n",
    "# Size of the train relative to val, eval for Providence\n",
    "# For all/all, age/old, age/young\n",
    "\n",
    "phase_data = {}\n",
    "\n",
    "for phase_type, phase_set in zip(['phase_child_sample', 'phase_child_finetune'], [phases[:], ['train', 'val']]):\n",
    "    for phase in phase_set:\n",
    "        print(f'Size of phase: {phase}', len(set(all_phono[all_phono[phase_type] == phase].transcript_id)))\n",
    "    \n",
    "\n",
    "# Make sure that there is a spread of age sampling for test\n",
    "# And also val\n",
    "\n",
    "for name in child_names:\n",
    "    \n",
    "    name_pool = all_phono[all_phono.target_child_name == name]\n",
    "    \n",
    "    for phase in ['eval', 'val']:\n",
    "        this_pool = name_pool[name_pool.phase_child_sample == phase]\n",
    "        # But, you need this per transcript.\n",
    "        \n",
    "        all_ages = data_cleaning.get_years(this_pool)\n",
    "        \n",
    "        # For the val/eval samples \n",
    "        # There is about the right number of transcripts for eval and val (print out the numbers)\n",
    "\n",
    "        for age in all_ages: \n",
    "            \n",
    "            this_sel_df = this_pool[this_pool.year == age]\n",
    "            \n",
    "            get_num_transcripts = lambda df : len(set(df.transcript_id))\n",
    "            all_num = get_num_transcripts(this_sel_df)\n",
    "            success_num = get_num_transcripts(this_sel_df[this_sel_df.partition == \"success\"])\n",
    "            yyy_num = get_num_transcripts(this_sel_df[this_sel_df.partition == \"yyy\"])\n",
    "            \n",
    "            # Request one transcript with both present.\n",
    "            # Expected behavior below for revised sampling with success and yyy present constraint.\n",
    "            \n",
    "            assert success_num == yyy_num == all_num == 1\n",
    "         \n",
    "print('Passed.')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Child work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n"
     ]
    }
   ],
   "source": [
    "# The phases are all disjoint\n",
    "\n",
    "phase_dict = defaultdict(dict)\n",
    "\n",
    "for phase_type, phase_set in zip(['phase_child_sample', 'phase_child_finetune'], [phases[:], ['train', 'val']]):\n",
    "    \n",
    "    for phase in phase_set:\n",
    "        phase_dict[phase_type][phase] = set(all_phono[all_phono[phase_type] == phase].transcript_id)\n",
    "        \n",
    "    # train/val/eval in phase_child_sample, as well as phase_child_finetune\n",
    "    this_type_dict = phase_dict[phase_type]\n",
    "    for p1 in phase_set:\n",
    "        for p2 in phase_set:\n",
    "            if p1 == p2: continue\n",
    "            assert not (this_type_dict[p1] & this_type_dict[p2])\n",
    "\n",
    "# The finetune phases are all disjoint from val/eval phases in sample\n",
    "# The train_sample phase is in the finetune_phase\n",
    "\n",
    "finetune_disjoint = phase_dict['phase_child_finetune']['train'] | phase_dict['phase_child_finetune']['val']\n",
    "sample_disjoint = phase_dict['phase_child_sample']['eval']\n",
    "\n",
    "\n",
    "# 7/25/21: https://www.geeksforgeeks.org/issubset-in-python/\n",
    "for phase in ['train', 'val']:\n",
    "    valid_for_finetune_partial = (phase_dict['phase_child_sample'][phase]) & set(data_cleaning.drop_errors(all_phono).transcript_id)\n",
    "    assert valid_for_finetune_partial.issubset(phase_dict['phase_child_finetune'][phase])\n",
    "# end cite\n",
    "\n",
    "assert len(finetune_disjoint & sample_disjoint) == 0\n",
    "    \n",
    "print('Passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n"
     ]
    }
   ],
   "source": [
    "for name in child_names:\n",
    "    for phase in ['train', 'val']:\n",
    "        # Make sure there is cgv, chi in both phases for child data finetuning\n",
    "        rel_df = all_phono[(all_phono.target_child_name == name) & (all_phono.phase_child_finetune == phase)]\n",
    "        check_chi_cgv_present(rel_df)\n",
    "        \n",
    "print('Passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed for phase_child_sample, phase: train\n",
      "Passed for phase_child_sample, phase: val\n",
      "Passed for phase_child_sample, phase: test\n",
      "Passed for phase_sample, phase: train\n",
      "Passed for phase_sample, phase: val\n",
      "Passed for phase_sample, phase: test\n",
      "Passed\n"
     ]
    }
   ],
   "source": [
    "# Write a check to ensure that if phase_child_sample (or phase_sample) then\n",
    "# every transcript contains both at least one yyy and at least one success\n",
    "\n",
    "def has_success_and_yyy(this_attr):\n",
    "\n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        \n",
    "        has_phase = all_phono[all_phono[this_attr] == phase]\n",
    "\n",
    "        has_success_phase = set(has_phase[has_phase.partition == 'success'].transcript_id)\n",
    "        has_yyy_phase = set(has_phase[has_phase.partition == 'yyy'].transcript_id)\n",
    "        gen_phase = set(has_phase.transcript_id)\n",
    "\n",
    "        # Note: no constraint on at least one yyy and at least one success for the finetune data.\n",
    "\n",
    "        assert has_success_phase == has_yyy_phase == gen_phase\n",
    "\n",
    "        print(f'Passed for {this_attr}, phase: {phase}')\n",
    "\n",
    "has_success_and_yyy('phase_child_sample')\n",
    "has_success_and_yyy('phase_sample')\n",
    "\n",
    "print('Passed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "child-listening-env",
   "language": "python",
   "name": "child-listening-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
