{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from os.path import join, exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from utils import load_splits, split_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_SAVE_PATH = 'eval/new_splits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_data_all(split_name, dataset_name, base_dir):\n",
    "    \n",
    "    phono_filename = 'pvd_utt_glosses_phono_cleaned_inflated.pkl'\n",
    "    success_utts_filename = 'success_utts.csv'\n",
    "    yyy_utts_filename = 'yyy_utts.csv'\n",
    "\n",
    "    data_filenames = [phono_filename, success_utts_filename, yyy_utts_filename]\n",
    "    this_folder_path = split_gen.get_split_folder(split_name, dataset_name, base_dir)\n",
    "    return {f : pd.read_csv(join(this_folder_path, f)) for f in filenames }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For making sure that the generated training files are disjoint.\n",
    "\n",
    "def disjoint_check_age_all(train_text_path, val_text_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Used to check that the finetuning data is correctly split between train and val.\n",
    "    This applies to the 'all' and 'age' splits.\n",
    "    \n",
    "    The evaluation text is definitely different for all, age because they are from different datasets\n",
    "        (manually checked)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 6/25/21 https://docs.python.org/3/library/collections.html\n",
    "    \n",
    "    with open(train_text_path, 'r') as f:\n",
    "        train_text = f.readlines()\n",
    "        assert set(train_text) == len(train_text), 'Not all utterances are unique. This is strictly not needed, but the check assumes this to not raise false positive.'\n",
    "    with open(val_text_path, 'r') as f:\n",
    "        val_text = f.readlines()\n",
    "        assert set(val_text) == len(val_text), 'Not all utterances are unique. This is strictly not needed, but the check assumes this to not raise false positive.'\n",
    "        \n",
    "    counts = Counter(train_text + val_text)\n",
    "    elem_types = set(counts.elements()) \n",
    "    \n",
    "    # Note: This can be true if the same sentence, but different utterance,\n",
    "    # appears more than once within the train and val sets themselves -- this case is OK.\n",
    "    \n",
    "    assert all(counts[e] == 1 for e in elem_types), \"Items may appear more than once in the union of train and val.\"\n",
    "    assert len(elem_types) == len(train_text) + len(val_text)\n",
    "    # Above: not strictly needed condition, but combined with above show that dataset merges to the whole.\n",
    "    \n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_dataset_dict = load_eval_data_all('all', 'all', BASE_SAVE_PATH)['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Disjoint sets for \"all\" split\n",
    "\n",
    "all_split_paths = load_splits.load_splits_folder_text('all', 'data/new_splits')\n",
    "assert len(all_split_paths) == 1\n",
    "\n",
    "disjoint_check_age_all(age_split_paths['all']['train'], age_split_paths['all']['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "age_split_paths = load_splits.load_splits_folder_text('age', 'data/new_splits')\n",
    "\n",
    "# Check for disjointedness within each of the age splits.\n",
    "\n",
    "for k in age_split_paths.keys():\n",
    "    disjoint_check_age_all(age_split_paths[k]['train'], age_split_paths[k]['val'])\n",
    "    \n",
    "# 2) Need to check that success and yyy dataframes all correspond to the proper age\n",
    "\n",
    "young_df_dict = load_eval_data_all('age', 'young', BASE_SAVE_PATH)\n",
    "old_df_dict = load_eval_data_all('age', 'old', BASE_SAVE_PATH)\n",
    "\n",
    "\n",
    "# This is in months\n",
    "assert all(young_df_dict[k]['target_age'] <= 36 for k in young_df_dict.keys()) \n",
    "assert all(old_df_dict[k]['target_child_age'] > 36 for k in old_df_dict.keys())\n",
    "\n",
    "\n",
    "# 3) Disjoint sets for young, old, and merge to form whole dataset\n",
    "for k in young_df_dict.keys():\n",
    "    this_entire_data = entire_dataset_dict[k].sort_values('utterance_id')\n",
    "    concat_df = pd.concat(young_df_dict[k], old_df_dict[k]).sort_values('utterance_id')\n",
    "    assert this_entire_data.equals(entire_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Do I want to check this?\n",
    "#name_split_paths = load_splits.load_splits_folder_text('child', 'data/new_splits')\n",
    "\n",
    "# how to find the names?\n",
    "names = glob.glob(join(BASE_SAVE_PATH, 'child/*'))\n",
    "\n",
    "print(names)\n",
    "\n",
    "name_split_data = \n",
    "names = name_split_paths.keys()\n",
    "\n",
    "# 4) Data always addresses the relevant child\n",
    "all_child_data = {}\n",
    "for name in names:\n",
    "    this_info_dict = load_eval_data_all('child', name, BASE_SAVE_PATH)\n",
    "    assert all(all(this_info_dict[k]['target_child_name'] == name) for k in this_info_dict.keys())\n",
    "    all_child_data[name] = this_info_dict\n",
    "\n",
    "# 5) Check that child data merges to the entire dataset?\n",
    "for k in entire_dataset_dict.keys():\n",
    "    \n",
    "    this_entire_data = entire_dataset_dict[k].sort_values('utterance_id')\n",
    "    concat_df = pd.concat([all_child_data[n][k] for n in names])\n",
    "\n",
    "    assert concat_df.equals(this_entire_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "child-listening-env",
   "language": "python",
   "name": "child-listening-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
