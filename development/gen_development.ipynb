{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface dataset version of childes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, exists\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests and verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules retrieved\n",
      "Downloading and preparing dataset childes/childes_data (download: 94.59 MiB, generated: 95.86 MiB, post-processed: Unknown size, total: 190.46 MiB) to ./user/wongn/temp_cache3/childes/childes_data/1.0.0/f8e634d7b7b5f2a90912db30fd2fa4d0cd911ac7fae250c1adeaedfcd8444872...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264036af51ab4a71a63031693f68eda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=81078591.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79b76f100de4162b5b945bd463e2cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=18109214.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0152d0453ede4bf2809a52698ede1d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae932c1034042ca9438f87525ab215f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset childes downloaded and prepared to ./user/wongn/temp_cache3/childes/childes_data/1.0.0/f8e634d7b7b5f2a90912db30fd2fa4d0cd911ac7fae250c1adeaedfcd8444872. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1290e92c9c4a5f873507ea955b2ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=722.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "first dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset childes (./user/wongn/temp_cache3/childes/childes_data/1.0.0/f8e634d7b7b5f2a90912db30fd2fa4d0cd911ac7fae250c1adeaedfcd8444872)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3eedb94ea934b81bccd721399a8ac77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=722.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "second dataset\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "import model_finetuning\n",
    "importlib.reload(model_finetuning)\n",
    "\n",
    "this_model, this_tokenizer = model_finetuning.get_bert_model()\n",
    "print('modules retrieved')\n",
    "maintain = model_finetuning.get_datasets(this_tokenizer, with_tags = True)\n",
    "print('first dataset')\n",
    "remove = model_finetuning.get_datasets(this_tokenizer, with_tags = False)\n",
    "print('second dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These should have chi and cgv as single tokens.\n",
      "['[CLS]', '[chi]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', '[cgv]', 'back', 'on', 'the', 'table', 'if', 'you', 'wanna', 'finish', 'it', '.', '[SEP]']\n",
      "These should not have chi and cgv inside.\n",
      "['[CLS]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', 'back', 'on', 'the', 'table', 'if', 'you', 'wanna', 'finish', 'it', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chi_test_idx = 0\n",
    "cgv_test_idx = 628\n",
    "\n",
    "run_maintain_idx = lambda test_idx : this_tokenizer.convert_ids_to_tokens(maintain['validation'][test_idx]['input_ids'])\n",
    "run_remove_idx = lambda test_idx : this_tokenizer.convert_ids_to_tokens(remove['validation'][test_idx]['input_ids'])\n",
    "\n",
    "\n",
    "print('These should have chi and cgv as single tokens.')\n",
    "print(run_maintain_idx(chi_test_idx))\n",
    "print(run_maintain_idx(cgv_test_idx))\n",
    "\n",
    "print('These should not have chi and cgv inside.')\n",
    "print(run_remove_idx(chi_test_idx))\n",
    "print(run_remove_idx(cgv_test_idx))\n",
    "\n",
    "# Verified 6/18 with fresh cache download!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to compare the two datasets that are being built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original dataset from Dr. Meylan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-9-73454d861dc4>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-73454d861dc4>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    if data_args.train_file is not None:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def orig_dataset_gen():\n",
    "    \n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    # Detecting last checkpoint.\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None:\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "\n",
    "    # Set seed before initializing model.\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    data_files = {}\n",
    "        if data_args.train_file is not None:\n",
    "            data_files[\"train\"] = data_args.train_file\n",
    "        if data_args.validation_file is not None:\n",
    "            data_files[\"validation\"] = data_args.validation_file\n",
    "        extension = data_args.train_file.split(\".\")[-1]\n",
    "        if extension == \"txt\":\n",
    "            extension = \"text\"\n",
    "        datasets = load_dataset(extension, data_files=data_files)\n",
    "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "    config_kwargs = {\n",
    "        \"cache_dir\": model_args.cache_dir,\n",
    "        \"revision\": model_args.model_revision,\n",
    "        \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "    }\n",
    "    if model_args.config_name:\n",
    "        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "    elif model_args.model_name_or_path:\n",
    "        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[model_args.model_type]()\n",
    "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "    tokenizer_kwargs = {\n",
    "        \"cache_dir\": model_args.cache_dir,\n",
    "        \"use_fast\": model_args.use_fast_tokenizer,\n",
    "        \"revision\": model_args.model_revision,\n",
    "        \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "    }\n",
    "    if model_args.tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "    elif model_args.model_name_or_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "        )\n",
    "\n",
    "    if model_args.model_name_or_path:\n",
    "        model = AutoModelForMaskedLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Training new model from scratch\")\n",
    "        model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # First we tokenize all the texts.\n",
    "    if training_args.do_train:\n",
    "        column_names = datasets[\"train\"].column_names\n",
    "    else:\n",
    "        column_names = datasets[\"validation\"].column_names\n",
    "    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "    if data_args.line_by_line:\n",
    "        # When using line_by_line, we just tokenize each nonempty line.\n",
    "        padding = \"max_length\" if data_args.pad_to_max_length else False\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            # Remove empty lines\n",
    "            examples[\"text\"] = [line for line in examples[\"text\"] if len(line) > 0 and not line.isspace()]\n",
    "            return tokenizer(\n",
    "                examples[\"text\"],\n",
    "                padding=padding,\n",
    "                truncation=True,\n",
    "                max_length=data_args.max_seq_length,\n",
    "                # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "                # receives the `special_tokens_mask`.\n",
    "                return_special_tokens_mask=True,\n",
    "            )\n",
    "\n",
    "        tokenized_datasets = datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=[text_column_name],\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "        )\n",
    "    else:\n",
    "        # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n",
    "        # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n",
    "        # efficient when it receives the `special_tokens_mask`.\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n",
    "\n",
    "        tokenized_datasets = datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "        )\n",
    "\n",
    "        if data_args.max_seq_length is None:\n",
    "            max_seq_length = tokenizer.model_max_length\n",
    "            if max_seq_length > 1024:\n",
    "                logger.warn(\n",
    "                    f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "                    \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n",
    "                )\n",
    "                max_seq_length = 1024\n",
    "        else:\n",
    "            if data_args.max_seq_length > tokenizer.model_max_length:\n",
    "                logger.warn(\n",
    "                    f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "                    f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "                )\n",
    "            max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "        # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "        # max_seq_length.\n",
    "        def group_texts(examples):\n",
    "            # Concatenate all texts.\n",
    "            concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "            total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "            # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "            # customize this part to your needs.\n",
    "            total_length = (total_length // max_seq_length) * max_seq_length\n",
    "            # Split by chunks of max_len.\n",
    "            result = {\n",
    "                k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "                for k, t in concatenated_examples.items()\n",
    "            }\n",
    "            return result\n",
    "\n",
    "        # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "        # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "        # might be slower to preprocess.\n",
    "        #\n",
    "        # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "        # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "        tokenized_datasets = tokenized_datasets.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "        )\n",
    "        \n",
    "        # They use multiprocessing -- could this be the difference in speed?\n",
    "        # Do you just want to change your script to use this setup instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "child-listening-env",
   "language": "python",
   "name": "child-listening-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
