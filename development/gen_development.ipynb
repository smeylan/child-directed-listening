{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface dataset version of childes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, exists\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[chi]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n",
      "['[CLS]', '[cgv]', 'i', \"'\", 'm', 'not', 'going', 'to', 'do', 'anything', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#2/20: https://huggingface.co/transformers/quickstart.html\n",
    "\n",
    "this_tokenizer = tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 6/19/21 Below line from Dr. Meylan\n",
    "this_tokenizer.add_tokens(['[chi]','[cgv]']) # This has been tested...\n",
    "print(this_tokenizer.convert_ids_to_tokens(this_tokenizer.encode(\"[CHI] i'm not going to do anything.\")))\n",
    "print(this_tokenizer.convert_ids_to_tokens(this_tokenizer.encode(\"[CGV] i'm not going to do anything.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6/19/21: https://askubuntu.com/questions/409025/permission-denied-when-running-sh-scripts\n",
    "!chmod +x run_with_tags.sh\n",
    "!./run_with_tags.sh # Temporary only, just to check how fast this runs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying new huggingface dataset behavior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_test_idx = 0\n",
    "cgv_test_idx = 628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DS_PATH = '/home/nwong/chompsky/childes/child_listening_continuation/datasets/datasets/childes_data_with_tags'\n",
    "this_ds = load_dataset(LOCAL_DS_PATH)\n",
    "\n",
    "result_chi = this_ds['validation'][chi_test_idx]\n",
    "result_cgv = this_ds['validation'][cgv_test_idx]\n",
    "\n",
    "# Check if the speaker tags are correctly inside.\n",
    "print(result_chi)\n",
    "print()\n",
    "print(result_cgv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DS_PATH = '/home/nwong/chompsky/childes/child_listening_continuation/datasets/datasets/childes_data_no_tags'\n",
    "this_ds = load_dataset(LOCAL_DS_PATH)\n",
    "\n",
    "result_chi = this_ds['validation'][chi_test_idx]\n",
    "result_cgv = this_ds['validation'][cgv_test_idx]\n",
    "\n",
    "# Check if the speaker tags are really removed.\n",
    "print(result_chi)\n",
    "print()\n",
    "print(result_cgv)\n",
    "\n",
    "# Note this was before the line for the dataset name (doesn't affect the contents) was corrected, which it is now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old tests and verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "import model_finetuning\n",
    "importlib.reload(model_finetuning)\n",
    "\n",
    "this_model, this_tokenizer = model_finetuning.get_bert_model()\n",
    "print('modules retrieved')\n",
    "maintain = model_finetuning.get_datasets(this_tokenizer, with_tags = True)\n",
    "print('first dataset')\n",
    "remove = model_finetuning.get_datasets(this_tokenizer, with_tags = False)\n",
    "print('second dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_maintain_idx = lambda test_idx : this_tokenizer.convert_ids_to_tokens(maintain['validation'][test_idx]['input_ids'])\n",
    "run_remove_idx = lambda test_idx : this_tokenizer.convert_ids_to_tokens(remove['validation'][test_idx]['input_ids'])\n",
    "\n",
    "\n",
    "print('These should have chi and cgv as single tokens.')\n",
    "print(run_maintain_idx(chi_test_idx))\n",
    "print(run_maintain_idx(cgv_test_idx))\n",
    "\n",
    "print('These should not have chi and cgv inside.')\n",
    "print(run_remove_idx(chi_test_idx))\n",
    "print(run_remove_idx(cgv_test_idx))\n",
    "\n",
    "# Verified 6/18 with fresh cache download!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "child-listening-env",
   "language": "python",
   "name": "child-listening-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
