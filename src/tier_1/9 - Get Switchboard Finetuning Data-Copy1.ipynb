{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "from os.path import join, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../')\n",
    "from src.utils import split_gen, data_cleaning, load_splits, configuration, paths\n",
    "config = configuration.Config()\n",
    "np.random.seed(config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Preprocess Switchboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../../data/switchboard’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir ../../data/switchboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 20.1M  100 20.1M    0     0  11.0M      0  0:00:01  0:00:01 --:--:-- 11.0M\n"
     ]
    }
   ],
   "source": [
    "!curl -o ../../data/switchboard/ptree_word_alignments.tar.gz https://isip.piconepress.com/projects/switchboard/releases/ptree_word_alignments.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!tar -xvzf ../../data/switchboard/ptree_word_alignments.tar.gz -C ../../data/switchboard/;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_swbd_word(w):\n",
    "    cleaned_w = re.sub(\"\\\\[laughter-\", \"\", w)\n",
    "    cleaned_w = re.sub(\"\\\\[vocalized\", \"\", cleaned_w)\n",
    "    cleaned_w = re.sub(\"\\\\[\", \"\", cleaned_w)\n",
    "    cleaned_w = re.sub(\"\\\\]\", \"\", cleaned_w)\n",
    "    cleaned_w = re.sub(\"\\\\-$\", \"\", cleaned_w)\n",
    "    cleaned_w = re.sub(\"^\\\\-\", \"\", cleaned_w)\n",
    "    cleaned_w = re.sub(\"\\\\{\", \"\", cleaned_w)\n",
    "    cleaned_w = re.sub(\"\\\\}\", \"\",cleaned_w)        \n",
    "    cleaned_w = re.sub(\"_1\", \"\",cleaned_w)        \n",
    "    cleaned_w = re.sub(\"^-$\", \"\",cleaned_w)        \n",
    "    cleaned_w = re.sub(\"^.*/$\", \"\",cleaned_w)        \n",
    "    cleaned_w = cleaned_w.lower()\n",
    "    return(cleaned_w)\n",
    "    \n",
    "\n",
    "def read_penn_tagged_swithboard_file(path, tokens_to_exclude):\n",
    "    df = pd.read_table(path, sep='\\t', header=None)\n",
    "    df.columns = ['id', 'speaker.utt','start','stop', 'unk','word1','word2']\n",
    "    \n",
    "    df = df.loc[~df.word2.isin(tokens_to_exclude)]\n",
    "    df['utt_id'] = [int(x.split('.')[1]) for x in df['speaker.utt']]\n",
    "    df['speaker'] = [x.split('.')[0] for x in df['speaker.utt']]\n",
    "    df['conversation'] = [re.sub('A|B','',  x.split('-')[0]) for x  in df['id']]\n",
    "\n",
    "    \n",
    "    # preprocess the words with the function below\n",
    "    df['cleaned_word'] = [clean_swbd_word(w) for w in df['word2']]\n",
    "    \n",
    "    # pull out the start time for everything with the new system\n",
    "    utts = df.groupby(['utt_id', 'speaker', 'conversation']).cleaned_word.agg(lambda x: ' '.join(x)).reset_index()\n",
    "    return(utts)\n",
    "                              \n",
    "\n",
    "def read_penn_tagged_swbd_pair(a_path, tokens_to_exclude):\n",
    "    b_path = a_path.replace('A-','B-')\n",
    "    \n",
    "    a_df = read_penn_tagged_swithboard_file(a_path, tokens_to_exclude)\n",
    "    b_df = read_penn_tagged_swithboard_file(b_path, tokens_to_exclude)\n",
    "        \n",
    "    combined = pd.concat([a_df, b_df]).sort_values(by=['utt_id'])\n",
    "    return(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_exclude = [\"[vocalized-noise]\", \"[noise]\", \"[laughter]\",\n",
    "\"[silence]\", \"\",\"+++\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_paths = glob.glob('../../data/switchboard/data/alignments/*/*A-*.text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "results = Parallel(n_jobs=48)(delayed(read_penn_tagged_swbd_pair)(a_path,tokens_to_exclude) for a_path in a_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_convos = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utt_id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>conversation</th>\n",
       "      <th>cleaned_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>sw4660</td>\n",
       "      <td>okay how do you get your news mostly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>@B</td>\n",
       "      <td>sw4660</td>\n",
       "      <td>generally i get most of my news from uh the ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>sw4660</td>\n",
       "      <td>um-hum um-hum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>sw4660</td>\n",
       "      <td>and just fall off on the articles that i like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>sw4660</td>\n",
       "      <td>um-hum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>239</td>\n",
       "      <td>A</td>\n",
       "      <td>sw3057</td>\n",
       "      <td>well around here we have some you know the chu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>240</td>\n",
       "      <td>B</td>\n",
       "      <td>sw3057</td>\n",
       "      <td>um-hum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>241</td>\n",
       "      <td>A</td>\n",
       "      <td>sw3057</td>\n",
       "      <td>uh take your the children and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>242</td>\n",
       "      <td>B</td>\n",
       "      <td>sw3057</td>\n",
       "      <td>um-hum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>243</td>\n",
       "      <td>A</td>\n",
       "      <td>sw3057</td>\n",
       "      <td>i know our kids thoroughly the children thorou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119680 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     utt_id speaker conversation  \\\n",
       "0         1       A       sw4660   \n",
       "0         2      @B       sw4660   \n",
       "1         3       A       sw4660   \n",
       "1         4       B       sw4660   \n",
       "2         5       A       sw4660   \n",
       "..      ...     ...          ...   \n",
       "116     239       A       sw3057   \n",
       "116     240       B       sw3057   \n",
       "117     241       A       sw3057   \n",
       "117     242       B       sw3057   \n",
       "118     243       A       sw3057   \n",
       "\n",
       "                                          cleaned_word  \n",
       "0                 okay how do you get your news mostly  \n",
       "0    generally i get most of my news from uh the ra...  \n",
       "1                                        um-hum um-hum  \n",
       "1        and just fall off on the articles that i like  \n",
       "2                                               um-hum  \n",
       "..                                                 ...  \n",
       "116  well around here we have some you know the chu...  \n",
       "116                                             um-hum  \n",
       "117                      uh take your the children and  \n",
       "117                                             um-hum  \n",
       "118  i know our kids thoroughly the children thorou...  \n",
       "\n",
       "[119680 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_convos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "swbd_txt_path = os.path.join(config.project_root, 'output/finetune/switchboard/all', 'switchboard_cleaned.txt')\n",
    "all_convos[['cleaned_word']].to_csv(swbd_txt_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_convos.columns =  ['utt_id','speaker','conversation','gloss']\n",
    "all_convos['gloss_with_punct'] = [x+'.' for x in all_convos['gloss']]\n",
    "all_convos['utterance_id'] = range(all_convos.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_convos['transcript_id'] = all_convos['conversation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"all\" split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/notebooks/child-directed-listening/src/tier_1/../../src/utils/data_cleaning.py:156: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  utt_data['contains_error'] = ['xxx' in str(x) or 'yyy' in str(x) for x in all_lowercase]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written to /home/stephan/notebooks/child-directed-listening/output/experiments/full_scale/extract_data/n=5000/Switchboard_all_no_tags/train.txt\n",
      "File written to /home/stephan/notebooks/child-directed-listening/output/experiments/full_scale/extract_data/n=5000/Switchboard_all_no_tags/eval.txt\n",
      "Writing split glosses to: /home/stephan/notebooks/child-directed-listening/output/experiments/full_scale/extract_data/n=5000/Switchboard_all_no_tags/data_pool_with_phases.pkl\n"
     ]
    }
   ],
   "source": [
    "switchboard_split_folder = paths.get_directory({\n",
    "            \"task_phase\" : 'extract_data',\n",
    "            \"training_split\" : 'Switchboard',\n",
    "            \"training_dataset\" : 'all',    \n",
    "            \"test_split\" : None,\n",
    "            \"test_dataset\" : None,\n",
    "            \"model_type\" : None,\n",
    "            \"context_width\" : None,\n",
    "            \"n_samples\" : config.n_across_time,\n",
    "            \"task_name\" : None,\n",
    "            \"use_tags\" : False\n",
    "})\n",
    "\n",
    "split_glosses_df, train_df = split_gen.exec_split_gen(all_convos, switchboard_split_folder, 'train') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_convos['word_count'] = [len(x.split(' ')) for x in all_convos['gloss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1463757"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(all_convos['word_count'])\n",
    "# only 1.5m words -- is this after removing silence tokens, etc.?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "child-directed-listening",
   "language": "python",
   "name": "child-directed-listening"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
